{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PubMed Knowledge Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is part of a series that walks through the process of generating a knowledge graph of PubMed articles.\n",
    "\n",
    "This notebook will\n",
    "* Define a lexical graph schema\n",
    "* Populate a Neo4j instance with articles chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter some Numpy warnings that pop up during ingestion\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "from math import ceil\n",
    "from typing import Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allows for async operations in notebooks\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexical Graph Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use Unstructured.IO to partition and chunk our articles. \n",
    "\n",
    "This process breaks the articles into sensible chunks that may be used as context in our application. \n",
    "\n",
    "These chunks will also have relationships to the extracted entities, but we will add these later.\n",
    "\n",
    "The lexical graph is based on the data model defined in the [Neo4j Connector Documentation](https://graphrag.com/reference/knowledge-graph/lexical-graph-extracted-entities/) section of [Unstructured](unstructured.io).\n",
    "\n",
    "The main difference here is that we are capturing Text, Image and Table Elements in distinct nodes, instead of grouping them all into a single `UnstructuredElement` node.\n",
    "\n",
    "Here is the data model we will be using.\n",
    "\n",
    "\n",
    "<img src=\"./assets/images/lexical-data-model.png\" alt=\"lexical-data-model\" width=\"800px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexandergilmore/Documents/projects/ps-nashville/pubmed-knowledge-graph/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pydantic import BaseModel, Field\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "\n",
    "from unstructured.documents.elements import CompositeElement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------\n",
    "# Nodes\n",
    "# ------------\n",
    "\n",
    "class Document(BaseModel):\n",
    "    \"\"\"\n",
    "    A Document.\n",
    "    This is the top level node in our knowledge graph.\n",
    "    Documents are made of many Chunks.\n",
    "    \"\"\"\n",
    "    id: str = Field(..., description=\"The id of the document\")\n",
    "    name: str = Field(..., description=\"The name of the document\")\n",
    "    source: str = Field(..., description=\"The source of the document\")\n",
    "\n",
    "class Chunk(BaseModel):\n",
    "    \"\"\"\n",
    "    A Chunk.\n",
    "    This is a collection of `UnstructuredElements`.\n",
    "    Unstructured.IO represents Chunks as `CompositeElement` objects.\n",
    "    \"\"\"\n",
    "    id: str = Field(..., description=\"The id of the chunk\")\n",
    "    type: str = Field(..., description=\"The type of the chunk\")\n",
    "    text: str = Field(..., description=\"The text of the chunk\")\n",
    "\n",
    "class ChunkWithEmbedding(Chunk):\n",
    "    \"\"\"\n",
    "    A Chunk with an embedding.\n",
    "    This is used to represent chunks that have been embedded.\n",
    "    \"\"\"\n",
    "    embedding: list[float] = Field(..., description=\"The embedding of the chunk text field\")\n",
    "\n",
    "class UnstructuredElement(BaseModel):\n",
    "    \"\"\"\n",
    "    A base class for all unstructured elements. \n",
    "    These are the smallest units in our chunking process. \n",
    "    One or more of these elements are combined to form a Chunk.\n",
    "    \"\"\"\n",
    "    id: str = Field(..., description=\"The id of the element\")\n",
    "    text: str = Field(..., description=\"The text of the element\")\n",
    "    type: str = Field(..., description=\"The type of the element\")\n",
    "    page_number: int = Field(..., description=\"The page number of the element\")\n",
    "\n",
    "class TextElement(UnstructuredElement):\n",
    "    \"\"\"\n",
    "    A TextElement. Structurally identical to the UnstructuredElement class.\n",
    "    This is used to represent text elements that contain no tables or images.\n",
    "    \"\"\"\n",
    "\n",
    "class ImageElement(UnstructuredElement):\n",
    "    \"\"\"\n",
    "    An ImageElement.\n",
    "    \"\"\"\n",
    "    image_base64: str = Field(..., description=\"The base64 encoded image\")\n",
    "    image_mime_type: str = Field(..., description=\"The mime type of the image\")\n",
    "\n",
    "class TableElement(UnstructuredElement):\n",
    "    \"\"\"\n",
    "    A TableElement. \n",
    "    This may also optionally have image features.\n",
    "    \"\"\"\n",
    "    image_base64: str | None = Field(None, description=\"The base64 encoded table\")\n",
    "    image_mime_type: str | None = Field(None, description=\"The mime type of the table\")\n",
    "    text_as_html: str | None = Field(None, description=\"The text of the table as HTML\")\n",
    "    \n",
    "# -------------\n",
    "# Relationships\n",
    "# -------------\n",
    "\n",
    "class ChunkPartOfDocument(BaseModel):\n",
    "    \"\"\"\n",
    "    (:Chunk {id: $chunk_id})-[:PART_OF_DOCUMENT]->(:Document {id: $document_id})\n",
    "    \"\"\"\n",
    "    chunk_id: str = Field(..., description=\"The id of the chunk\")\n",
    "    document_id: str = Field(..., description=\"The id of the document\")\n",
    "\n",
    "class UnstructuredElementPartOfChunk(BaseModel):\n",
    "    \"\"\"\n",
    "    (:UnstructuredElement {id: $unstructured_element_id})-[:PART_OF_CHUNK]->(:Chunk {id: $chunk_id})\n",
    "\n",
    "    This covers TextElement, ImageElement and TableElement nodes since they all share the UnstructuredElement label.\n",
    "    \"\"\" \n",
    "    unstructured_element_id: str = Field(..., description=\"The id of the unstructured element\")\n",
    "    chunk_id: str = Field(..., description=\"The id of the chunk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chunk_has_next_chunk_relationship_dataframe(chunk_dataframe: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create the DataFrame for loading (:Chunk)-[:HAS_NEXT_CHUNK]->(:Chunk) relationships.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    chunk_dataframe : pd.DataFrame\n",
    "        A Pandas DataFrame containing the Chunk node records.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A Pandas DataFrame containing the columns `source_id` and `target_id`\n",
    "    \"\"\"\n",
    "    df = chunk_dataframe.copy()\n",
    "    df['next_id'] = df['id'].shift(-1)\n",
    "    df.dropna(inplace=True)\n",
    "    res = df[['id', 'next_id']].rename({\"id\": \"source_id\", \"next_id\": \"target_id\"}, axis=1)\n",
    "    return res\n",
    "\n",
    "def extract_document_title(text_elements_dataframe: pd.DataFrame) -> str:\n",
    "    \"\"\"\n",
    "    Extract the title of the document from the text elements.\n",
    "    Here we assume that the first 'Title' element is the title of the document.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        The title of the document.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return text_elements_dataframe[text_elements_dataframe['type'] == 'Title'].iloc[0]['text']\n",
    "    except Exception as e:\n",
    "        print(f\"Unable to extract document title: {e}\")\n",
    "        return 'unknown title'\n",
    "\n",
    "def parse_node_and_relationship_from_composite_element(composite_element: CompositeElement, parent_document_id: str) -> dict[str, dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Parse the nodes and relationships for a given chunk (CompositeElement). \n",
    "    This will find the following nodes:\n",
    "    * Chunk\n",
    "    * TextElement\n",
    "    * ImageElement\n",
    "    * TableElement\n",
    "    * UnstructuredElement (Shared label for TextElement, ImageElement and TableElement)\n",
    "\n",
    "    And the following relationships:\n",
    "    * (:Chunk)-[:PART_OF_DOCUMENT]->(:Document)\n",
    "    * (:UnstructuredElement)-[:PART_OF_CHUNK]->(:Chunk)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict[str, dict[str, Any]]\n",
    "        A dictionary containing a list of records for each node and relationship type.\n",
    "    \"\"\"\n",
    "    chunk = Chunk(id=composite_element.id, text=composite_element.text, type=composite_element.category)\n",
    "    chunk_part_of_document = ChunkPartOfDocument(chunk_id=chunk.id, document_id=parent_document_id)\n",
    "\n",
    "    text_elements: list[TextElement] = list()\n",
    "    image_elements: list[ImageElement] = list()\n",
    "    table_elements: list[TableElement] = list()\n",
    "    unstructured_element_part_of_chunk: list[UnstructuredElementPartOfChunk] = list()\n",
    "\n",
    "    # Chunks (CompositeElements) are made of many smaller text chunks (UnstructuredElements)\n",
    "    # We can parse what type of elements these subchunks are and load them as well\n",
    "    # This will give us access to images and tables from the document\n",
    "    for element in composite_element.metadata.orig_elements:\n",
    "        match element.category:\n",
    "            case \"NarrativeText\":\n",
    "                text_elements.append(TextElement(id=element.id, \n",
    "                                                 text=element.text, \n",
    "                                                 type=element.category, \n",
    "                                                 page_number=element.metadata.page_number))\n",
    "            case \"Image\":\n",
    "                image_elements.append(ImageElement(id=element.id, \n",
    "                                                   text=element.text,\n",
    "                                                   type=element.category, \n",
    "                                                   page_number=element.metadata.page_number,\n",
    "                                                   image_base64=element.metadata.image_base64, \n",
    "                                                   image_mime_type=element.metadata.image_mime_type))\n",
    "            case \"Table\":\n",
    "                table_elements.append(TableElement(id=element.id, \n",
    "                                                   text=element.text,\n",
    "                                                   type=element.category, \n",
    "                                                   page_number=element.metadata.page_number,\n",
    "                                                   image_base_64=element.metadata.image_base64,\n",
    "                                                   image_mime_type=element.metadata.image_mime_type,\n",
    "                                                   text_as_html=element.metadata.text_as_html))\n",
    "            # Assume some kind of text element if we can't match the category\n",
    "            # Could be headers, figure captions, etc\n",
    "            case _:\n",
    "                try:\n",
    "                    text_elements.append(TextElement(id=element.id, \n",
    "                                                 text=element.text, \n",
    "                                                 type=element.category, \n",
    "                                                 page_number=element.metadata.page_number))\n",
    "                except Exception as e:\n",
    "                    print(f\"Error parsing text element: {e}\")\n",
    "\n",
    "        unstructured_element_part_of_chunk.append(UnstructuredElementPartOfChunk(unstructured_element_id=element.id, chunk_id=chunk.id))\n",
    "\n",
    "    # we return a list of records for each entity and relationship instead of the Pydantic classes\n",
    "    return {\n",
    "        \"nodes\": {\n",
    "            \"chunk\": [chunk.model_dump()],\n",
    "            \"text_element\": [el.model_dump() for el in text_elements],\n",
    "            \"image_element\": [el.model_dump() for el in image_elements],\n",
    "            \"table_element\": [el.model_dump() for el in table_elements],\n",
    "        },\n",
    "        \"relationships\": {\n",
    "            \"chunk_part_of_document\": [chunk_part_of_document.model_dump()],\n",
    "            \"unstructured_element_part_of_chunk\": [rel.model_dump() for rel in unstructured_element_part_of_chunk],\n",
    "        }\n",
    "    }\n",
    "\n",
    "def parse_nodes_and_relationships_from_composite_elements(composite_elements: list[CompositeElement], parent_doc_id: str) -> dict[str, dict[str, pd.DataFrame]]:\n",
    "    \"\"\"\n",
    "    Parse entity nodes and document relationships for a set of chunks (CompositeElements) and their parent document\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    composite_elements : list[CompositeElement]\n",
    "        A list of CompositeElements to parse.\n",
    "    parent_doc_id : str\n",
    "        The id of the parent document.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict[str, dict[str, pd.DataFrame]]\n",
    "        A dictionary containing the node and relationship Pandas DataFrames for ingestion into the knowledge graph.\n",
    "    \"\"\"\n",
    "    \n",
    "    data = {\n",
    "        \"nodes\": {\n",
    "            \"document\": list(),\n",
    "            \"chunk\": list(),\n",
    "            \"text_element\": list(),\n",
    "            \"image_element\": list(),\n",
    "            \"table_element\": list(),\n",
    "        },\n",
    "        \"relationships\": {\n",
    "            \"chunk_part_of_document\": list(),\n",
    "            \"unstructured_element_part_of_chunk\": list(),\n",
    "            \"chunk_has_next_chunk\": list()\n",
    "        }\n",
    "    }\n",
    "\n",
    "    for composite_element in composite_elements:\n",
    "        new_data = parse_node_and_relationship_from_composite_element(composite_element, parent_doc_id)\n",
    "\n",
    "        # update the records with new nodes and relationships\n",
    "        data[\"nodes\"][\"chunk\"].extend(new_data[\"nodes\"][\"chunk\"])\n",
    "        data[\"relationships\"][\"chunk_part_of_document\"].extend(new_data[\"relationships\"][\"chunk_part_of_document\"])\n",
    "        data[\"nodes\"][\"text_element\"].extend(new_data[\"nodes\"][\"text_element\"])\n",
    "        data[\"nodes\"][\"image_element\"].extend(new_data[\"nodes\"][\"image_element\"])\n",
    "        data[\"nodes\"][\"table_element\"].extend(new_data[\"nodes\"][\"table_element\"])\n",
    "        data[\"relationships\"][\"unstructured_element_part_of_chunk\"].extend(new_data[\"relationships\"][\"unstructured_element_part_of_chunk\"])\n",
    "\n",
    "    # convert to pandas dataframe for ingestion\n",
    "    # node DataFrames\n",
    "    data[\"nodes\"][\"chunk\"] = pd.DataFrame(data[\"nodes\"][\"chunk\"])\n",
    "    data[\"nodes\"][\"text_element\"] = pd.DataFrame(data[\"nodes\"][\"text_element\"])\n",
    "    data[\"nodes\"][\"image_element\"] = pd.DataFrame(data[\"nodes\"][\"image_element\"])\n",
    "    data[\"nodes\"][\"table_element\"] = pd.DataFrame(data[\"nodes\"][\"table_element\"])\n",
    "\n",
    "    document_title = extract_document_title(data[\"nodes\"][\"text_element\"])\n",
    "    data[\"nodes\"][\"document\"] = pd.DataFrame([Document(id=parent_doc_id, name=document_title, source=\"pubmed\").model_dump()])\n",
    "\n",
    "    # relationship DataFrames\n",
    "    data[\"relationships\"][\"chunk_part_of_document\"] = pd.DataFrame(data[\"relationships\"][\"chunk_part_of_document\"])\n",
    "    data[\"relationships\"][\"unstructured_element_part_of_chunk\"] = pd.DataFrame(data[\"relationships\"][\"unstructured_element_part_of_chunk\"])\n",
    "    data[\"relationships\"][\"chunk_has_next_chunk\"] = create_chunk_has_next_chunk_relationship_dataframe(data[\"nodes\"][\"chunk\"])\n",
    "\n",
    "    return data\n",
    "\n",
    "def process_pdf_article(file_name: str) -> dict[str, dict[str, pd.DataFrame]]:\n",
    "    \"\"\"\n",
    "    Process an article and return the nodes and relationships for ingestion into the knowledge graph.\n",
    "    Assumes that the article is stored in the \"articles/pdf\" directory.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_name : str\n",
    "        The name of the file to process.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict[str, dict[str, pd.DataFrame]]\n",
    "        A dictionary containing the node and relationship Pandas DataFrames for ingestion into the knowledge graph.\n",
    "    \"\"\"\n",
    "\n",
    "    doc_id = hashlib.sha256(file_name.encode()).hexdigest()\n",
    "\n",
    "    partitioned_doc = partition_pdf(\"articles/pdf/\" + file_name,                   # path to the article file\n",
    "                                    strategy=\"hi_res\",                             # required to extract images        \n",
    "                                    extract_images_in_pdf=True,                    # required to extract images\n",
    "                                    extract_image_block_types=[\"Image\", \"Table\"],  # extract images and tables as base64\n",
    "                                    extract_image_block_to_payload=True,           # required to extract images as base64\n",
    "                                    chunking_strategy=\"by_title\",                  # chunk by title - this breaks by indentified sections\n",
    "                                    combine_text_under_n_chars=200,                # combine text under 200 characters\n",
    "                                    max_characters=1000,                           # 1000 <= characters per chunk\n",
    "                                    multipage_sections=True)                       # combine multi-page sections\n",
    "    # return partitioned_doc\n",
    "    return parse_nodes_and_relationships_from_composite_elements(partitioned_doc, doc_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdf_articles(article_file_names: list[str]) -> dict[str, dict[str, pd.DataFrame]]:\n",
    "    \"\"\"\n",
    "    Process a list of articles and return the nodes and relationships for ingestion into the knowledge graph.\n",
    "    Assumes that the articles are stored in the \"articles/pdf/\" directory\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    article_file_names : list[str]\n",
    "        A list of the names of the files to process.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict[str, dict[str, pd.DataFrame]]\n",
    "        A dictionary containing the node and relationship Pandas DataFrames for ingestion into the knowledge graph.\n",
    "    \"\"\"\n",
    "\n",
    "    # initialize the DataFrames\n",
    "    data = {\n",
    "        \"nodes\": {\n",
    "            \"document\": pd.DataFrame(),\n",
    "            \"chunk\": pd.DataFrame(),\n",
    "            \"text_element\": pd.DataFrame(),\n",
    "            \"image_element\": pd.DataFrame(),\n",
    "            \"table_element\": pd.DataFrame(),\n",
    "        },\n",
    "        \"relationships\": {\n",
    "            \"chunk_part_of_document\": pd.DataFrame(),\n",
    "            \"unstructured_element_part_of_chunk\": pd.DataFrame(),\n",
    "            \"chunk_has_next_chunk\": pd.DataFrame()\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # process each article individually\n",
    "    # this will\n",
    "    # * partition the article into chunks using Unstructured\n",
    "    # * Identify TextElements, ImageElements, and TableElements in each chunk\n",
    "    # * Create DataFrames for all lexical nodes and relationships found in the article\n",
    "    # * Update the global DataFrames with the new article data\n",
    "    for file_name in article_file_names:\n",
    "        print(f\"Processing article: {file_name}\")\n",
    "        # process a single article \n",
    "        article_data = process_pdf_article(file_name)\n",
    "\n",
    "        # update the DataFrames with the new article data\n",
    "        data[\"nodes\"][\"document\"] = pd.concat([data[\"nodes\"][\"document\"], article_data[\"nodes\"][\"document\"]], ignore_index=True)\n",
    "        data[\"nodes\"][\"chunk\"] = pd.concat([data[\"nodes\"][\"chunk\"], article_data[\"nodes\"][\"chunk\"]], ignore_index=True)\n",
    "        data[\"nodes\"][\"text_element\"] = pd.concat([data[\"nodes\"][\"text_element\"], article_data[\"nodes\"][\"text_element\"]], ignore_index=True)\n",
    "        data[\"nodes\"][\"image_element\"] = pd.concat([data[\"nodes\"][\"image_element\"], article_data[\"nodes\"][\"image_element\"]], ignore_index=True)\n",
    "        data[\"nodes\"][\"table_element\"] = pd.concat([data[\"nodes\"][\"table_element\"], article_data[\"nodes\"][\"table_element\"]], ignore_index=True)\n",
    "        data[\"relationships\"][\"chunk_part_of_document\"] = pd.concat([data[\"relationships\"][\"chunk_part_of_document\"], article_data[\"relationships\"][\"chunk_part_of_document\"]], ignore_index=True)\n",
    "        data[\"relationships\"][\"unstructured_element_part_of_chunk\"] = pd.concat([data[\"relationships\"][\"unstructured_element_part_of_chunk\"], article_data[\"relationships\"][\"unstructured_element_part_of_chunk\"]], ignore_index=True)\n",
    "        data[\"relationships\"][\"chunk_has_next_chunk\"] = pd.concat([data[\"relationships\"][\"chunk_has_next_chunk\"], article_data[\"relationships\"][\"chunk_has_next_chunk\"]], ignore_index=True)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to collect the article file names to pass to Unstructured for parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_names = os.listdir(\"articles/pdf/\")[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now defined \n",
    "* Lexical data model\n",
    "* Partitioning and chunking logic for articles\n",
    "\n",
    "It is now time to define our ingestion logic. We will run ingest in three stages \n",
    "\n",
    "1. Load lexical graph\n",
    "2. Embed lexical graph Chunk nodes\n",
    "\n",
    "Decoupling these stages allows us easily make changes as we iterate our ingestion process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using PyNeoInstance to ingest our data into Neo4j. \n",
    "\n",
    "This allows for easy and manageable database and query configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from pyneoinstance import Neo4jInstance, load_yaml_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our database credentials and all of our queries are stored in the `pyneoinstance_config.yaml` file. \n",
    "\n",
    "This makes it easy to manage our queries and keeps the notebook code clean. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_yaml_file(\"pyneoinstance_config.yaml\")\n",
    "\n",
    "db_info = config['db_info']\n",
    "\n",
    "constraints = config['initializing_queries']['constraints']\n",
    "indexes = config['initializing_queries']['indexes']\n",
    "\n",
    "node_load_queries = config['loading_queries']['nodes']\n",
    "relationship_load_queries = config['loading_queries']['relationships']\n",
    "\n",
    "processing_queries = config['processing_queries']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This graph object will handle database connections and read / write transactions for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = Neo4jInstance(db_info.get('uri', os.getenv(\"NEO4J_URI\", \"neo4j://localhost:7687\")), # use config value -> use env value -> use default value\n",
    "                      db_info.get('user', os.getenv(\"NEO4J_USER\", \"neo4j\")), \n",
    "                      db_info.get('password', os.getenv(\"NEO4J_PASSWORD\", \"password\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a helper function for ingesting data using the PyNeoInstance library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_partition(data: pd.DataFrame, batch_size: int = 500) -> int:\n",
    "    \"\"\"\n",
    "    Determine the data partition based on the desired batch size.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : pd.DataFrame\n",
    "        The Pandas DataFrame to partition.\n",
    "    batch_size : int\n",
    "        The desired batch size.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        The partition size.\n",
    "    \"\"\"\n",
    "    \n",
    "    partition = int(len(data) / batch_size)\n",
    "    print(\"partition: \"+str(partition if partition > 1 else 1))\n",
    "    return partition if partition > 1 else 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we write all the constraints and indexes we need for the entire graph.\n",
    "\n",
    "We will be utilizing the vector index to perform similarity search over our `Chunk` nodes.\n",
    "\n",
    "The query to set this index may be found in the `pyneoinstance_config.yaml` file and looks like this:\n",
    "\n",
    "```cypher\n",
    "CREATE VECTOR INDEX chunk_vector_index IF NOT EXISTS\n",
    "    FOR (c:Chunk)\n",
    "    ON c.embedding\n",
    "    OPTIONS { indexConfig: {\n",
    "        `vector.dimensions`: 768,\n",
    "        `vector.similarity_function`: 'cosine'\n",
    "    }}\n",
    "```\n",
    "\n",
    "Since we set the dimensions to 768, we must ensure that we use 768 dimensions when generating our embeddings as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_constraints_and_indexes() -> None:\n",
    "    \"\"\"\n",
    "    Create constraints and indexes for the lexical, entity and patient journey graphs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if constraints and len(constraints) > 0:\n",
    "            graph.execute_write_queries(database=db_info['database'], queries=list(constraints.values()))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    try:\n",
    "        if indexes and len(indexes) > 0:\n",
    "            graph.execute_write_queries(database=db_info['database'], queries=list(indexes.values()))\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_constraints_and_indexes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingest Lexical Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing | Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process the articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2 articles\n",
      "*****\n",
      "* nihms-1852972.pdf\n",
      "* fendo-11-00178.pdf\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loaded {len(article_names)} articles\\n{'*'*5}\")\n",
    "[print(f\"* {article_name}\") for article_name in article_names]\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing article: nihms-1852972.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cannot set gray non-stroke color because /'R50' is an invalid float value\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing article: fendo-11-00178.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cannot set gray non-stroke color because /'R50' is an invalid float value\n"
     ]
    }
   ],
   "source": [
    "lexical_ingest_records = process_pdf_articles(article_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the first few records "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>753d70915e2a2a747cee355745bd17ff08c45f90d938b1...</td>\n",
       "      <td>HHS Public Access</td>\n",
       "      <td>pubmed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>e0048a13f033f7fe71581406cd2d3bac1ffc4db7ce88a8...</td>\n",
       "      <td>OPEN ACCESS</td>\n",
       "      <td>pubmed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  id               name  \\\n",
       "0  753d70915e2a2a747cee355745bd17ff08c45f90d938b1...  HHS Public Access   \n",
       "1  e0048a13f033f7fe71581406cd2d3bac1ffc4db7ce88a8...        OPEN ACCESS   \n",
       "\n",
       "   source  \n",
       "0  pubmed  \n",
       "1  pubmed  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexical_ingest_records[\"nodes\"][\"document\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Document and Chunk nodes into the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_lexical_nodes(document_dataframe: pd.DataFrame, \n",
    "                       chunk_dataframe: pd.DataFrame, \n",
    "                       text_element_dataframe: pd.DataFrame, \n",
    "                       image_element_dataframe: pd.DataFrame, \n",
    "                       table_element_dataframe: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Load lexical nodes into the graph. These include Document and Chunk nodes.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    document_dataframe : pd.DataFrame\n",
    "        A Pandas DataFrame of Document nodes to load into the graph. \n",
    "    chunk_dataframe : pd.DataFrame\n",
    "        A Pandas DataFrame of Chunk nodes to load into the graph.\n",
    "    text_element_dataframe : pd.DataFrame\n",
    "        A Pandas DataFrame of TextElement nodes to load into the graph. \n",
    "    image_element_dataframe : pd.DataFrame\n",
    "        A Pandas DataFrame of ImageElement nodes to load into the graph. \n",
    "    table_element_dataframe : pd.DataFrame\n",
    "        A Pandas DataFrame of TableElement nodes to load into the graph. \n",
    "    \"\"\"\n",
    "    \n",
    "    lexical_nodes_ingest_iterator = list(zip([document_dataframe, \n",
    "                                              chunk_dataframe, \n",
    "                                              text_element_dataframe, \n",
    "                                              image_element_dataframe, \n",
    "                                              table_element_dataframe], \n",
    "                                              ['document', \n",
    "                                               'chunk', \n",
    "                                               'text_element', \n",
    "                                               'image_element', \n",
    "                                               'table_element']))\n",
    "\n",
    "    for data, query in lexical_nodes_ingest_iterator:\n",
    "        res = graph.execute_write_query_with_data(database=db_info['database'], \n",
    "                                                    data=data, \n",
    "                                                    query=node_load_queries[query], \n",
    "                                                    partitions=get_partition(data, batch_size=500), \n",
    "                                                    parallel=True,\n",
    "                                                    workers=2)\n",
    "        print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "partition: 1\n",
      "{'labels_added': 2, 'nodes_created': 2, 'properties_set': 6}\n",
      "partition: 1\n",
      "{'labels_added': 209, 'nodes_created': 209, 'properties_set': 627}\n",
      "partition: 2\n",
      "{'labels_added': 2768, 'nodes_created': 1384, 'properties_set': 5536}\n",
      "partition: 1\n",
      "{'labels_added': 12, 'nodes_created': 6, 'properties_set': 36}\n",
      "partition: 1\n",
      "{'labels_added': 10, 'nodes_created': 5, 'properties_set': 30}\n"
     ]
    }
   ],
   "source": [
    "load_lexical_nodes(lexical_ingest_records[\"nodes\"][\"document\"], \n",
    "                   lexical_ingest_records[\"nodes\"][\"chunk\"], \n",
    "                   lexical_ingest_records[\"nodes\"][\"text_element\"], \n",
    "                   lexical_ingest_records[\"nodes\"][\"image_element\"], \n",
    "                   lexical_ingest_records[\"nodes\"][\"table_element\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the lexical relationships into the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_lexical_relationships(chunk_part_of_document_dataframe: pd.DataFrame, \n",
    "                               unstructured_element_part_of_chunk_dataframe: pd.DataFrame, \n",
    "                               chunk_has_next_chunk_dataframe: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Load lexical relationships into the graph.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    chunk_part_of_document_dataframe : pd.DataFrame\n",
    "        A Pandas DataFrame of Chunk - PART_OF -> Document relationships to load into the graph.\n",
    "        Should have columns `chunk_id` and `document_id`.\n",
    "    unstructured_element_part_of_chunk_dataframe : pd.DataFrame\n",
    "        A Pandas DataFrame of UnstructuredElement - PART_OF -> Chunk relationships to load into the graph.\n",
    "        Should have columns `unstructured_element_id` and `chunk_id`.\n",
    "    chunk_has_next_chunk_dataframe : pd.DataFrame\n",
    "        A Pandas DataFrame of Chunk - HAS_NEXT_CHUNK -> Chunk relationships to load into the graph.\n",
    "        Should have columns `source_id` and `target_id`.\n",
    "    \"\"\"\n",
    "    lexical_relationships_ingest_iterator = list(zip([chunk_part_of_document_dataframe, \n",
    "                                                      unstructured_element_part_of_chunk_dataframe, \n",
    "                                                      chunk_has_next_chunk_dataframe], \n",
    "                                                      ['chunk_part_of_document', \n",
    "                                                       'unstructured_element_part_of_chunk', \n",
    "                                                       'chunk_has_next_chunk']))\n",
    "\n",
    "    for data, query in lexical_relationships_ingest_iterator:\n",
    "        res = graph.execute_write_query_with_data(database=db_info['database'], \n",
    "                                                    data=data, \n",
    "                                                    query=relationship_load_queries[query], \n",
    "                                                    partitions=get_partition(data, batch_size=500))\n",
    "        print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "partition: 1\n",
      "{'relationships_created': 209}\n",
      "partition: 2\n",
      "{'relationships_created': 1428}\n",
      "partition: 1\n",
      "{'relationships_created': 207}\n"
     ]
    }
   ],
   "source": [
    "load_lexical_relationships(lexical_ingest_records[\"relationships\"][\"chunk_part_of_document\"], \n",
    "                          lexical_ingest_records[\"relationships\"][\"unstructured_element_part_of_chunk\"],\n",
    "                          lexical_ingest_records[\"relationships\"][\"chunk_has_next_chunk\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embed Lexical Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will read Chunk nodes from the graph that don't have embedding properties yet. \n",
    "\n",
    "We will then embed the Chunk text property and add the embedding as a property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "from openai import AsyncOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_client = AsyncOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunks_to_embed(min_length: int = 20) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Get the chunks to embed.\n",
    "    \"\"\"\n",
    "    chunks_to_embed = graph.execute_read_query(database=db_info['database'], \n",
    "                                               query=processing_queries['get_chunk_nodes_to_embed'],\n",
    "                                               parameters={'min_length': min_length})\n",
    "    return chunks_to_embed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To learn more about OpenAI embedding models follow this [link](https://platform.openai.com/docs/guides/embeddings/embedding-models#embedding-models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def create_single_chunk_embedding(chunk_text: str, chunk_id: str, failed_cache: list[tuple[str, str]]) -> list[float]:\n",
    "    \"\"\"\n",
    "    Create embedding for a single Chunk node's text.\n",
    "\n",
    "    Parameters  \n",
    "    ----------\n",
    "    chunk_text : str\n",
    "        The text of the chunk to embed.\n",
    "    chunk_id : str\n",
    "        The id of the chunk to embed.\n",
    "    failed_cache : list[tuple[str, str]]\n",
    "        A list of tuples, where the first element is the chunk id and the second element is the text chunk.\n",
    "        This is used to log failed embeddings across batches.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list[float]\n",
    "        The embedding for the chunk text.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        response = await embedding_client.embeddings.create(\n",
    "            model=\"text-embedding-3-small\",\n",
    "            input=chunk_text,\n",
    "            encoding_format=\"float\",\n",
    "            dimensions=768, # must be the same dimensions as the vector index\n",
    "        )\n",
    "        return response.data[0].embedding\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        failed_cache.append((chunk_id, chunk_text))\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def create_chunk_embeddings(chunk_nodes_dataframe: pd.DataFrame, batch_size: int = 100) -> list[tuple[str, list[Any]]]:\n",
    "    \"\"\"\n",
    "    Create embeddings for a Pandas DataFrame of Chunk nodes.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    chunk_nodes_dataframe : pd.DataFrame\n",
    "        A Pandas DataFrame where each row represents a Chunk node.\n",
    "        Has columns `id` and `text`.\n",
    "    batch_size : int\n",
    "        The number of text chunks to process in each batch.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list[tuple[str, list[float]]]\n",
    "        A list of tuples, where the first element is the chunk id and the second element is the embedding for the chunk text.\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    async def _create_embeddings_for_batch(batch: pd.DataFrame, failed_cache: list[tuple[str, str]]) -> list[tuple[str, list[dict[str, Any]]]]:\n",
    "        \"\"\"\n",
    "        Create embeddings for a batch of text chunks.\n",
    "        Failed extractions are maintained in the `failed_cache` list that is passed to the embedding creation function.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch : pd.DataFrame\n",
    "            A Pandas DataFrame where each row represents a text chunk.\n",
    "            Has columns `id` and `text`.\n",
    "        failed_cache : list[tuple[str, str]]\n",
    "            A list of tuples, where the first element is the chunk id and the second element is the text chunk.\n",
    "            This is used to log failed embeddings across batches.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        list[tuple[str, list[dict[str, Any]]]]\n",
    "            A list of tuples, where the first element is the chunk id and the second element is a list of entities found in the chunk.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Create tasks for all nodes in the batch\n",
    "        # order is maintained\n",
    "        tasks = [create_single_chunk_embedding(row[\"text\"], row['id'], failed_cache) for _, row in batch.iterrows()]\n",
    "        # Execute all tasks concurrently\n",
    "        embedding_results = await asyncio.gather(*tasks)\n",
    "\n",
    "        # filter results to only include non-None values\n",
    "        embedding_results = [(id, embedding) for id, embedding in zip(batch[\"id\"], embedding_results) if embedding is not None]\n",
    "\n",
    "        return embedding_results\n",
    "\n",
    "    \n",
    "    async def _create_embeddings_in_batches(chunk_nodes_dataframe: pd.DataFrame, batch_size: int) -> tuple[list[tuple[str, list[dict[str, Any]]]], list[tuple[str, str]]]:\n",
    "        \"\"\"\n",
    "        Create embeddings for a Pandas DataFrame of text chunks in batches.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        chunk_nodes_dataframe : pd.DataFrame\n",
    "            A Pandas DataFrame where each row represents a text chunk.\n",
    "            Has columns `id` and `text`.\n",
    "        batch_size : int\n",
    "            The number of text chunks to process in each batch.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tuple[list[tuple[str, list[float]]], list[tuple[str, str]]]\n",
    "            A tuple of two lists. The first list contains tuples of chunk id and list of embeddings for the chunk text.\n",
    "            The second list contains tuples of chunk id and text chunk that failed to be processed.\n",
    "        \"\"\"\n",
    "\n",
    "        results = list()\n",
    "        failed_cache: list[tuple[str, str]] = list() # [(chunk_id, text_chunk), ...]\n",
    "        for batch_idx, i in enumerate(range(0, len(chunk_nodes_dataframe), batch_size)):\n",
    "            print(f\"Processing batch {batch_idx+1} of {ceil(len(chunk_nodes_dataframe)/(batch_size))}  \\n\", end=\"\\r\") \n",
    "            if i + batch_size >= len(chunk_nodes_dataframe):\n",
    "                batch = chunk_nodes_dataframe.iloc[i:]\n",
    "            else:\n",
    "                batch = chunk_nodes_dataframe.iloc[i:i+batch_size]\n",
    "            batch_results = await _create_embeddings_for_batch(batch, failed_cache)\n",
    "\n",
    "            # Add extracted records to the results list\n",
    "            results.extend(batch_results)\n",
    "\n",
    "        return results, failed_cache\n",
    "\n",
    "    # first pass through chunks\n",
    "    results, failed = await _create_embeddings_in_batches(chunk_nodes_dataframe, batch_size)\n",
    "    print(f\"Successful chunks : {len(results)}\")\n",
    "    print(f\"Failed chunks     : {len(failed)}\")\n",
    "    print(\"--------------------------------\")\n",
    "    print(\"Retrying failed chunks...\")\n",
    "\n",
    "    # retry failed chunks once\n",
    "    retry_df = pd.DataFrame(failed, columns=[\"id\", \"text\"])\n",
    "    retry_results, failed = await _create_embeddings_in_batches(retry_df, batch_size)\n",
    "    print(f\"Successful retries : {len(retry_results)}\")\n",
    "    print(f\"Failed retries     : {len(failed)}\")\n",
    "\n",
    "    print(\"--------------------------------\")\n",
    "    print(f\"Overall Success Rate : {round(len(results + retry_results) / len(chunk_nodes_dataframe) * 100, 2)}%\")\n",
    "\n",
    "    return results + retry_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = get_chunks_to_embed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14c257d5146e625ccbe4b0435317b173</td>\n",
       "      <td>The adverse events associated with GLP-1RA are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18032216f80a07578785896148f43e26</td>\n",
       "      <td>The rates of secondary-outcome events (Table 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19f0d07d88fc2175b63d25ea52201783</td>\n",
       "      <td>GLP-1 is secreted by L-cells found in the ileu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 id  \\\n",
       "0  14c257d5146e625ccbe4b0435317b173   \n",
       "1  18032216f80a07578785896148f43e26   \n",
       "2  19f0d07d88fc2175b63d25ea52201783   \n",
       "\n",
       "                                                text  \n",
       "0  The adverse events associated with GLP-1RA are...  \n",
       "1  The rates of secondary-outcome events (Table 2...  \n",
       "2  GLP-1 is secreted by L-cells found in the ileu...  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 1 of 2  \n",
      "Processing batch 2 of 2  \n",
      "Successful chunks : 196\n",
      "Failed chunks     : 0\n",
      "--------------------------------\n",
      "Retrying failed chunks...\n",
      "Successful retries : 0\n",
      "Failed retries     : 0\n",
      "--------------------------------\n",
      "Overall Success Rate : 100.0%\n"
     ]
    }
   ],
   "source": [
    "chunks_with_embeddings = await create_chunk_embeddings(chunks, batch_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neo4j has special functions to write embedding properties.\n",
    "\n",
    "These functions store the embedding in a more space-efficient manner compared to using `SET`.\n",
    "\n",
    "We can use \n",
    "* `db.create.setNodeVectorProperty` for node properties (requires Neo4j 5.13)\n",
    "* `db.create.setRelationshipVectorProperty` for relationship properties (requires Neo4j 5.18)\n",
    "\n",
    "The query looks something like this:\n",
    "\n",
    "```cypher\n",
    "UNWIND $rows as row\n",
    "MATCH (c:Chunk {id: row.id})\n",
    "CALL db.create.setNodeVectorProperty(c, 'embedding', row.embedding)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_embeddings_to_chunks(df: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Write embeddings to chunks.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        A Pandas DataFrame where each row represents a Chunk node.\n",
    "        Has columns `id` and `embedding`.\n",
    "    \"\"\"\n",
    "    graph.execute_write_query_with_data(database=db_info['database'], \n",
    "                              query=processing_queries['write_embeddings_by_chunk_id'],\n",
    "                              data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_df = pd.DataFrame(chunks_with_embeddings, columns=['id', 'embedding'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_embeddings_to_chunks(embeddings_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
