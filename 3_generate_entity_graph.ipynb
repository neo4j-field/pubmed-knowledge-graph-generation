{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PubMed Knowledge Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is part of a series that walks through the process of generating a knowledge graph of PubMed articles.\n",
    "\n",
    "This notebook will\n",
    "* Define an entity data model\n",
    "* Extract entities from the Chunk nodes in a Neo4j graph according to the defined schema\n",
    "* Ingest the entities as defined by the Entity Graph Data Model\n",
    "* Connect entities with their respective Chunk nodes\n",
    "* Connect entities with existing patient journey graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter some Numpy warnings that pop up during ingestion\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import hashlib\n",
    "from math import ceil\n",
    "from typing import Any, Optional, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allows for async operations in notebooks\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pydantic import BaseModel, Field, computed_field, field_validator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Domain Graph Schema Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to define our knowledge graph schema. This information will be passed to the entity extraction LLM to control which entities and relationships are pulled out of the text.\n",
    "\n",
    "This is necessary to prevent our schema from growing too large with an unbounded extraction process.\n",
    "\n",
    "We are using Pydantic to define the schema here since it can be used to validate any returned results as well. This ensures that all data we are ingesting into Neo4j adheres to this structure.\n",
    "\n",
    "Here is what our entity graph data model looks like.\n",
    "\n",
    "<img src=\"./assets/images/entity-data-model.png\" alt=\"entity-data-model\" width=\"600px\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------\n",
    "# Nodes\n",
    "# -------------\n",
    "\n",
    "class Medication(BaseModel):\n",
    "    \"\"\"\n",
    "    A substance used for medical treatment - a medicine or drug. \n",
    "    This is a general representation of a medication. \n",
    "    A Medication node may have relationships to TreatmentArm nodes that are specific to a particular study.\n",
    "    \"\"\"\n",
    "    \n",
    "    name: str = Field(..., description=\"Name of the medication. Should also be uniquely identifiable. Do not include dosage, administration, frequency, or other details.\")\n",
    "    medication_class: str = Field(..., description=\"Drug class (e.g., GLP-1 RA, SGLT2i)\")\n",
    "    mechanism: Optional[str] = Field(None, description=\"Mechanism of action\")\n",
    "    generic_name: Optional[str] = Field(None, description=\"Generic name of the medication\")\n",
    "    brand_names: Optional[List[str]] = Field(None, description=\"Commercial brand names\")\n",
    "    approval_status: Optional[str] = Field(None, description=\"FDA approval status\")\n",
    "    \n",
    "    class Config:\n",
    "        json_schema_extra = {\n",
    "            \"examples\": [\n",
    "                {\n",
    "                    \"name\": \"semaglutide\", \n",
    "                    \"medication_class\": \"GLP-1 receptor agonist\",\n",
    "                    \"mechanism\": \"GLP-1 receptor activation\",\n",
    "                    \"generic_name\": \"semaglutide\",\n",
    "                    \"brand_names\": [\"ozempic\", \"wegovy\", \"rybelsus\"],\n",
    "                    \"approval_status\": \"FDA approved\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "\n",
    "    @field_validator(\"name\", \"medication_class\")\n",
    "    def validate_lower_case(cls, v: str) -> str:\n",
    "        \"\"\"\n",
    "        Validate that the field value is all lower case.\n",
    "        \"\"\"\n",
    "        return v.lower()\n",
    "    \n",
    "    @field_validator(\"generic_name\")\n",
    "    def validate_generic_name(cls, v: str | None) -> str | None:\n",
    "        \"\"\"\n",
    "        Validate that the generic name is all lower case.\n",
    "        \"\"\"\n",
    "        if v is not None:\n",
    "            return v.lower()\n",
    "        return v\n",
    "    \n",
    "    @field_validator(\"brand_names\")\n",
    "    def validate_brand_names(cls, v: list[str] | None) -> list[str] | None:\n",
    "        \"\"\"\n",
    "        Validate that the brand names are all lower case.\n",
    "        \"\"\"\n",
    "        if v is not None:\n",
    "            return [name.lower() for name in v]\n",
    "        return v\n",
    "\n",
    "\n",
    "class TreatmentArm(BaseModel):\n",
    "    \"\"\"\n",
    "    A treatment arm is an explicit instance of a participant group in a study that receive the same treatment.\n",
    "    A treatment arm should have relationships to Medication and ClinicalOutcome nodes.\n",
    "    \"\"\"\n",
    "    study_name: str = Field(..., description=\"Name of the study. This is used to uniquely identify the TreatmentArm node.\")\n",
    "    name: str = Field(..., description=\"Name of the treatment arm\")\n",
    "\n",
    "    class Config:\n",
    "        json_schema_extra = {\n",
    "            \"examples\": [\n",
    "                {\n",
    "                    \"study_name\": \"Study 1\",\n",
    "                    \"name\": \"Treatment arm 1\",\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "\n",
    "    @computed_field(return_type=str)\n",
    "    def treatment_arm_id(self) -> str:\n",
    "        \"\"\"\n",
    "        The unique id of the treatment arm.\n",
    "        This is a sha256 hash of the study name and treatment arm name.\n",
    "        \"\"\"\n",
    "        return hashlib.sha256(f\"{self.study_name}_{self.name}\".encode()).hexdigest()\n",
    "    \n",
    "\n",
    "class ClinicalOutcome(BaseModel):\n",
    "    \"\"\"\n",
    "    A clinical outcome of a treatment arm.\n",
    "    This describes the resulting effect a treatment has on a treatment arm population.\n",
    "    \"\"\"\n",
    "    \n",
    "    study_name: str = Field(..., description=\"Name of the study this outcome is associated with. This is used to uniquely identify the ClinicalOutcome node.\")\n",
    "    name: str = Field(..., description=\"Name of the clinical outcome.\")\n",
    "\n",
    "    @computed_field(return_type=str)\n",
    "    def clinical_outcome_id(self) -> str:\n",
    "        \"\"\"\n",
    "        The unique id of the clinical outcome.\n",
    "        This is a sha256 hash of the study name and the name of the outcome.\n",
    "        \"\"\"\n",
    "        return hashlib.sha256(f\"{self.study_name}_{self.name}\".encode()).hexdigest()\n",
    "    \n",
    "    class Config:\n",
    "        json_schema_extra = {\n",
    "            \"examples\": [\n",
    "                # don't include the clinical_outcome_id in the example since this is computed from extracted fields\n",
    "                {\n",
    "                    \"study_name\": \"Study 1\",\n",
    "                    \"name\": \"A1C controlled\",\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "\n",
    "\n",
    "class MedicalCondition(BaseModel):\n",
    "    \"\"\"Medical conditions and comorbidities studied\"\"\"\n",
    "    \n",
    "    name: str = Field(..., description=\"Name of the medical condition\")\n",
    "    category: str = Field(..., description=\"Category of condition\")\n",
    "    icd10_code: Optional[str] = Field(None, description=\"ICD-10 code when available\")\n",
    "    \n",
    "    @field_validator(\"icd10_code\")\n",
    "    def validate_icd10_code(cls, v: str | None) -> str | None:\n",
    "        \"\"\"\n",
    "        Validate that the ICD-10 code is valid.\n",
    "        \"\"\"\n",
    "        \n",
    "        if v is None:\n",
    "            return v\n",
    "\n",
    "        # ICD-10 codes are 3-7 characters long\n",
    "        if len(v) < 3 or len(v) > 7:\n",
    "            raise ValueError(\"ICD-10 code must be between 3 and 7 characters long.\")\n",
    "        # first character must be a letter\n",
    "        elif not v[0].isalpha():\n",
    "            raise ValueError(\"ICD-10 code must start with a letter.\")\n",
    "        # first character not case sensitive, can't be U, O, or I\n",
    "        elif v[0].upper() in [\"U\", \"O\", \"I\"]:\n",
    "            raise ValueError(\"ICD-10 code can not start with 'U', 'O', or 'I'.\")\n",
    "        # second character must be a digit\n",
    "        elif not v[1].isdigit():\n",
    "            raise ValueError(\"ICD-10 code second character must be a digit.\")\n",
    "        # '.' must separate the first 3 characters from the rest of the code\n",
    "        # examples:\n",
    "        # S52 Fracture of forearm\n",
    "        # S52.5 Fracture of lower end of radius\n",
    "        # S52.52 Torus fracture of lower end of radius\n",
    "        # S52.521 Torus fracture of lower end of right radius\n",
    "        # S52.521A Torus fracture of lower end of right radius, initial encounter, closed fracture\n",
    "        elif len(v) > 3 and not v[3] == '.':\n",
    "            raise ValueError(\"ICD-10 code must have a '.' after the first 3 characters.\")\n",
    "        return v\n",
    "    \n",
    "    class Config:\n",
    "        json_schema_extra = {\n",
    "            \"examples\": [\n",
    "                {\n",
    "                    \"name\": \"Type 2 diabetes mellitus\",\n",
    "                    \"category\": \"diabetes\",\n",
    "                    \"icd10_code\": \"E11\",\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "\n",
    "\n",
    "class StudyPopulation(BaseModel):\n",
    "    \"\"\"Patient populations and demographics in research studies\"\"\"\n",
    "    \n",
    "    study_name: str = Field(..., description=\"Name of the study. This is used to uniquely identify the StudyPopulation node.\")\n",
    "    description: str = Field(..., description=\"Description of the population\")\n",
    "    min_age: Optional[int] = Field(None, description=\"Minimum age in years\")\n",
    "    max_age: Optional[int] = Field(None, description=\"Maximum age in years\")\n",
    "    male_percentage: Optional[float] = Field(None, description=\"Percentage of male gender participants\")\n",
    "    female_percentage: Optional[float] = Field(None, description=\"Percentage of female gender participants\")\n",
    "    other_gender_percentage: Optional[float] = Field(None, description=\"Percentage of participants that identify as another gender\")\n",
    "    sample_size: Optional[int] = Field(None, description=\"Number of participants\")\n",
    "    study_type: str = Field(..., description=\"Type of study\")\n",
    "    inclusion_criteria: Optional[List[str]] = Field(None, description=\"Key inclusion criteria\")\n",
    "    exclusion_criteria: Optional[List[str]] = Field(None, description=\"Key exclusion criteria\")\n",
    "    study_duration: Optional[str] = Field(None, description=\"Duration of study\")\n",
    "    \n",
    "    class Config:\n",
    "        json_schema_extra = {\n",
    "            \"examples\": [\n",
    "                {\n",
    "                    \"study_name\": \"Study 1\",\n",
    "                    \"description\": \"Adults with T2DM and schizophrenia\",\n",
    "                    \"min_age\": 30,\n",
    "                    \"max_age\": 39,\n",
    "                    \"male_percentage\": 46.0,\n",
    "                    \"female_percentage\": 53.0,\n",
    "                    \"other_gender_percentage\": 1.0,\n",
    "                    \"sample_size\": 100,\n",
    "                    \"study_type\": \"Observational study\",\n",
    "                    \"inclusion_criteria\": [\"Type 2 diabetes diagnosis\", \"Schizophrenia diagnosis\", \"Age ≥18\"],\n",
    "                    \"study_duration\": \"12 months\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "\n",
    "    @computed_field(return_type=str)\n",
    "    def study_population_id(self) -> str:\n",
    "        \"\"\"\n",
    "        The unique id of the study population.\n",
    "        This is a sha256 hash of the study name.\n",
    "        \"\"\"\n",
    "        return hashlib.sha256(f\"{self.study_name}_{self.description}\".encode()).hexdigest()\n",
    "\n",
    "\n",
    "# -------------\n",
    "# Relationships\n",
    "# -------------\n",
    "\n",
    "class MedicationUsedInTreatmentArm(BaseModel):\n",
    "    \"\"\"\n",
    "    Study-specific medication usage - how a Medication was used in a particular TreatmentArm.\n",
    "    This describes an instance of a medication that is used in a particular treatment arm. \n",
    "    All treatment arms should have a relationship with at least one Medication node.\n",
    "    \"\"\"\n",
    "    study_name: str = Field(..., description=\"Name of the study.\")\n",
    "    treatment_arm_name: str = Field(..., description=\"Name of the treatment arm.\")\n",
    "    medication_name: str = Field(..., description=\"Name of the medication.\")\n",
    "    dosage: Optional[str] = Field(None, description=\"Dosage used in this study\")\n",
    "    route: Optional[str] = Field(None, description=\"Route of administration\")\n",
    "    frequency: Optional[str] = Field(None, description=\"Dosing frequency\")\n",
    "    treatment_duration: Optional[str] = Field(None, description=\"Duration of treatment\")\n",
    "    comparator: Optional[str] = Field(None, description=\"What this was compared against\")\n",
    "    adherence_rate: Optional[float] = Field(None, description=\"Treatment adherence rate\")\n",
    "    formulation: Optional[str] = Field(None, description=\"Specific formulation used\")\n",
    "\n",
    "    @computed_field(return_type=str)\n",
    "    def treatment_arm_id(self) -> str:\n",
    "        \"\"\"\n",
    "        The unique id of the treatment arm.\n",
    "        This is a sha256 hash of the study name and treatment arm.\n",
    "        \"\"\"\n",
    "        return hashlib.sha256(f\"{self.study_name}_{self.treatment_arm_name}\".encode()).hexdigest()\n",
    "    \n",
    "    @field_validator(\"medication_name\")\n",
    "    def validate_medication_name(cls, v: str) -> str:\n",
    "        \"\"\"\n",
    "        Validate that the medication name is all lower case.\n",
    "        \"\"\"\n",
    "        return v.lower()\n",
    "    \n",
    "    class Config:\n",
    "        json_schema_extra = {\n",
    "            # don't include the study_medication_id in the example since this is computed from extracted fields\n",
    "            \"examples\": [\n",
    "                {\n",
    "                    \"study_name\": \"Study 1\",\n",
    "                    \"treatment_arm_name\": \"Treatment arm 1\",\n",
    "                    \"medication_name\": \"Medication 1\",\n",
    "                    \"dosage\": \"1.0 mg\",\n",
    "                    \"route\": \"subcutaneous\",\n",
    "                    \"frequency\": \"weekly\",\n",
    "                    \"treatment_duration\": \"12 weeks\",\n",
    "                    \"comparator\": \"placebo\",\n",
    "                    \"adherence_rate\": 85.5,\n",
    "                    \"formulation\": \"pre-filled pen\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "\n",
    "\n",
    "class TreatmentArmHasClinicalOutcome(BaseModel):\n",
    "    \"\"\"\n",
    "    Links TreatmentArm to ClinicalOutcome nodes.\n",
    "    TreatmentArm nodes should have a relationship with a ClinicalOutcome node.\n",
    "    Pattern: (:TreatmentArm)-[:HAS_CLINICAL_OUTCOME]->(:ClinicalOutcome)\n",
    "    \"\"\"\n",
    "    study_name: str = Field(..., description=\"Name of the study. This is used to uniquely identify the TreatmentArm node.\")\n",
    "    treatment_arm_name: str = Field(..., description=\"Name of the treatment arm.\")\n",
    "    clinical_outcome_name: str = Field(..., description=\"Name of the clinical outcome.\")\n",
    "\n",
    "    @computed_field(return_type=str)\n",
    "    def clinical_outcome_id(self) -> str:\n",
    "        \"\"\"\n",
    "        The unique id of the clinical outcome.\n",
    "        This is a sha256 hash of the study name and the name of the outcome.\n",
    "        \"\"\"\n",
    "        return hashlib.sha256(f\"{self.study_name}_{self.clinical_outcome_name}\".encode()).hexdigest()\n",
    "    \n",
    "    @computed_field(return_type=str)\n",
    "    def treatment_arm_id(self) -> str:\n",
    "        \"\"\"\n",
    "        The unique id of the treatment arm.\n",
    "        This is a sha256 hash of the study name and treatment arm.\n",
    "        \"\"\"\n",
    "        return hashlib.sha256(f\"{self.study_name}_{self.treatment_arm_name}\".encode()).hexdigest()\n",
    "\n",
    "\n",
    "class StudyPopulationHasMedicalCondition(BaseModel):\n",
    "    \"\"\"\n",
    "    Links StudyPopulation to MedicalCondition nodes.\n",
    "    StudyPopulation nodes should have a relationship with a MedicalCondition node.\n",
    "    Pattern: (:StudyPopulation)-[:HAS_MEDICAL_CONDITION]->(:MedicalCondition)\n",
    "    \"\"\"\n",
    "    study_name: str = Field(..., description=\"Name of the study. This is used to uniquely identify the StudyPopulation node.\")\n",
    "    study_population_description: str = Field(..., description=\"Description of the study population.\")\n",
    "    medical_condition_name: str\n",
    "\n",
    "    @computed_field(return_type=str)\n",
    "    def study_population_id(self) -> str:\n",
    "        \"\"\"\n",
    "        The unique id of the study population.\n",
    "        This is a sha256 hash of the study name and population description.\n",
    "        \"\"\"\n",
    "        return hashlib.sha256(f\"{self.study_name}_{self.study_population_description}\".encode()).hexdigest()\n",
    "\n",
    "\n",
    "class StudyPopulationInTreatmentArm(BaseModel):\n",
    "    \"\"\"\n",
    "    Links StudyPopulation to TreatmentArm nodes.\n",
    "    StudyPopulation nodes should have a relationship with a TreatmentArm node.\n",
    "    Pattern: (:StudyPopulation)-[:IN_TREATMENT_ARM]->(:TreatmentArm)\n",
    "    \"\"\"\n",
    "    study_name: str = Field(..., description=\"Name of the study. This is used to uniquely identify the StudyPopulation node.\")\n",
    "    study_population_description: str = Field(..., description=\"Description of the study population.\")\n",
    "    treatment_arm_name: str = Field(..., description=\"Name of the treatment arm.\")\n",
    "\n",
    "    @computed_field(return_type=str)\n",
    "    def treatment_arm_id(self) -> str:\n",
    "        \"\"\"\n",
    "        The unique id of the treatment arm.\n",
    "        This is a sha256 hash of the study name and treatment arm.\n",
    "        \"\"\"\n",
    "        return hashlib.sha256(f\"{self.study_name}_{self.treatment_arm_name}\".encode()).hexdigest()\n",
    "\n",
    "    @computed_field(return_type=str)\n",
    "    def study_population_id(self) -> str:\n",
    "        \"\"\"\n",
    "        The unique id of the study population.\n",
    "        This is a sha256 hash of the study name and population description.\n",
    "        \"\"\"\n",
    "        return hashlib.sha256(f\"{self.study_name}_{self.study_population_description}\".encode()).hexdigest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity Extraction via LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using [OpenAI](https://platform.openai.com/docs/overview) and the [Instructor](https://python.useinstructor.com/) library to perform our entity extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AsyncOpenAI\n",
    "import instructor\n",
    "from instructor.exceptions import IncompleteOutputException, InstructorRetryException, ValidationError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructor handles requesting structured outputs from the LLM. \n",
    "\n",
    "If the LLM fails to return output that adheres to the response models, Instructor will also handle the retry logic and pass any errors to inform corrections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = instructor.from_openai(AsyncOpenAI())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the system prompt defines the overall behavior of the LLM\n",
    "system_prompt = \"\"\"You are a healthcare research expert that is responsible for extracting detailed entities from PubMed articles. \n",
    "You will be provided a graph data model schema and must extract entities and relationships to populate a knowledge graph.\"\"\"\n",
    "\n",
    "user_prompt = \"\"\"Rules:\n",
    "* Use the provided schema to extract entities and relationships from the provided text.\n",
    "* Follow the schema desciptions strictly.\n",
    "* IMPORTANT: For each relationship you extract, ensure you ALSO extract the individual entity nodes it connects.\n",
    "* If a field is not provided, do not include it in the response. It should be null.\n",
    "* If no entities are found, return an empty list\n",
    "\n",
    "Text Chunk:\n",
    "{text_chunk}\"\"\"\n",
    "\n",
    "class ResponseModel(BaseModel):\n",
    "    \"\"\"\n",
    "    The response model for the extracted entities and relationships.\n",
    "    \"\"\"\n",
    "    clinical_outcome: list[ClinicalOutcome] = Field(default_factory=list, description=\"The clinical outcomes in the chunk\")\n",
    "    treatment_arm: list[TreatmentArm] = Field(default_factory=list, description=\"The treatment arms in the chunk\")\n",
    "    study_population: list[StudyPopulation] = Field(default_factory=list, description=\"The study populations in the chunk\")\n",
    "    medical_condition: list[MedicalCondition] = Field(default_factory=list, description=\"The medical conditions in the chunk\")\n",
    "    medication: list[Medication] = Field(default_factory=list, description=\"The medications in the chunk\")\n",
    "    medication_used_in_treatment_arm: list[MedicationUsedInTreatmentArm] = Field(default_factory=list, description=\"The medications used in treatment arms in the chunk\")\n",
    "    treatment_arm_has_clinical_outcome: list[TreatmentArmHasClinicalOutcome] = Field(default_factory=list, description=\"The treatment arms that have clinical outcomes in the chunk\")\n",
    "    study_population_has_medical_condition: list[StudyPopulationHasMedicalCondition] = Field(default_factory=list, description=\"The study populations that have medical conditions in the chunk\")\n",
    "    study_population_in_treatment_arm: list[StudyPopulationInTreatmentArm] = Field(default_factory=list, description=\"The study populations that are in treatment arms in the chunk\")\n",
    "\n",
    "\n",
    "async def extract_entities_from_text_chunk(text_chunk: str, chunk_id: str, failed_cache: list[tuple[str, str]]) -> list:\n",
    "    \"\"\"\n",
    "    Extract entities and relationships from a text chunk.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text_chunk : str\n",
    "        The text chunk to extract entities from.\n",
    "    chunk_id : str\n",
    "        The id of the text chunk. Used for debugging.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ResponseModel\n",
    "        An object containing lists of entities and relationships extracted from the text chunk.\n",
    "        If the response is truncated, an empty list is returned.\n",
    "        If retries are exhausted, an empty list is returned.\n",
    "        If the response is invalid, an empty list is returned.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        response = await client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt.format(text_chunk=text_chunk)}\n",
    "            ],\n",
    "            response_model=ResponseModel,\n",
    "            temperature=0.0\n",
    "        )\n",
    "        return response\n",
    "    except IncompleteOutputException as e:\n",
    "        # Handle truncated output\n",
    "        print(f\"* Response output truncated. Skipping chunk {chunk_id}.\")\n",
    "        failed_cache.append((chunk_id, text_chunk))\n",
    "        return None\n",
    "    except InstructorRetryException as e:\n",
    "        # Handle retry exhaustion\n",
    "        print(f\"* Failed after {e.n_attempts} attempts. Skipping chunk {chunk_id}.\")\n",
    "        failed_cache.append((chunk_id, text_chunk))\n",
    "        return None\n",
    "    except ValidationError as e:\n",
    "        # Handle validation errors\n",
    "        print(f\"* Validation failed. Skipping chunk {chunk_id}.\\nError: {e}\")\n",
    "        failed_cache.append((chunk_id, text_chunk))\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def extract_entities_from_chunk_nodes(chunk_nodes_dataframe: pd.DataFrame, batch_size: int = 100) -> list[tuple[str, list[Any]]]:\n",
    "    \"\"\"\n",
    "    Process a Pandas DataFrame of Chunk nodes and return the entities found in each chunk.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    chunk_nodes_dataframe : pd.DataFrame\n",
    "        A Pandas DataFrame where each row represents a Chunk node.\n",
    "        Has columns `id` and `text`.\n",
    "    batch_size : int\n",
    "        The number of text chunks to process in each batch.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list[tuple[str, list[dict[str, Any]]]]\n",
    "        A list of tuples, where the first element is the chunk id and the second element is a list of entities found in the chunk.\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    async def _extract_batch(batch: pd.DataFrame, failed_cache: list[tuple[str, str]]) -> list[tuple[str, list[dict[str, Any]]]]:\n",
    "        \"\"\"\n",
    "        Extract entities from a batch of text chunks.\n",
    "        Failed extractions are maintained in the `failed_cache` list that is passed to the extraction function.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch : pd.DataFrame\n",
    "            A Pandas DataFrame where each row represents a text chunk.\n",
    "            Has columns `id` and `text`.\n",
    "        failed_cache : list[tuple[str, str]]\n",
    "            A list of tuples, where the first element is the chunk id and the second element is the text chunk.\n",
    "            This is used to log failed extractions across batches.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        list[tuple[str, list[dict[str, Any]]]]\n",
    "            A list of tuples, where the first element is the chunk id and the second element is a list of entities found in the chunk.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Create tasks for all nodes in the batch\n",
    "        # order is maintained\n",
    "        tasks = [extract_entities_from_text_chunk(row[\"text\"], row['id'], failed_cache) for _, row in batch.iterrows()]\n",
    "        # Execute all tasks concurrently\n",
    "        extraction_results = await asyncio.gather(*tasks)\n",
    "\n",
    "        # filter results to only include non-None values\n",
    "        extraction_results = [(id, entities) for id, entities in zip(batch[\"id\"], extraction_results) if entities is not None]\n",
    "\n",
    "        return extraction_results\n",
    "\n",
    "    \n",
    "    async def _extract_in_batches(chunk_nodes_dataframe: pd.DataFrame, batch_size: int) -> tuple[list[tuple[str, list[dict[str, Any]]]], list[tuple[str, str]]]:\n",
    "        \"\"\"\n",
    "        Extract entities from a Pandas DataFrame of text chunks in batches.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        chunk_nodes_dataframe : pd.DataFrame\n",
    "            A Pandas DataFrame where each row represents a text chunk.\n",
    "            Has columns `id` and `text`.\n",
    "        batch_size : int\n",
    "            The number of text chunks to process in each batch.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tuple[list[tuple[str, list[dict[str, Any]]]], list[tuple[str, str]]]\n",
    "            A tuple of two lists. The first list contains tuples of chunk id and list of entities found in the chunk.\n",
    "            The second list contains tuples of chunk id and text chunk that failed to be processed.\n",
    "        \"\"\"\n",
    "\n",
    "        results = list()\n",
    "        failed_cache: list[tuple[str, str]] = list() # [(chunk_id, text_chunk), ...]\n",
    "        for batch_idx, i in enumerate(range(0, len(chunk_nodes_dataframe), batch_size)):\n",
    "            print(f\"Processing batch {batch_idx+1} of {ceil(len(chunk_nodes_dataframe)/(batch_size))}  \\n\", end=\"\\r\") \n",
    "            if i + batch_size >= len(chunk_nodes_dataframe):\n",
    "                batch = chunk_nodes_dataframe.iloc[i:]\n",
    "            else:\n",
    "                batch = chunk_nodes_dataframe.iloc[i:i+batch_size]\n",
    "            batch_results = await _extract_batch(batch, failed_cache)\n",
    "\n",
    "            # Add extracted records to the results list\n",
    "            results.extend(batch_results)\n",
    "\n",
    "        return results, failed_cache\n",
    "\n",
    "    # first pass through chunks\n",
    "    results, failed = await _extract_in_batches(chunk_nodes_dataframe, batch_size)\n",
    "    print(f\"Successful chunks : {len(results)}\")\n",
    "    print(f\"Failed chunks     : {len(failed)}\")\n",
    "    print(\"--------------------------------\")\n",
    "    print(\"Retrying failed chunks...\")\n",
    "\n",
    "    # retry failed chunks once\n",
    "    retry_df = pd.DataFrame(failed, columns=[\"id\", \"text\"])\n",
    "    retry_results, failed = await _extract_in_batches(retry_df, batch_size)\n",
    "    print(f\"Successful retries : {len(retry_results)}\")\n",
    "    print(f\"Failed retries     : {len(failed)}\")\n",
    "\n",
    "    print(\"--------------------------------\")\n",
    "    print(f\"Overall Success Rate : {round(len(results + retry_results) / len(chunk_nodes_dataframe) * 100, 2)}%\")\n",
    "\n",
    "    return results + retry_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now defined \n",
    "* Entity data model\n",
    "* Entity extraction logic for chunks\n",
    "\n",
    "It is now time to define our ingestion logic. We will run ingest in two stages \n",
    "\n",
    "1. Extract Domain / Entity Graph from lexical graph Chunk nodes\n",
    "2. Ingest entities into Graph\n",
    "\n",
    "Decoupling these stages allows us easily make changes as we iterate our ingestion process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from pyneoinstance import Neo4jInstance, load_yaml_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our database credentials and all of our queries are stored in the `pyneoinstance_config.yaml` file. \n",
    "\n",
    "This makes it easy to manage our queries and keeps the notebook code clean. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_yaml_file(\"pyneoinstance_config.yaml\")\n",
    "\n",
    "db_info = config['db_info']\n",
    "\n",
    "constraints = config['initializing_queries']['constraints']\n",
    "indexes = config['initializing_queries']['indexes']\n",
    "\n",
    "node_load_queries = config['loading_queries']['nodes']\n",
    "relationship_load_queries = config['loading_queries']['relationships']\n",
    "\n",
    "processing_queries = config['processing_queries']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This graph object will handle database connections and read / write transactions for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = Neo4jInstance(db_info.get('uri', os.getenv(\"NEO4J_URI\", \"neo4j://localhost:7687\")), # use config value -> use env value -> use default value\n",
    "                      db_info.get('user', os.getenv(\"NEO4J_USER\", \"neo4j\")), \n",
    "                      db_info.get('password', os.getenv(\"NEO4J_PASSWORD\", \"password\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a helper function for ingesting data using the PyNeoInstance library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_partition(data: pd.DataFrame, batch_size: int = 500) -> int:\n",
    "    \"\"\"\n",
    "    Determine the data partition based on the desired batch size.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : pd.DataFrame\n",
    "        The Pandas DataFrame to partition.\n",
    "    batch_size : int\n",
    "        The desired batch size.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        The partition size.\n",
    "    \"\"\"\n",
    "    \n",
    "    partition = int(len(data) / batch_size)\n",
    "    print(\"partition: \"+str(partition if partition > 1 else 1))\n",
    "    return partition if partition > 1 else 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we write all the constraints and indexes we need for both the lexical and domain graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_constraints_and_indexes() -> None:\n",
    "    \"\"\"\n",
    "    Create constraints and indexes for the lexical and domain graphs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if constraints and len(constraints) > 0:\n",
    "            graph.execute_write_queries(database=db_info['database'], queries=list(constraints.values()))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    try:\n",
    "        if indexes and len(indexes) > 0:\n",
    "            graph.execute_write_queries(database=db_info['database'], queries=list(indexes.values()))\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_constraints_and_indexes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Entities from Lexical Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now perform entity extraction on the Chunk nodes to augment and connect to our patient journey graph.\n",
    "\n",
    "First we need to gather the chunk nodes to extract entities from. \n",
    "\n",
    "This is done by finding all chunk nodes in the database that do not have a `HAS_ENTITY` relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunk_nodes_to_process(min_length: int = 100) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Retrieve Chunk node id and text from the database that don't have an embedding.\n",
    "    These chunks may then be used as input to the entity extraction process.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    min_length : int\n",
    "        The minimum length the text must be to be included in the DataFrame.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A Pandas DataFrame where each row represents a Chunk node that has text and is at least `min_length` characters long.\n",
    "        Has columns `id` and `text`.\n",
    "    \"\"\"\n",
    "    return graph.execute_read_query(database=db_info['database'], \n",
    "                            query=processing_queries['get_chunk_nodes_to_extract_entities'], \n",
    "                            parameters={\"min_length\": min_length},\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownPropertyKeyWarning} {category: UNRECOGNIZED} {title: The provided property key is not in the database} {description: One of the property names in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing property name is: embedding)} {position: line: 3, column: 9, offset: 49} for query: 'MATCH (c:Chunk)\\nWHERE c.text IS NOT NULL\\n  AND c.embedding IS NULL\\n  AND size(c.text) >= $min_length\\nRETURN c.id as id, c.text as text\\n'\n"
     ]
    }
   ],
   "source": [
    "chunks_to_process = get_chunk_nodes_to_process(min_length=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 494 chunks to process\n",
      "\n",
      "First chunk:\n",
      "\n",
      "statistical significance (metformin + Ex9-39 vs. placebo + Ex9-39, P = 0.053). The glucose iAUC after metformin + saline was significantly smaller than the iAUC for metformin + Ex9-39 (P = 0.004). Based on individual iAUC values, the relative contribution of GLP-1 to the acute glucose-lowering effect of metformin was 75% ± 35%, calculated as follows: 100% × ([iAUCplacebo + saline – iAUCmetformin + saline] – [iAUCplacebo + Ex9–39 – iAUCmetformin + Ex9–39])/(iAUCplacebo + saline – iAUCmetformin + saline) (P = 0.05). Using a 2-way ANOVA, both metformin and Ex9-39 were shown to significantly affect postprandial plasma glucose (iAUC) (P = 0.005 and P = 0.002, respectively), but no interaction between the 2 factors was evident. The time courses of the C-peptide/glucose ratios are illustrated in Figure 2B, and the AUCs for C-peptide/glucose, insulin/glucose, and insulin secretion\n"
     ]
    }
   ],
   "source": [
    "print(f\"Found {len(chunks_to_process)} chunks to process\\n\")\n",
    "print(f\"First chunk:\\n\\n{chunks_to_process.loc[0,'text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now execute the entity extraction function.\n",
    "\n",
    "The success rate here is defined as the percent of chunks that did not fail, even if no entities were extracted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 1 of 2  \n",
      "* Failed after 3 attempts. Skipping chunk 0cb20654f80cf1f2555a19dc6e4053fb.\n",
      "* Failed after 3 attempts. Skipping chunk 09f614ab8b34bc560fcff074fc18c66b.\n",
      "* Failed after 3 attempts. Skipping chunk 0ae41eb129a57859103e320c0cafe3d7.\n",
      "* Failed after 3 attempts. Skipping chunk 027e9b03adf9e1f31148048ec58ea1bf.\n",
      "Processing batch 2 of 2  \n",
      "* Failed after 3 attempts. Skipping chunk 108097d1207b409cc062bb0e85364cbe.\n",
      "* Failed after 3 attempts. Skipping chunk 0e294557cd2b203ae3fb600c43667cf2.\n",
      "Successful chunks : 34\n",
      "Failed chunks     : 6\n",
      "--------------------------------\n",
      "Retrying failed chunks...\n",
      "Processing batch 1 of 0  \n",
      "* Failed after 3 attempts. Skipping chunk 09f614ab8b34bc560fcff074fc18c66b.\n",
      "* Failed after 3 attempts. Skipping chunk 108097d1207b409cc062bb0e85364cbe.\n",
      "* Failed after 3 attempts. Skipping chunk 0cb20654f80cf1f2555a19dc6e4053fb.\n",
      "* Failed after 3 attempts. Skipping chunk 0ae41eb129a57859103e320c0cafe3d7.\n",
      "* Failed after 3 attempts. Skipping chunk 027e9b03adf9e1f31148048ec58ea1bf.\n",
      "* Failed after 3 attempts. Skipping chunk 0e294557cd2b203ae3fb600c43667cf2.\n",
      "Successful retries : 0\n",
      "Failed retries     : 6\n",
      "--------------------------------\n",
      "Overall Success Rate : 85.0%\n"
     ]
    }
   ],
   "source": [
    "extracted_entities_with_chunk_ids = await extract_entities_from_chunk_nodes(chunks_to_process[:40], batch_size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingest Entities Into Knowledge Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will take the extracted entities contained in the `ResponseModel` and parse them into Pandas DataFrames for each node and relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENTITY_LABELS = {\n",
    "    \"medication\", \n",
    "    \"treatment_arm\",\n",
    "    \"medical_condition\",\n",
    "    \"study_population\",\n",
    "    \"clinical_outcome\",\n",
    "}\n",
    "\n",
    "ENTITY_RELS = {\n",
    "    \"medication_used_in_treatment_arm\",\n",
    "    \"treatment_arm_has_clinical_outcome\",\n",
    "    \"study_population_in_treatment_arm\",\n",
    "    \"study_population_has_medical_condition\",\n",
    "}\n",
    "\n",
    "def prepare_entities_for_ingestion_from_response_models(response_models: tuple[str, list[ResponseModel]]) -> dict[str, dict[str, pd.DataFrame]]:\n",
    "    \"\"\"\n",
    "    This function will take the extracted entities contained in the `ResponseModel` and parse them into Pandas DataFrames for each node and relationship.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    response_models : tuple[str, list[ResponseModel]]\n",
    "        A tuple containing the chunk id and the response model.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict[str, dict[str, pd.DataFrame]]\n",
    "        A dictionary containing the node and relationship Pandas DataFrames for ingestion into the knowledge graph.\n",
    "    \"\"\"\n",
    "    \n",
    "    records_node_dict = {lbl: list() for lbl in ENTITY_LABELS}\n",
    "    records_rel_dict = {lbl: list() for lbl in ENTITY_RELS}\n",
    "\n",
    "    for chunk_id, response_model in response_models:\n",
    "\n",
    "\n",
    "        for label in ENTITY_LABELS:\n",
    "            to_add = [x.model_dump() for x in response_model.__getattribute__(label)]\n",
    "            [x.update({\"chunk_id\": chunk_id}) for x in to_add]\n",
    "            records_node_dict[label].extend(to_add)\n",
    "        \n",
    "        for rel in ENTITY_RELS:\n",
    "            to_add = [x.model_dump() for x in response_model.__getattribute__(rel)]\n",
    "            [x.update({\"chunk_id\": chunk_id}) for x in to_add]\n",
    "            records_rel_dict[rel].extend(to_add)\n",
    "\n",
    "    for key, value in records_node_dict.items():\n",
    "        records_node_dict[key] = pd.DataFrame(value).replace({float('nan'): None})\n",
    "\n",
    "    for key, value in records_rel_dict.items():\n",
    "        records_rel_dict[key] = pd.DataFrame(value).replace({float('nan'): None})\n",
    "\n",
    "    return {\"nodes\": records_node_dict, \"relationships\": records_rel_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_entity_nodes(medication_dataframe: pd.DataFrame, \n",
    "                      medical_condition_dataframe: pd.DataFrame, \n",
    "                      treatment_arm_dataframe: pd.DataFrame, \n",
    "                      study_population_dataframe: pd.DataFrame, \n",
    "                      clinical_outcome_dataframe: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Load entity nodes into the graph.\n",
    "    \"\"\"\n",
    "    \n",
    "    entity_nodes_ingest_iterator = list(zip([medication_dataframe, \n",
    "                                             medical_condition_dataframe, \n",
    "                                             treatment_arm_dataframe, \n",
    "                                             study_population_dataframe, \n",
    "                                             clinical_outcome_dataframe], \n",
    "                                             ['medication', \n",
    "                                              'medical_condition', \n",
    "                                              'treatment_arm', \n",
    "                                              'study_population', \n",
    "                                              'clinical_outcome']))\n",
    "\n",
    "    for data, query in entity_nodes_ingest_iterator:\n",
    "        if len(data) > 0:\n",
    "            print(f\"Loading {len(data)} {query} nodes\")\n",
    "            res = graph.execute_write_query_with_data(database=db_info['database'], \n",
    "                                                    data=data, \n",
    "                                                    query=node_load_queries[query], \n",
    "                                                    partitions=get_partition(data, batch_size=500),\n",
    "                                                    parallel=False)\n",
    "            print(res)\n",
    "        else:\n",
    "            print(f\"No {query} nodes to load\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_entity_relationships(medication_used_in_treatment_arm_dataframe: pd.DataFrame,\n",
    "                              treatment_arm_has_clinical_outcome_dataframe: pd.DataFrame,\n",
    "                              study_population_in_treatment_arm_dataframe: pd.DataFrame,\n",
    "                              study_population_has_medical_condition_dataframe: pd.DataFrame,\n",
    "                              ) -> None:\n",
    "    \"\"\"\n",
    "    Load entity relationships into the graph.\n",
    "    \"\"\"\n",
    "    entity_relationships_ingest_iterator = list(zip([medication_used_in_treatment_arm_dataframe, \n",
    "                                                      treatment_arm_has_clinical_outcome_dataframe, \n",
    "                                                      study_population_in_treatment_arm_dataframe, \n",
    "                                                      study_population_has_medical_condition_dataframe, \n",
    "                                                      ], \n",
    "                                                      ['medication_used_in_treatment_arm', \n",
    "                                                       'treatment_arm_has_clinical_outcome', \n",
    "                                                       'study_population_in_treatment_arm', \n",
    "                                                       'study_population_has_medical_condition', \n",
    "                                                       ]))\n",
    "    \n",
    "    for data, query in entity_relationships_ingest_iterator:\n",
    "        if len(data) > 0:\n",
    "            print(f\"Loading {len(data)} {query} relationships\")\n",
    "            res = graph.execute_write_query_with_data(database=db_info['database'], \n",
    "                                                    data=data, \n",
    "                                                    query=relationship_load_queries[query], \n",
    "                                                    partitions=get_partition(data, batch_size=500),\n",
    "                                                    parallel=False)\n",
    "            print(res)\n",
    "        else:\n",
    "            print(f\"No {query} relationships to load\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lexical and domain knowledge graphs will be linked with `HAS_ENTITY` relationships between Chunk nodes and domain graph nodes. \n",
    "\n",
    "This will adhere to the data model defined in the ['Lexical Graph with Extracted Entities'](https://graphrag.com/reference/knowledge-graph/lexical-graph-extracted-entities/) section of [graphrag.com](graphrag.com).\n",
    "\n",
    "\n",
    "This is the combined lexical and domain graph data model.\n",
    "\n",
    "<img src=\"./assets/images/lexical-entity-data-model-h.png\" alt=\"lexical-entity-data-model\" height=\"700px\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def link_entities_to_chunks(medication_link_dataframe: pd.DataFrame, \n",
    "                      medical_condition_link_dataframe: pd.DataFrame, \n",
    "                      treatment_arm_link_dataframe: pd.DataFrame, \n",
    "                      study_population_link_dataframe: pd.DataFrame, \n",
    "                      clinical_outcome_link_dataframe: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Link entities to chunks.\n",
    "    \"\"\"\n",
    "    entity_link_iterator = list(zip([medication_link_dataframe, \n",
    "                                     medical_condition_link_dataframe, \n",
    "                                     treatment_arm_link_dataframe, \n",
    "                                     study_population_link_dataframe, \n",
    "                                     clinical_outcome_link_dataframe], \n",
    "                                     [\"chunk_has_entity_medication\",\n",
    "                                      \"chunk_has_entity_medical_condition\",\n",
    "                                      \"chunk_has_entity_treatment_arm\",\n",
    "                                      \"chunk_has_entity_study_population\",\n",
    "                                      \"chunk_has_entity_clinical_outcome\"]))\n",
    "    \n",
    "    for data, query in entity_link_iterator:\n",
    "        if len(data) > 0:\n",
    "            print(f\"Linking {len(data)} {query} entities to chunks\")\n",
    "            res = graph.execute_write_query_with_data(database=db_info['database'], \n",
    "                                                    data=data, \n",
    "                                                    query=relationship_load_queries[query], \n",
    "                                                    partitions=get_partition(data, batch_size=500),\n",
    "                                                    parallel=False)\n",
    "            print(res)\n",
    "        else:\n",
    "            print(f\"No {query} relationships to load\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingest_records = prepare_entities_for_ingestion_from_response_models(extracted_entities_with_chunk_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we load the entity nodes and relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 254 medication nodes\n",
      "partition: 1\n",
      "{'labels_added': 45, 'nodes_created': 45, 'properties_set': 1315}\n",
      "Loading 90 medical_condition nodes\n",
      "partition: 1\n",
      "{'labels_added': 46, 'nodes_created': 46, 'properties_set': 226}\n",
      "Loading 110 treatment_arm nodes\n",
      "partition: 1\n",
      "{'labels_added': 90, 'nodes_created': 90, 'properties_set': 310}\n",
      "Loading 23 study_population nodes\n",
      "partition: 1\n",
      "{'labels_added': 23, 'nodes_created': 23, 'properties_set': 322}\n",
      "Loading 139 clinical_outcome nodes\n",
      "partition: 1\n",
      "{'labels_added': 138, 'nodes_created': 138, 'properties_set': 416}\n"
     ]
    }
   ],
   "source": [
    "load_entity_nodes(ingest_records[\"nodes\"][\"medication\"], \n",
    "                  ingest_records[\"nodes\"][\"medical_condition\"], \n",
    "                  ingest_records[\"nodes\"][\"treatment_arm\"], \n",
    "                  ingest_records[\"nodes\"][\"study_population\"], \n",
    "                  ingest_records[\"nodes\"][\"clinical_outcome\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 121 medication_used_in_treatment_arm relationships\n",
      "partition: 1\n",
      "{'relationships_created': 106, 'properties_set': 1060}\n",
      "Loading 104 treatment_arm_has_clinical_outcome relationships\n",
      "partition: 1\n",
      "{'relationships_created': 104}\n",
      "Loading 41 study_population_in_treatment_arm relationships\n",
      "partition: 1\n",
      "{'relationships_created': 38}\n",
      "Loading 24 study_population_has_medical_condition relationships\n",
      "partition: 1\n",
      "{'relationships_created': 22}\n"
     ]
    }
   ],
   "source": [
    "load_entity_relationships(ingest_records[\"relationships\"][\"medication_used_in_treatment_arm\"], \n",
    "                          ingest_records[\"relationships\"][\"treatment_arm_has_clinical_outcome\"], \n",
    "                          ingest_records[\"relationships\"][\"study_population_in_treatment_arm\"], \n",
    "                          ingest_records[\"relationships\"][\"study_population_has_medical_condition\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Link Entities to Lexical Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we link the loaded entities to their respective `Chunk` nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linking 254 chunk_has_entity_medication entities to chunks\n",
      "partition: 1\n",
      "{'relationships_created': 254}\n",
      "Linking 90 chunk_has_entity_medical_condition entities to chunks\n",
      "partition: 1\n",
      "{'relationships_created': 90}\n",
      "Linking 110 chunk_has_entity_treatment_arm entities to chunks\n",
      "partition: 1\n",
      "{'relationships_created': 110}\n",
      "Linking 23 chunk_has_entity_study_population entities to chunks\n",
      "partition: 1\n",
      "{'relationships_created': 23}\n",
      "Linking 139 chunk_has_entity_clinical_outcome entities to chunks\n",
      "partition: 1\n",
      "{'relationships_created': 139}\n"
     ]
    }
   ],
   "source": [
    "link_entities_to_chunks(ingest_records[\"nodes\"][\"medication\"], \n",
    "                        ingest_records[\"nodes\"][\"medical_condition\"], \n",
    "                        ingest_records[\"nodes\"][\"treatment_arm\"], \n",
    "                        ingest_records[\"nodes\"][\"study_population\"], \n",
    "                        ingest_records[\"nodes\"][\"clinical_outcome\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Link Entity Graph to the Rest of Domain Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we execute custom Cypher to link the extracted entities with the existing patient journey graph.\n",
    "\n",
    "This will create the following relationships\n",
    "\n",
    "* (:Demographic)-[:IN_STUDY_POPULATION]->(:StudyPopulation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other links already exist since we are extracting `Medication` and `MedicalCondition` nodes from the text and these entities already exist in the patient journey graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def link_domain_and_patient_journey_graph() -> None:\n",
    "    \"\"\"\n",
    "    Link the domain graph with the patient journey graph. \n",
    "    This process doesn't require any input DataFrames. \n",
    "    Instead it attempts to link nodes based on matching properties.\n",
    "    \"\"\"\n",
    "\n",
    "    queries = [\"demographic_in_study_population\"]\n",
    "\n",
    "    for q in queries:\n",
    "        res = graph.execute_write_query(database=db_info['database'], \n",
    "                                        query=relationship_load_queries[q])\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    }
   ],
   "source": [
    "link_domain_and_patient_journey_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resolve Entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now can perform some entity resolution. \n",
    "\n",
    "The entity extraction process may find entities that are slight variations of existing entities.\n",
    "\n",
    "We can merge these entities together with some Cypher.\n",
    "\n",
    "The following entitites will be resolved:\n",
    "* `Medication`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolve_entities() -> None:\n",
    "    \"\"\"\n",
    "    Resolve extracted entities.\n",
    "    This process doesn't require any input DataFrames.\n",
    "    \"\"\"\n",
    "\n",
    "    queries = [\"resolve_medications\"]\n",
    "\n",
    "    for q in queries:\n",
    "        res = graph.execute_write_query(database=db_info['database'], \n",
    "                                        query=processing_queries[q])\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    }
   ],
   "source": [
    "resolve_entities()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
