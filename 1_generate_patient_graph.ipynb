{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PubMed Knowledge Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is part of a series that walks through the process of generating a knowledge graph of PubMed articles.\n",
    "\n",
    "This notebook will\n",
    "* Load structured patient journey data into a Neo4j instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "from pyneoinstance import Neo4jInstance, load_yaml_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be loading patient data that follows this data model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![patient-data-model](assets/images/patient-data-model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyNeoInstance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our database credentials and all of our queries are stored in the `pyneoinstance_config.yaml` file. \n",
    "\n",
    "This makes it easy to manage our queries and keeps the notebook code clean. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_yaml_file(\"pyneoinstance_config.yaml\")\n",
    "\n",
    "db_info = config['db_info']\n",
    "\n",
    "constraints = config['initializing_queries']['constraints']\n",
    "indexes = config['initializing_queries']['indexes']\n",
    "\n",
    "claim_queries = config['loading_queries']['patient_journey_claims']\n",
    "protocol_query = config['loading_queries']['patient_journey_protocol']\n",
    "post_processing_queries = config['processing_queries']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This graph object will handle database connections and read / write transactions for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = Neo4jInstance(db_info.get('uri', os.getenv(\"NEO4J_URI\", \"neo4j://localhost:7687\")), # use config value -> use env value -> use default value\n",
    "                      db_info.get('user', os.getenv(\"NEO4J_USER\", \"neo4j\")), \n",
    "                      db_info.get('password', os.getenv(\"NEO4J_PASSWORD\", \"password\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a helper function for ingesting data using the PyNeoInstance library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_partition(data: pd.DataFrame, batch_size: int = 500) -> int:\n",
    "    \"\"\"\n",
    "    Determine the data partition based on the desired batch size.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : pd.DataFrame\n",
    "        The Pandas DataFrame to partition.\n",
    "    batch_size : int\n",
    "        The desired batch size.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        The partition size.\n",
    "    \"\"\"\n",
    "    \n",
    "    partition = int(len(data) / batch_size)\n",
    "    print(\"partition: \"+str(partition if partition > 1 else 1))\n",
    "    return partition if partition > 1 else 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a mapping to add icd10 codes based on the existing icd9 codes in our Claims data. This is just a mock dataset so we're not too worried about detailed icd10 codes here.\n",
    "\n",
    "This cell was used to process the `claims_with_all_codes.csv` file, but is not currently being used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "icd9_to_icd10 = {\n",
    "    '244.9': 'E03.9',      # Hypothyroidism, unspecified\n",
    "    '250.0': 'E11.9',      # Diabetes mellitus type 2 without complications\n",
    "    '272.4': 'E78.5',      # Hyperlipidemia, unspecified\n",
    "    '300.0': 'F41.9',      # Anxiety disorder, unspecified\n",
    "    '311.0': 'F32.9',      # Major depressive disorder, single episode, unspecified\n",
    "    '401.9': 'I10',        # Essential (primary) hypertension\n",
    "    '414.0': 'I25.10',     # Atherosclerotic heart disease without angina pectoris\n",
    "    '493.9': 'J45.909',    # Asthma, unspecified, uncomplicated\n",
    "    '496.0': 'J44.9',      # COPD, unspecified\n",
    "    '530.81': 'K21.9',     # Gastroesophageal reflux disease without esophagitis\n",
    "    '585.9': 'N18.9',      # Chronic kidney disease, unspecified\n",
    "    '715.9': 'M19.90'      # Osteoarthritis, unspecified site\n",
    "}\n",
    "\n",
    "icd10_to_condition = {\n",
    "    'E03.9': 'Hypothyroidism, unspecified',\n",
    "    'E11.9': 'Diabetes mellitus type 2 without complications',\n",
    "    'E78.5': 'Hyperlipidemia, unspecified',\n",
    "    'F41.9': 'Anxiety disorder, unspecified',\n",
    "    'F32.9': 'Major depressive disorder, single episode, unspecified',\n",
    "    'I10': 'Essential (primary) hypertension',\n",
    "    'I25.10': 'Atherosclerotic heart disease without angina pectoris',\n",
    "    'J45.909': 'Asthma, unspecified, uncomplicated',\n",
    "    'J44.9': 'COPD, unspecified',\n",
    "    'K21.9': 'Gastroesophageal reflux disease without esophagitis',\n",
    "    'N18.9': 'Chronic kidney disease, unspecified',\n",
    "    'M19.90': 'Osteoarthritis, unspecified site'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constraints + Indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_constraints_and_indexes() -> None:\n",
    "    \"\"\"\n",
    "    Create constraints and indexes for the lexical and domain graphs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if constraints and len(constraints) > 0:\n",
    "            graph.execute_write_queries(database=db_info['database'], queries=list(constraints.values()))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    try:\n",
    "        if indexes and len(indexes) > 0:\n",
    "            graph.execute_write_queries(database=db_info['database'], queries=list(indexes.values()))\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_constraints_and_indexes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our patient journey data will be loaded from a Pandas DataFrame.\n",
    "\n",
    "We will load two types of patient journey data\n",
    "* Claims\n",
    "* Protocol\n",
    "\n",
    "After ingesting the data, we will connect the events sequentially for each patient with a Cypher query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_patient_journey_protocol_data(graph: Neo4jInstance, data: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Load patient journey protocol data into the graph.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    graph: Neo4jInstance\n",
    "        The graph to load the data into.\n",
    "    dataframe_tuples: list[tuple[str, pd.DataFrame]]\n",
    "        A list of tuples, where the first element is the name of the query in the config and the second element is the associated dataframe.\n",
    "    \"\"\"\n",
    "    print(f\"Loading {len(data)} patient journey protocol rows\")\n",
    "    \n",
    "    res = graph.execute_write_query_with_data(database=db_info['database'], \n",
    "                                            data=data, \n",
    "                                            query=protocol_query,\n",
    "                                            partitions=get_partition(data, batch_size=500),\n",
    "                                            parallel=False)\n",
    "    print(res)\n",
    "def load_patient_journey_claims_data(graph: Neo4jInstance, dataframe_tuples: list[tuple[str, pd.DataFrame]]) -> None:\n",
    "    \"\"\"\n",
    "    Load patient journey claims data into the graph.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    graph: Neo4jInstance\n",
    "        The graph to load the data into.\n",
    "    dataframe_tuples: list[tuple[str, pd.DataFrame]]\n",
    "        A list of tuples, where the first element is the name of the query in the config and the second element is the associated dataframe.\n",
    "    \"\"\"\n",
    "\n",
    "    # load csv data\n",
    "    for query_name, dataframe in dataframe_tuples:\n",
    "        print(f\"Loading {len(dataframe)} {query_name} rows\")\n",
    "        res = graph.execute_write_query_with_data(database=db_info['database'], \n",
    "                                                data=dataframe, \n",
    "                                                query=claim_queries[query_name], \n",
    "                                                partitions=get_partition(dataframe, batch_size=500),\n",
    "                                                parallel=False)\n",
    "        print(res)\n",
    "\n",
    "def sequence_patient_events(graph: Neo4jInstance) -> None:\n",
    "    \"\"\"\n",
    "    Post processing function to connect all patient events sequentially.\n",
    "    Patient will be connected to their most recent event.\n",
    "\n",
    "    (:Event)*<-[:PREVIOUS]-(:Event)<-[:MOST_RECENT_EVENT]->(:Patient)\n",
    "    \"\"\"\n",
    "    res = graph.execute_write_query(database=db_info['database'], \n",
    "                                    query=post_processing_queries['sequence_patient_events'])\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "protocol_data = pd.read_csv(\"data/protocol/extended_patient_journey.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "claims_data_path = \"data/claims\"\n",
    "claims_data_tuples = [\n",
    "    (\"patients\", pd.read_csv(f\"{claims_data_path}/patients.csv\")),\n",
    "    (\"providers\", pd.read_csv(f\"{claims_data_path}/providers.csv\")),\n",
    "    (\"claims\", pd.read_csv(f\"{claims_data_path}/claims_with_all_codes.csv\", converters={'icd9': str})),\n",
    "    (\"events\", pd.read_csv(f\"{claims_data_path}/patient_journey_with_providers.csv\")),\n",
    "    # (\"conditions\", pd.read_csv(f\"{claims_data_path}/conditions.csv\")),\n",
    "    (\"care_gaps\", pd.read_csv(f\"{claims_data_path}/care_gaps.csv\")),\n",
    "    (\"risk_scores\", pd.read_csv(f\"{claims_data_path}/risk_scores.csv\"))\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we load our patient journey data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 221 patient journey protocol rows\n",
      "partition: 1\n",
      "{'labels_added': 550, 'relationships_created': 1597, 'nodes_created': 550, 'properties_set': 1316}\n"
     ]
    }
   ],
   "source": [
    "load_patient_journey_protocol_data(graph, protocol_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 50 patients rows\n",
      "partition: 1\n",
      "{'labels_added': 50, 'nodes_created': 50, 'properties_set': 50}\n",
      "Loading 20 providers rows\n",
      "partition: 1\n",
      "{'labels_added': 20, 'nodes_created': 20, 'properties_set': 60}\n",
      "Loading 310 claims rows\n",
      "partition: 1\n",
      "{'labels_added': 644, 'relationships_created': 1547, 'nodes_created': 644, 'properties_set': 3756}\n",
      "Loading 333 events rows\n",
      "partition: 1\n",
      "{'labels_added': 328, 'relationships_created': 329, 'nodes_created': 328, 'properties_set': 989}\n",
      "Loading 43 care_gaps rows\n",
      "partition: 1\n",
      "{'labels_added': 85, 'relationships_created': 86, 'nodes_created': 85, 'properties_set': 170}\n",
      "Loading 50 risk_scores rows\n",
      "partition: 1\n",
      "{'labels_added': 50, 'relationships_created': 50, 'nodes_created': 50, 'properties_set': 100}\n"
     ]
    }
   ],
   "source": [
    "load_patient_journey_claims_data(graph, claims_data_tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'relationships_created': 900, 'relationships_deleted': 900}\n"
     ]
    }
   ],
   "source": [
    "sequence_patient_events(graph)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
