{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PubMed Knowledge Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is part of a series that walks through the process of generating a knowledge graph of PubMed articles.\n",
    "\n",
    "This notebook will\n",
    "* Extract entities from the Chunk nodes in a Neo4j graph according to the defined schema\n",
    "* Load the entities and connect them with their respective Chunk nodes\n",
    "* Connect the entities as defined by the Domain Graph Data Model\n",
    "* Connect extracted entities with existing patient journey graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter some Numpy warnings that pop up during ingestion\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import hashlib\n",
    "from typing import Any, Optional, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allows for async operations in notebooks\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pydantic import BaseModel, Field, computed_field, field_validator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Domain Graph Schema Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to define our knowledge graph schema. This information will be passed to the entity extraction LLM to control which entities and relationships are pulled out of the text.\n",
    "\n",
    "This is necessary to prevent our schema from growing too large with an unbounded extraction process.\n",
    "\n",
    "We are using Pydantic to define the schema here since it can be used to validate any returned results as well. This ensures that all data we are ingesting into Neo4j adheres to this structure.\n",
    "\n",
    "Here is what our domain graph data model looks like.\n",
    "\n",
    "<img src=\"./assets/images/domain-data-model-v1.png\" alt=\"domain-data-model\" width=\"600px\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------\n",
    "# Nodes\n",
    "# -------------\n",
    "\n",
    "class Medication(BaseModel):\n",
    "    \"\"\"\n",
    "    A substance used for medical treatment - a medicine or drug. \n",
    "    This is a general representation of a medication. \n",
    "    A Medication node may have relationships to TreatmentArm nodes that are specific to a particular study.\n",
    "    \"\"\"\n",
    "    \n",
    "    name: str = Field(..., description=\"Name of the medication. Should also be uniquely identifiable.\")\n",
    "    medication_class: str = Field(..., description=\"Drug class (e.g., GLP-1 RA, SGLT2i)\")\n",
    "    mechanism: Optional[str] = Field(None, description=\"Mechanism of action\")\n",
    "    generic_name: Optional[str] = Field(None, description=\"Generic name if different from name\")\n",
    "    brand_names: Optional[List[str]] = Field(None, description=\"Commercial brand names\")\n",
    "    approval_status: Optional[str] = Field(None, description=\"FDA approval status\")\n",
    "    \n",
    "    class Config:\n",
    "        json_schema_extra = {\n",
    "            \"examples\": [\n",
    "                {\n",
    "                    \"name\": \"Semaglutide\", \n",
    "                    \"medication_class\": \"GLP-1 receptor agonist\",\n",
    "                    \"mechanism\": \"GLP-1 receptor activation\",\n",
    "                    \"generic_name\": \"semaglutide\",\n",
    "                    \"brand_names\": [\"Ozempic\", \"Wegovy\", \"Rybelsus\"],\n",
    "                    \"approval_status\": \"FDA approved\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "\n",
    "class TreatmentArm(BaseModel):\n",
    "    \"\"\"\n",
    "    A treatment arm is a group of participants in a study that receive the same treatment.\n",
    "    A treatment arm should have relationships to Medication and ClinicalOutcome nodes.\n",
    "    \"\"\"\n",
    "    study_name: str = Field(..., description=\"Name of the study. This is used to uniquely identify the TreatmentArm node.\")\n",
    "    name: str = Field(..., description=\"Name of the treatment arm\")\n",
    "\n",
    "    class Config:\n",
    "        json_schema_extra = {\n",
    "            \"examples\": [\n",
    "                {\n",
    "                    \"study_name\": \"Study 1\",\n",
    "                    \"name\": \"Treatment arm 1\",\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "\n",
    "    @computed_field(return_type=str)\n",
    "    def treatment_arm_id(self) -> str:\n",
    "        \"\"\"\n",
    "        The unique id of the treatment arm.\n",
    "        This is a sha256 hash of the study name and treatment arm name.\n",
    "        \"\"\"\n",
    "        return hashlib.sha256(f\"{self.study_name}_{self.name}\".encode()).hexdigest()\n",
    "    \n",
    "\n",
    "class ClinicalOutcome(BaseModel):\n",
    "    \"\"\"\n",
    "    Measured clinical outcomes and biomarkers.\n",
    "    This node represents a clinical outcome present in a study.\n",
    "    ClinicalOutcome nodes should have relationships with other entity nodes from a study.\n",
    "    ClinicalOutcome nodes should not have relationships with entities that exist outside the study.\n",
    "    \"\"\"\n",
    "    \n",
    "    study_name: str = Field(..., description=\"Name of the study this outcome is associated with. This is used to uniquely identify the ClinicalOutcome node.\")\n",
    "    name: str = Field(..., description=\"A concise detailed name for the outcome.\")\n",
    "\n",
    "    @computed_field(return_type=str)\n",
    "    def clinical_outcome_id(self) -> str:\n",
    "        \"\"\"\n",
    "        The unique id of the clinical outcome.\n",
    "        This is a sha256 hash of the study name and the name of the outcome.\n",
    "        \"\"\"\n",
    "        return hashlib.sha256(f\"{self.study_name}_{self.name}\".encode()).hexdigest()\n",
    "    \n",
    "    class Config:\n",
    "        json_schema_extra = {\n",
    "            \"examples\": [\n",
    "                # don't include the clinical_outcome_id in the example since this is computed from extracted fields\n",
    "                {\n",
    "                    \"study_name\": \"Study 1\",\n",
    "                    \"name\": \"A1C controlled\",\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "\n",
    "\n",
    "class MedicalCondition(BaseModel):\n",
    "    \"\"\"Medical conditions and comorbidities studied\"\"\"\n",
    "    \n",
    "    name: str = Field(..., description=\"Name of the medical condition\")\n",
    "    category: str = Field(..., description=\"Category of condition\")\n",
    "    severity: Optional[str] = Field(None, description=\"Severity or stage when specified\")\n",
    "    icd10_code: Optional[str] = Field(None, description=\"ICD-10 code when available\")\n",
    "    duration: Optional[str] = Field(None, description=\"Duration of condition if specified\")\n",
    "    \n",
    "    @field_validator(\"icd10_code\")\n",
    "    def validate_icd10_code(cls, v: str) -> str:\n",
    "        \"\"\"\n",
    "        Validate that the ICD-10 code is valid.\n",
    "        \"\"\"\n",
    "        # ICD-10 codes are 3-7 characters long\n",
    "        if len(v) < 3 or len(v) > 7:\n",
    "            raise ValueError(\"ICD-10 code must be between 3 and 7 characters long.\")\n",
    "        # first character must be a letter\n",
    "        elif not v[0].isalpha():\n",
    "            raise ValueError(\"ICD-10 code must start with a letter.\")\n",
    "        # first character not case sensitive, can't be U, O, or I\n",
    "        elif v[0].upper() in [\"U\", \"O\", \"I\"]:\n",
    "            raise ValueError(\"ICD-10 code can not start with 'U', 'O', or 'I'.\")\n",
    "        # second character must be a digit\n",
    "        elif not v[1].isdigit():\n",
    "            raise ValueError(\"ICD-10 code second character must be a digit.\")\n",
    "        # '.' must separate the first 3 characters from the rest of the code\n",
    "        # examples:\n",
    "        # S52 Fracture of forearm\n",
    "        # S52.5 Fracture of lower end of radius\n",
    "        # S52.52 Torus fracture of lower end of radius\n",
    "        # S52.521 Torus fracture of lower end of right radius\n",
    "        # S52.521A Torus fracture of lower end of right radius, initial encounter, closed fracture\n",
    "        elif len(v) > 3 and not v[3] == '.':\n",
    "            raise ValueError(\"ICD-10 code must have a '.' after the first 3 characters.\")\n",
    "        return v\n",
    "    \n",
    "    class Config:\n",
    "        json_schema_extra = {\n",
    "            \"examples\": [\n",
    "                {\n",
    "                    \"name\": \"Type 2 diabetes mellitus\",\n",
    "                    \"category\": \"Primary condition\", \n",
    "                    \"severity\": \"moderate\",\n",
    "                    \"icd10_code\": \"E11\",\n",
    "                    \"duration\": \"5-10 years\",\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "\n",
    "\n",
    "class StudyPopulation(BaseModel):\n",
    "    \"\"\"Patient populations and demographics in research studies\"\"\"\n",
    "    \n",
    "    study_name: str = Field(..., description=\"Name of the study. This is used to uniquely identify the StudyPopulation node.\")\n",
    "    description: str = Field(..., description=\"Description of the population\")\n",
    "    min_age: Optional[int] = Field(None, description=\"Minimum age in years\")\n",
    "    max_age: Optional[int] = Field(None, description=\"Maximum age in years\")\n",
    "    male_percentage: Optional[float] = Field(None, description=\"Percentage of male gender participants\")\n",
    "    female_percentage: Optional[float] = Field(None, description=\"Percentage of female gender participants\")\n",
    "    other_gender_percentage: Optional[float] = Field(None, description=\"Percentage of participants that identify as another gender\")\n",
    "    sample_size: Optional[int] = Field(None, description=\"Number of participants\")\n",
    "    study_type: str = Field(..., description=\"Type of study\")\n",
    "    location: Optional[str] = Field(None, description=\"Geographic location of study\")\n",
    "    inclusion_criteria: Optional[List[str]] = Field(None, description=\"Key inclusion criteria\")\n",
    "    exclusion_criteria: Optional[List[str]] = Field(None, description=\"Key exclusion criteria\")\n",
    "    study_duration: Optional[str] = Field(None, description=\"Duration of study\")\n",
    "    \n",
    "    class Config:\n",
    "        json_schema_extra = {\n",
    "            \"examples\": [\n",
    "                {\n",
    "                    \"study_name\": \"Study 1\",\n",
    "                    \"description\": \"Adults with T2DM and schizophrenia\",\n",
    "                    \"min_age\": 30,\n",
    "                    \"max_age\": 39,\n",
    "                    \"male_percentage\": 46.0,\n",
    "                    \"female_percentage\": 53.0,\n",
    "                    \"other_gender_percentage\": 1.0,\n",
    "                    \"sample_size\": 100,\n",
    "                    \"study_type\": \"Observational study\",\n",
    "                    \"location\": \"Denmark\",\n",
    "                    \"inclusion_criteria\": [\"Type 2 diabetes diagnosis\", \"Schizophrenia diagnosis\", \"Age ≥18\"],\n",
    "                    \"study_duration\": \"12 months\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "\n",
    "    @computed_field(return_type=str)\n",
    "    def study_population_id(self) -> str:\n",
    "        \"\"\"\n",
    "        The unique id of the study population.\n",
    "        This is a sha256 hash of the study name.\n",
    "        \"\"\"\n",
    "        return hashlib.sha256(f\"{self.study_name}_{self.description}\".encode()).hexdigest()\n",
    "\n",
    "\n",
    "# -------------\n",
    "# Relationships\n",
    "# -------------\n",
    "\n",
    "class MedicationUsedInTreatmentArm(BaseModel):\n",
    "    \"\"\"\n",
    "    Study-specific medication usage - how a Medication was used in a particular TreatmentArm.\n",
    "    This describes an instance of a medication that is used in a particular treatment arm. \n",
    "    All treatment arms should have a relationship with at least one Medication node.\n",
    "    \"\"\"\n",
    "    study_name: str = Field(..., description=\"Name of the study.\")\n",
    "    treatment_arm_name: str = Field(..., description=\"Name of the treatment arm.\")\n",
    "    medication_name: str = Field(..., description=\"Name of the medication.\")\n",
    "    dosage: Optional[str] = Field(None, description=\"Dosage used in this study\")\n",
    "    route: Optional[str] = Field(None, description=\"Route of administration\")\n",
    "    frequency: Optional[str] = Field(None, description=\"Dosing frequency\")\n",
    "    treatment_duration: Optional[str] = Field(None, description=\"Duration of treatment\")\n",
    "    comparator: Optional[str] = Field(None, description=\"What this was compared against\")\n",
    "    adherence_rate: Optional[float] = Field(None, description=\"Treatment adherence rate\")\n",
    "    formulation: Optional[str] = Field(None, description=\"Specific formulation used\")\n",
    "\n",
    "    @computed_field(return_type=str)\n",
    "    def treatment_arm_id(self) -> str:\n",
    "        \"\"\"\n",
    "        The unique id of the treatment arm.\n",
    "        This is a sha256 hash of the study name and treatment arm.\n",
    "        \"\"\"\n",
    "        return hashlib.sha256(f\"{self.study_name}_{self.treatment_arm_name}\".encode()).hexdigest()\n",
    "    \n",
    "    class Config:\n",
    "        json_schema_extra = {\n",
    "            # don't include the study_medication_id in the example since this is computed from extracted fields\n",
    "            \"examples\": [\n",
    "                {\n",
    "                    \"study_name\": \"Study 1\",\n",
    "                    \"treatment_arm_name\": \"Treatment arm 1\",\n",
    "                    \"medication_name\": \"Medication 1\",\n",
    "                    \"dosage\": \"1.0 mg\",\n",
    "                    \"route\": \"subcutaneous\",\n",
    "                    \"frequency\": \"weekly\",\n",
    "                    \"treatment_duration\": \"12 weeks\",\n",
    "                    \"comparator\": \"placebo\",\n",
    "                    \"adherence_rate\": 85.5,\n",
    "                    \"formulation\": \"pre-filled pen\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "\n",
    "\n",
    "class TreatmentArmHasClinicalOutcome(BaseModel):\n",
    "    \"\"\"\n",
    "    Links TreatmentArm to ClinicalOutcome nodes.\n",
    "    TreatmentArm nodes should have a relationship with a ClinicalOutcome node.\n",
    "    Pattern: (:TreatmentArm)-[:HAS_CLINICAL_OUTCOME]->(:ClinicalOutcome)\n",
    "    \"\"\"\n",
    "    study_name: str = Field(..., description=\"Name of the study. This is used to uniquely identify the TreatmentArm node.\")\n",
    "    treatment_arm_name: str = Field(..., description=\"Name of the treatment arm.\")\n",
    "    clinical_outcome_name: str = Field(..., description=\"Name of the clinical outcome\")\n",
    "\n",
    "    @computed_field(return_type=str)\n",
    "    def clinical_outcome_id(self) -> str:\n",
    "        \"\"\"\n",
    "        The unique id of the clinical outcome.\n",
    "        This is a sha256 hash of the study name and the name of the outcome.\n",
    "        \"\"\"\n",
    "        return hashlib.sha256(f\"{self.study_name}_{self.clinical_outcome_name}\".encode()).hexdigest()\n",
    "    \n",
    "    @computed_field(return_type=str)\n",
    "    def treatment_arm_id(self) -> str:\n",
    "        \"\"\"\n",
    "        The unique id of the treatment arm.\n",
    "        This is a sha256 hash of the study name and treatment arm.\n",
    "        \"\"\"\n",
    "        return hashlib.sha256(f\"{self.study_name}_{self.treatment_arm_name}\".encode()).hexdigest()\n",
    "\n",
    "\n",
    "class StudyPopulationHasMedicalCondition(BaseModel):\n",
    "    \"\"\"\n",
    "    Links StudyPopulation to MedicalCondition nodes.\n",
    "    StudyPopulation nodes should have a relationship with a MedicalCondition node.\n",
    "    Pattern: (:StudyPopulation)-[:HAS_MEDICAL_CONDITION]->(:MedicalCondition)\n",
    "    \"\"\"\n",
    "    study_name: str = Field(..., description=\"Name of the study. This is used to uniquely identify the StudyPopulation node.\")\n",
    "    study_population_description: str = Field(..., description=\"Description of the study population.\")\n",
    "    medical_condition_name: str\n",
    "\n",
    "    @computed_field(return_type=str)\n",
    "    def study_population_id(self) -> str:\n",
    "        \"\"\"\n",
    "        The unique id of the study population.\n",
    "        This is a sha256 hash of the study name and population description.\n",
    "        \"\"\"\n",
    "        return hashlib.sha256(f\"{self.study_name}_{self.study_population_description}\".encode()).hexdigest()\n",
    "\n",
    "\n",
    "class StudyPopulationInTreatmentArm(BaseModel):\n",
    "    \"\"\"\n",
    "    Links StudyPopulation to TreatmentArm nodes.\n",
    "    StudyPopulation nodes should have a relationship with a TreatmentArm node.\n",
    "    Pattern: (:StudyPopulation)-[:IN_TREATMENT_ARM]->(:TreatmentArm)\n",
    "    \"\"\"\n",
    "    study_name: str = Field(..., description=\"Name of the study. This is used to uniquely identify the StudyPopulation node.\")\n",
    "    study_population_description: str = Field(..., description=\"Description of the study population.\")\n",
    "    treatment_arm_name: str = Field(..., description=\"Name of the treatment arm.\")\n",
    "\n",
    "    @computed_field(return_type=str)\n",
    "    def treatment_arm_id(self) -> str:\n",
    "        \"\"\"\n",
    "        The unique id of the treatment arm.\n",
    "        This is a sha256 hash of the study name and treatment arm.\n",
    "        \"\"\"\n",
    "        return hashlib.sha256(f\"{self.study_name}_{self.treatment_arm_name}\".encode()).hexdigest()\n",
    "\n",
    "    @computed_field(return_type=str)\n",
    "    def study_population_id(self) -> str:\n",
    "        \"\"\"\n",
    "        The unique id of the study population.\n",
    "        This is a sha256 hash of the study name and population description.\n",
    "        \"\"\"\n",
    "        return hashlib.sha256(f\"{self.study_name}_{self.study_population_description}\".encode()).hexdigest()\n",
    "\n",
    "\n",
    "# class StudyPopulationHasClinicalOutcome(BaseModel):\n",
    "#     \"\"\"\n",
    "#     Links StudyPopulation to ClinicalOutcome nodes.\n",
    "#     StudyPopulation nodes should have a relationship with a ClinicalOutcome node.\n",
    "#     Pattern: (:StudyPopulation)-[:HAS_CLINICAL_OUTCOME]->(:ClinicalOutcome)\n",
    "#     \"\"\"\n",
    "#     study_name: str = Field(..., description=\"Name of the study. This is used to uniquely identify the StudyPopulation node.\")\n",
    "#     study_population_description: str = Field(..., description=\"Description of the study population.\")\n",
    "#     clinical_outcome_name: str = Field(..., description=\"Name of the clinical outcome to match on.\")\n",
    "\n",
    "#     @computed_field(return_type=str)\n",
    "#     def study_population_id(self) -> str:\n",
    "#         \"\"\"\n",
    "#         The unique id of the study population.\n",
    "#         This is a sha256 hash of the study name and population description.\n",
    "#         \"\"\"\n",
    "#         return hashlib.sha256(f\"{self.study_name}_{self.study_population_description}\".encode()).hexdigest()\n",
    "\n",
    "#     @computed_field(return_type=str)\n",
    "#     def clinical_outcome_id(self) -> str:\n",
    "#         \"\"\"\n",
    "#         The unique id of the clinical outcome.\n",
    "#         This is a sha256 hash of the study name and the name of the outcome.\n",
    "#         \"\"\"\n",
    "#         return hashlib.sha256(f\"{self.study_name}_{self.clinical_outcome_name}\".encode()).hexdigest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lexical and domain knowledge graphs will be linked with `HAS_ENTITY` relationships between Chunk nodes and domain graph nodes.\n",
    "\n",
    "This is the combined lexical and domain graph data model.\n",
    "\n",
    "IMAGE OF DATA MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity Extraction via LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using [OpenAI](https://platform.openai.com/docs/overview) and the [Instructor](https://python.useinstructor.com/) library to perform our entity extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AsyncOpenAI\n",
    "import instructor\n",
    "from instructor.exceptions import IncompleteOutputException, InstructorRetryException, ValidationError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructor handles requesting structured outputs from the LLM. \n",
    "\n",
    "If the LLM fails to return output that adheres to the response models, Instructor will also handle the retry logic and pass any errors to inform corrections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = instructor.from_openai(AsyncOpenAI())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the system prompt defines the overall behavior of the LLM\n",
    "system_prompt = \"\"\"\n",
    "You are a healthcare research expert that is responsible for extracting detailed entities from PubMed articles. \n",
    "You will be provided a graph data model schema and must extract entities and relationships to populate a knowledge graph.\n",
    "\"\"\"\n",
    "\n",
    "async def extract_entities_from_text_chunk(text_chunk: str) -> list:\n",
    "    \"\"\"\n",
    "    Extract entities and relationships from a text chunk.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text_chunk : str\n",
    "        The text chunk to extract entities from.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list[Medication | StudyMedication | ClinicalOutcome | StudyMedicationUsesMedication | StudyMedicationProducesClinicalOutcome],\n",
    "        A list of entities and relationships extracted from the text chunk.\n",
    "        If the response is truncated, an empty list is returned.\n",
    "        If retries are exhausted, an empty list is returned.\n",
    "        If the response is invalid, an empty list is returned.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = await client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": text_chunk}\n",
    "            ],\n",
    "            response_model=list[\n",
    "                            # first test batch  \n",
    "                            Medication | \n",
    "                            TreatmentArm | \n",
    "                            ClinicalOutcome | \n",
    "                            MedicationUsedInTreatmentArm | \n",
    "                            TreatmentArmHasClinicalOutcome |\n",
    "                            # then add these\n",
    "                            StudyPopulation |\n",
    "                            MedicalCondition |\n",
    "                            StudyPopulationHasMedicalCondition |\n",
    "                            StudyPopulationInTreatmentArm\n",
    "                            ],\n",
    "            temperature=0.0\n",
    "        )\n",
    "        return response\n",
    "    except IncompleteOutputException as e:\n",
    "        # Handle truncated output\n",
    "        print(f\"Response output truncated. Skipping chunk.\")\n",
    "        return list()\n",
    "    except InstructorRetryException as e:\n",
    "        # Handle retry exhaustion\n",
    "        print(f\"Failed after {e.n_attempts} attempts. Skipping chunk.\")\n",
    "        return list()\n",
    "    except ValidationError as e:\n",
    "        # Handle validation errors\n",
    "        print(f\"Validation failed. Skipping chunk.\\nError: {e}\")\n",
    "        return list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def extract_entities_from_chunk_nodes(chunk_nodes_dataframe: pd.DataFrame, batch_size: int = 100) -> list[tuple[str, list[Any]]]:\n",
    "    \"\"\"\n",
    "    Process a Pandas DataFrame of Chunk nodes and return the entities found in each chunk.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    chunk_nodes_dataframe : pd.DataFrame\n",
    "        A Pandas DataFrame where each row represents a Chunk node.\n",
    "        Has columns `id` and `text`.\n",
    "    batch_size : int\n",
    "        The number of text chunks to process in each batch.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list[tuple[str, list[dict[str, Any]]]]\n",
    "        A list of tuples, where the first element is the chunk id and the second element is a list of entities found in the chunk.\n",
    "    \"\"\"\n",
    "\n",
    "    results = list()\n",
    "\n",
    "    for batch_idx, i in enumerate(range(0, len(chunk_nodes_dataframe), batch_size)):\n",
    "        if i + batch_size >= len(chunk_nodes_dataframe):\n",
    "            batch = chunk_nodes_dataframe.iloc[i:]\n",
    "        else:\n",
    "            batch = chunk_nodes_dataframe.iloc[i:i+batch_size]\n",
    "        print(f\"Processing batch {batch_idx+1} of {int(len(chunk_nodes_dataframe)/(batch_size))}  \\n\", end=\"\\r\")\n",
    "        # TODO: implement cache of failed chunks\n",
    "        \n",
    "        # Create tasks for all nodes in the batch\n",
    "        # order is maintained\n",
    "        tasks = [extract_entities_from_text_chunk(row[\"text\"]) for _, row in batch.iterrows()]\n",
    "        # Execute all tasks concurrently\n",
    "        extraction_results = await asyncio.gather(*tasks)\n",
    "        # Add extracted records to the results list\n",
    "        results.extend(extraction_results)\n",
    "\n",
    "    # Return chunk_id paired with its entities from the results list\n",
    "    return [(chunk_id, entities) for chunk_id, entities in zip(chunk_nodes_dataframe[\"id\"], results)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now defined \n",
    "* Domain data model\n",
    "* Entity extraction logic for chunks\n",
    "\n",
    "It is now time to define our ingestion logic. We will run ingest in three stages \n",
    "\n",
    "1. Extract Domain / Entity Graph from lexical graph Chunk nodes\n",
    "2. Ingest entities into Domain Graph\n",
    "\n",
    "Decoupling these stages allows us easily make changes as we iterate our ingestion process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from pyneoinstance import Neo4jInstance, load_yaml_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our database credentials and all of our queries are stored in the `pyneoinstance_config.yaml` file. \n",
    "\n",
    "This makes it easy to manage our queries and keeps the notebook code clean. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_yaml_file(\"pyneoinstance_config.yaml\")\n",
    "\n",
    "db_info = config['db_info']\n",
    "\n",
    "constraints = config['initializing_queries']['constraints']\n",
    "indexes = config['initializing_queries']['indexes']\n",
    "\n",
    "node_load_queries = config['loading_queries']['nodes']\n",
    "relationship_load_queries = config['loading_queries']['relationships']\n",
    "\n",
    "processing_queries = config['processing_queries']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This graph object will handle database connections and read / write transactions for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = Neo4jInstance(db_info.get('uri', os.getenv(\"NEO4J_URI\", \"neo4j://localhost:7687\")), # use config value -> use env value -> use default value\n",
    "                      db_info.get('user', os.getenv(\"NEO4J_USER\", \"neo4j\")), \n",
    "                      db_info.get('password', os.getenv(\"NEO4J_PASSWORD\", \"password\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a helper function for ingesting data using the PyNeoInstance library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_partition(data: pd.DataFrame, batch_size: int = 500) -> int:\n",
    "    \"\"\"\n",
    "    Determine the data partition based on the desired batch size.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : pd.DataFrame\n",
    "        The Pandas DataFrame to partition.\n",
    "    batch_size : int\n",
    "        The desired batch size.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        The partition size.\n",
    "    \"\"\"\n",
    "    \n",
    "    partition = int(len(data) / batch_size)\n",
    "    print(\"partition: \"+str(partition if partition > 1 else 1))\n",
    "    return partition if partition > 1 else 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we write all the constraints and indexes we need for both the lexical and domain graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_constraints_and_indexes() -> None:\n",
    "    \"\"\"\n",
    "    Create constraints and indexes for the lexical and domain graphs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if constraints and len(constraints) > 0:\n",
    "            graph.execute_write_queries(database=db_info['database'], queries=list(constraints.values()))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    try:\n",
    "        if indexes and len(indexes) > 0:\n",
    "            graph.execute_write_queries(database=db_info['database'], queries=list(indexes.values()))\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_constraints_and_indexes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Entities from Lexical Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now perform entity extraction on the Chunk nodes to augment and connect to our patient journey graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunk_nodes_to_process(min_length: int = 100) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Retrieve Chunk node id and text from the database that don't have an embedding.\n",
    "    These chunks may then be used as input to the entity extraction process.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    min_length : int\n",
    "        The minimum length the text must be to be included in the DataFrame.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A Pandas DataFrame where each row represents a Chunk node that has text and is at least `min_length` characters long.\n",
    "        Has columns `id` and `text`.\n",
    "    \"\"\"\n",
    "    return graph.execute_read_query(database=db_info['database'], \n",
    "                            query=processing_queries['get_chunk_nodes_to_process'], \n",
    "                            parameters={\"min_length\": min_length},\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownPropertyKeyWarning} {category: UNRECOGNIZED} {title: The provided property key is not in the database} {description: One of the property names in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing property name is: embedding)} {position: line: 3, column: 9, offset: 49} for query: 'MATCH (c:Chunk)\\nWHERE c.text IS NOT NULL\\n  AND c.embedding IS NULL\\n  AND size(c.text) >= $min_length\\nRETURN c.id as id, c.text as text\\n'\n"
     ]
    }
   ],
   "source": [
    "chunks_to_process = get_chunk_nodes_to_process(min_length=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 494 chunks to process\n",
      "\n",
      "First chunk:\n",
      "\n",
      "statistical significance (metformin + Ex9-39 vs. placebo + Ex9-39, P = 0.053). The glucose iAUC after metformin + saline was significantly smaller than the iAUC for metformin + Ex9-39 (P = 0.004). Based on individual iAUC values, the relative contribution of GLP-1 to the acute glucose-lowering effect of metformin was 75% ± 35%, calculated as follows: 100% × ([iAUCplacebo + saline – iAUCmetformin + saline] – [iAUCplacebo + Ex9–39 – iAUCmetformin + Ex9–39])/(iAUCplacebo + saline – iAUCmetformin + saline) (P = 0.05). Using a 2-way ANOVA, both metformin and Ex9-39 were shown to significantly affect postprandial plasma glucose (iAUC) (P = 0.005 and P = 0.002, respectively), but no interaction between the 2 factors was evident. The time courses of the C-peptide/glucose ratios are illustrated in Figure 2B, and the AUCs for C-peptide/glucose, insulin/glucose, and insulin secretion\n"
     ]
    }
   ],
   "source": [
    "print(f\"Found {len(chunks_to_process)} chunks to process\\n\")\n",
    "print(f\"First chunk:\\n\\n{chunks_to_process.loc[0,'text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 1 of 10  \n",
      "Failed after 3 attempts. Skipping chunk.\n",
      "Failed after 3 attempts. Skipping chunk.\n",
      "Processing batch 2 of 10  \n",
      "Processing batch 3 of 10  \n",
      "Failed after 3 attempts. Skipping chunk.\n",
      "Processing batch 4 of 10  \n",
      "Processing batch 5 of 10  \n",
      "Processing batch 6 of 10  \n",
      "Processing batch 7 of 10  \n",
      "Processing batch 8 of 10  \n",
      "Processing batch 9 of 10  \n",
      "Failed after 3 attempts. Skipping chunk.\n",
      "Processing batch 10 of 10  \n",
      "Failed after 3 attempts. Skipping chunk.\n"
     ]
    }
   ],
   "source": [
    "entity_ingest_records = await extract_entities_from_chunk_nodes(chunks_to_process[:200], batch_size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingest Entities Into Knowledge Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These functions load the extracted entities and relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These functions link the extracted entities with their text chunk nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENTITY_LABELS = {\n",
    "    \"Medication\", \n",
    "    \"TreatmentArm\",\n",
    "    \"MedicalCondition\",\n",
    "    \"StudyPopulation\",\n",
    "    \"ClinicalOutcome\",\n",
    "}\n",
    "\n",
    "ENTITY_RELS = {\n",
    "    \"MedicationUsedInTreatmentArm\",\n",
    "    \"TreatmentArmHasClinicalOutcome\",\n",
    "    \"StudyPopulationInTreatmentArm\",\n",
    "    \"StudyPopulationHasMedicalCondition\",\n",
    "    # \"StudyPopulationHasClinicalOutcome\",\n",
    "}\n",
    "\n",
    "def prepare_entities_for_ingestion(entities: list[tuple[str, list[Any]]]) -> dict[str, dict[str, pd.DataFrame]]:\n",
    "    \"\"\"\n",
    "    Prepare entities for ingestion into the graph.\n",
    "    This function takes the results of the `get_chunk_nodes_to_process_by_article_name` function and returns a dictionary of entity label keys and pandas dataframes of entities.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    entities : list[tuple[str, list[Any]]]\n",
    "        A list of tuples, where the first element is the chunk id and the second element is a list of entities found in the chunk.\n",
    "        Entities are Pydantic models that adhere to the domain graph data model.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict[str, dict[str, pd.DataFrame]]\n",
    "        A dictionary of entity label to pandas dataframe of entities.\n",
    "\n",
    "        {\n",
    "            \"nodes\": {\n",
    "                \"Medication\": pd.DataFrame(...),\n",
    "                \"StudyMedication\": pd.DataFrame(...),\n",
    "                ...\n",
    "            },\n",
    "            \"relationships\": {\n",
    "                \"StudyMedicationUsesMedication\": pd.DataFrame(...),\n",
    "                \"StudyMedicationProducesClinicalOutcome\": pd.DataFrame(...),\n",
    "                ...\n",
    "            }\n",
    "        }\n",
    "    \"\"\"\n",
    "\n",
    "    records_node_dict = {lbl: list() for lbl in ENTITY_LABELS}\n",
    "    records_rel_dict = {lbl: list() for lbl in ENTITY_RELS}\n",
    "\n",
    "    for chunk_id, entities in entities:\n",
    "        for entity in entities:\n",
    "            to_add = entity.model_dump()\n",
    "            to_add.update({\"chunk_id\": chunk_id})\n",
    "            # nodes\n",
    "            if entity.__class__.__name__ in ENTITY_LABELS:\n",
    "                records_node_dict[entity.__class__.__name__].append(to_add)\n",
    "            # rels\n",
    "            elif entity.__class__.__name__ in ENTITY_RELS:\n",
    "                records_rel_dict[entity.__class__.__name__].append(to_add)\n",
    "            else:\n",
    "                print(f\"Unknown entity type: {entity.__class__.__name__}\")\n",
    "\n",
    "    for key, value in records_node_dict.items():\n",
    "        records_node_dict[key] = pd.DataFrame(value).replace({float('nan'): None})\n",
    "\n",
    "    for key, value in records_rel_dict.items():\n",
    "        records_rel_dict[key] = pd.DataFrame(value).replace({float('nan'): None})\n",
    "\n",
    "    return {\"nodes\": records_node_dict, \"relationships\": records_rel_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_entity_nodes(medication_dataframe: pd.DataFrame, \n",
    "                      medical_condition_dataframe: pd.DataFrame, \n",
    "                      treatment_arm_dataframe: pd.DataFrame, \n",
    "                      study_population_dataframe: pd.DataFrame, \n",
    "                      clinical_outcome_dataframe: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Load entity nodes into the graph.\n",
    "    \"\"\"\n",
    "    \n",
    "    entity_nodes_ingest_iterator = list(zip([medication_dataframe, \n",
    "                                             medical_condition_dataframe, \n",
    "                                             treatment_arm_dataframe, \n",
    "                                             study_population_dataframe, \n",
    "                                             clinical_outcome_dataframe], \n",
    "                                             ['medication', \n",
    "                                              'medical_condition', \n",
    "                                              'treatment_arm', \n",
    "                                              'study_population', \n",
    "                                              'clinical_outcome']))\n",
    "\n",
    "    for data, query in entity_nodes_ingest_iterator:\n",
    "        if len(data) > 0:\n",
    "            print(f\"Loading {len(data)} {query} nodes\")\n",
    "            res = graph.execute_write_query_with_data(database=db_info['database'], \n",
    "                                                    data=data, \n",
    "                                                    query=node_load_queries[query], \n",
    "                                                    partitions=get_partition(data, batch_size=500),\n",
    "                                                    parallel=False)\n",
    "            print(res)\n",
    "        else:\n",
    "            print(f\"No {query} nodes to load\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_entity_relationships(medication_used_in_treatment_arm_dataframe: pd.DataFrame,\n",
    "                              treatment_arm_has_clinical_outcome_dataframe: pd.DataFrame,\n",
    "                              study_population_in_treatment_arm_dataframe: pd.DataFrame,\n",
    "                              study_population_has_medical_condition_dataframe: pd.DataFrame,\n",
    "                            #   study_population_receives_study_medication_dataframe: pd.DataFrame,\n",
    "                            #   study_population_has_outcome_dataframe: pd.DataFrame,\n",
    "                              ) -> None:\n",
    "    \"\"\"\n",
    "    Load entity relationships into the graph.\n",
    "    \"\"\"\n",
    "    entity_relationships_ingest_iterator = list(zip([medication_used_in_treatment_arm_dataframe, \n",
    "                                                      treatment_arm_has_clinical_outcome_dataframe, \n",
    "                                                      study_population_in_treatment_arm_dataframe, \n",
    "                                                      study_population_has_medical_condition_dataframe, \n",
    "                                                    #   study_population_has_outcome_dataframe\n",
    "                                                      ], \n",
    "                                                      ['medication_used_in_treatment_arm', \n",
    "                                                       'treatment_arm_has_clinical_outcome', \n",
    "                                                       'study_population_in_treatment_arm', \n",
    "                                                       'study_population_has_medical_condition', \n",
    "                                                    #    'study_population_receives_study_medication', \n",
    "                                                    #    'study_population_has_clinical_outcome'\n",
    "                                                       ]))\n",
    "    \n",
    "    for data, query in entity_relationships_ingest_iterator:\n",
    "        if len(data) > 0:\n",
    "            print(f\"Loading {len(data)} {query} relationships\")\n",
    "            res = graph.execute_write_query_with_data(database=db_info['database'], \n",
    "                                                    data=data, \n",
    "                                                    query=relationship_load_queries[query], \n",
    "                                                    partitions=get_partition(data, batch_size=500),\n",
    "                                                    parallel=False)\n",
    "            print(res)\n",
    "        else:\n",
    "            print(f\"No {query} relationships to load\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def link_entities_to_chunks(medication_link_dataframe: pd.DataFrame, \n",
    "                      medical_condition_link_dataframe: pd.DataFrame, \n",
    "                      treatment_arm_link_dataframe: pd.DataFrame, \n",
    "                      study_population_link_dataframe: pd.DataFrame, \n",
    "                      clinical_outcome_link_dataframe: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Link entities to chunks.\n",
    "    \"\"\"\n",
    "    entity_link_iterator = list(zip([medication_link_dataframe, \n",
    "                                     medical_condition_link_dataframe, \n",
    "                                     treatment_arm_link_dataframe, \n",
    "                                     study_population_link_dataframe, \n",
    "                                     clinical_outcome_link_dataframe], \n",
    "                                     [\"chunk_has_entity_medication\",\n",
    "                                      \"chunk_has_entity_medical_condition\",\n",
    "                                      \"chunk_has_entity_treatment_arm\",\n",
    "                                      \"chunk_has_entity_study_population\",\n",
    "                                      \"chunk_has_entity_clinical_outcome\"]))\n",
    "    \n",
    "    for data, query in entity_link_iterator:\n",
    "        if len(data) > 0:\n",
    "            print(f\"Linking {len(data)} {query} entities to chunks\")\n",
    "            res = graph.execute_write_query_with_data(database=db_info['database'], \n",
    "                                                    data=data, \n",
    "                                                    query=relationship_load_queries[query], \n",
    "                                                    partitions=get_partition(data, batch_size=500),\n",
    "                                                    parallel=False)\n",
    "            print(res)\n",
    "        else:\n",
    "            print(f\"No {query} relationships to load\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingest_records = prepare_entities_for_ingestion(entity_ingest_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - This columns are not in your data: ['prevalence']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 289 medication nodes\n",
      "partition: 1\n",
      "{'properties_set': 1445}\n",
      "Loading 53 medical_condition nodes\n",
      "partition: 1\n",
      "{'properties_set': 265}\n",
      "Loading 246 treatment_arm nodes\n",
      "partition: 1\n",
      "{'properties_set': 492}\n",
      "Loading 74 study_population nodes\n",
      "partition: 1\n",
      "{'properties_set': 962}\n",
      "No clinical_outcome nodes to load\n"
     ]
    }
   ],
   "source": [
    "load_entity_nodes(ingest_records[\"nodes\"][\"Medication\"], \n",
    "                  ingest_records[\"nodes\"][\"MedicalCondition\"], \n",
    "                  ingest_records[\"nodes\"][\"TreatmentArm\"], \n",
    "                  ingest_records[\"nodes\"][\"StudyPopulation\"], \n",
    "                  ingest_records[\"nodes\"][\"ClinicalOutcome\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 91 medication_used_in_treatment_arm relationships\n",
      "partition: 1\n",
      "{}\n",
      "Loading 124 treatment_arm_has_clinical_outcome relationships\n",
      "partition: 1\n",
      "{}\n",
      "Loading 43 study_population_in_treatment_arm relationships\n",
      "partition: 1\n",
      "{'relationships_created': 40}\n",
      "Loading 32 study_population_has_medical_condition relationships\n",
      "partition: 1\n",
      "{'relationships_created': 30}\n"
     ]
    }
   ],
   "source": [
    "load_entity_relationships(ingest_records[\"relationships\"][\"MedicationUsedInTreatmentArm\"], \n",
    "                          ingest_records[\"relationships\"][\"TreatmentArmHasClinicalOutcome\"], \n",
    "                          ingest_records[\"relationships\"][\"StudyPopulationInTreatmentArm\"], \n",
    "                          ingest_records[\"relationships\"][\"StudyPopulationHasMedicalCondition\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we link the loaded entities to their respective Chunk nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linking 289 chunk_has_entity_medication entities to chunks\n",
      "partition: 1\n",
      "{}\n",
      "Linking 53 chunk_has_entity_medical_condition entities to chunks\n",
      "partition: 1\n",
      "{}\n",
      "Linking 246 chunk_has_entity_treatment_arm entities to chunks\n",
      "partition: 1\n",
      "{'relationships_created': 246}\n",
      "Linking 74 chunk_has_entity_study_population entities to chunks\n",
      "partition: 1\n",
      "{'relationships_created': 70}\n",
      "No chunk_has_entity_clinical_outcome relationships to load\n"
     ]
    }
   ],
   "source": [
    "link_entities_to_chunks(ingest_records[\"nodes\"][\"Medication\"], \n",
    "                        ingest_records[\"nodes\"][\"MedicalCondition\"], \n",
    "                        ingest_records[\"nodes\"][\"TreatmentArm\"], \n",
    "                        ingest_records[\"nodes\"][\"StudyPopulation\"], \n",
    "                        ingest_records[\"nodes\"][\"ClinicalOutcome\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def link_domain_and_patient_journey_graph() -> None:\n",
    "    \"\"\"\n",
    "    Link the domain graph with the patient journey graph. \n",
    "    This process doesn't require any input DataFrames. \n",
    "    Instead it attempts to link nodes based on matching properties.\n",
    "    \"\"\"\n",
    "\n",
    "    queries = [\"demographic_in_study_population\"]\n",
    "\n",
    "    for q in queries:\n",
    "        res = graph.execute_write_query(database=db_info['database'], \n",
    "                                        query=relationship_load_queries[q])\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'relationships_created': 150, 'properties_set': 150}\n"
     ]
    }
   ],
   "source": [
    "link_domain_and_patient_journey_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
