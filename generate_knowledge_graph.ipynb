{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PubMed Knowledge Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook walks through the process of generating a knowledge graph of PubMed articles.\n",
    "\n",
    "This notebook will\n",
    "* Download a selection of articles from PubMed\n",
    "* Define a knowledge graph schema\n",
    "* Extract entities from the articles according to the defined schema\n",
    "* Populate a Neo4j instance with articles and extracted entities\n",
    "* Connect extracted entities with existing patient journey data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook requires a local repo of articles. You may download a sample of 20 PubMed articles by running the following command.\n",
    "\n",
    "```bash\n",
    "python3 ./scripts/fetch_pubmed_articles.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter some Numpy warnings that pop up during ingestion\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import hashlib\n",
    "from typing import Any, Optional, List\n",
    "# import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allows for async operations in notebooks\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexical Graph Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use Unstructured.IO to partition and chunk our articles. \n",
    "\n",
    "This process breaks the articles into sensible chunks that may be used as context in our application. \n",
    "\n",
    "These chunks will also have relationships to the extracted entities, but we will add these relationships later.\n",
    "\n",
    "The lexical graph will adhere to the structure defined in the ['Lexical Graph with Extracted Entities'](https://graphrag.com/reference/knowledge-graph/lexical-graph-extracted-entities/) section of [graphrag.com](graphrag.com).\n",
    "\n",
    "Here is the data model we will be using.\n",
    "\n",
    "\n",
    "<img src=\"./assets/images/lexical-data-model.png\" alt=\"lexical-data-model\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexandergilmore/Documents/projects/ps-nashville/pubmed-knowledge-graph/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pydantic import BaseModel, Field\n",
    "from unstructured.chunking.title import chunk_by_title\n",
    "from unstructured.partition.auto import partition\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "from unstructured.partition.xml import partition_xml\n",
    "\n",
    "from unstructured.documents.elements import CompositeElement\n",
    "\n",
    "from uuid import uuid4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------\n",
    "# Nodes\n",
    "# ------------\n",
    "\n",
    "class Document(BaseModel):\n",
    "    \"\"\"\n",
    "    A Document.\n",
    "    This is the top level node in our knowledge graph.\n",
    "    Documents are made of many Chunks.\n",
    "    \"\"\"\n",
    "    id: str = Field(..., description=\"The id of the document\")\n",
    "    name: str = Field(..., description=\"The name of the document\")\n",
    "    source: str = Field(..., description=\"The source of the document\")\n",
    "\n",
    "class Chunk(BaseModel):\n",
    "    \"\"\"\n",
    "    A Chunk.\n",
    "    This is a collection of UnstructuredElements.\n",
    "    \"\"\"\n",
    "    id: str = Field(..., description=\"The id of the chunk\")\n",
    "    type: str = Field(..., description=\"The type of the chunk\")\n",
    "    text: str = Field(..., description=\"The text of the chunk\")\n",
    "\n",
    "class ChunkWithEmbedding(Chunk):\n",
    "    \"\"\"\n",
    "    A Chunk with an embedding.\n",
    "    This is used to represent chunks that have been embedded.\n",
    "    \"\"\"\n",
    "    embedding: list[float] = Field(..., description=\"The embedding of the chunk text field\")\n",
    "\n",
    "class UnstructuredElement(BaseModel):\n",
    "    \"\"\"\n",
    "    A base class for all unstructured elements. \n",
    "    These are the smallest units in our chunking process. \n",
    "    One or more of these elements are combined to form a Chunk.\n",
    "    \"\"\"\n",
    "    id: str = Field(..., description=\"The id of the element\")\n",
    "    text: str = Field(..., description=\"The text of the element\")\n",
    "    type: str = Field(..., description=\"The type of the element\")\n",
    "    page_number: int = Field(..., description=\"The page number of the element\")\n",
    "\n",
    "class TextElement(UnstructuredElement):\n",
    "    \"\"\"\n",
    "    A TextElement. Structurally identical to the UnstructuredElement class.\n",
    "    This is used to represent text elements that contain no tables or images.\n",
    "    \"\"\"\n",
    "\n",
    "class ImageElement(UnstructuredElement):\n",
    "    \"\"\"\n",
    "    An ImageElement.\n",
    "    \"\"\"\n",
    "    image_base64: str = Field(..., description=\"The base64 encoded image\")\n",
    "    image_mime_type: str = Field(..., description=\"The mime type of the image\")\n",
    "\n",
    "class TableElement(UnstructuredElement):\n",
    "    \"\"\"\n",
    "    A TableElement. \n",
    "    This may also have image features and so it inherits from ImageElement.\n",
    "    \"\"\"\n",
    "    image_base64: str | None = Field(None, description=\"The base64 encoded table\")\n",
    "    image_mime_type: str | None = Field(None, description=\"The mime type of the table\")\n",
    "    text_as_html: str | None = Field(None, description=\"The text of the table as HTML\")\n",
    "    \n",
    "# -------------\n",
    "# Relationships\n",
    "# -------------\n",
    "\n",
    "class ChunkPartOfDocument(BaseModel):\n",
    "    \"\"\"\n",
    "    (:Chunk {id: $chunk_id})-[:PART_OF_DOCUMENT]->(:Document {id: $document_id})\n",
    "    \"\"\"\n",
    "    chunk_id: str = Field(..., description=\"The id of the chunk\")\n",
    "    document_id: str = Field(..., description=\"The id of the document\")\n",
    "\n",
    "class ChunkHasEntity(BaseModel):\n",
    "    \"\"\"\n",
    "    (:Chunk {id: $chunk_id})-[:HAS_ENTITY]->(:Entity {id: $entity_id})\n",
    "    \"\"\"\n",
    "    chunk_id: str = Field(..., description=\"The id of the chunk\")\n",
    "    entity_id: str = Field(..., description=\"The id of the entity\")\n",
    "\n",
    "class UnstructuredElementPartOfChunk(BaseModel):\n",
    "    \"\"\"\n",
    "    (:UnstructuredElement {id: $unstructured_element_id})-[:PART_OF_CHUNK]->(:Chunk {id: $chunk_id})\n",
    "\n",
    "    This covers TextElement, ImageElement and TableElement nodes since they all share the UnstructuredElement label.\n",
    "    \"\"\" \n",
    "    unstructured_element_id: str = Field(..., description=\"The id of the unstructured element\")\n",
    "    chunk_id: str = Field(..., description=\"The id of the chunk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_article_file_name(file_name: str) -> tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Parse the article file name and return the PubMed ID and title.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_name : str\n",
    "        The name of the file to parse.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple[str, str]\n",
    "        The PubMed ID and title of the article.\n",
    "    \"\"\"\n",
    "    doc_id, title = file_name.split(\"-\", 1)\n",
    "    title = title.replace(\"_\", \" \")\n",
    "\n",
    "    return doc_id, title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chunk_has_next_chunk_relationship_dataframe(chunk_dataframe: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create the DataFrame for loading (:Chunk)-[:HAS_NEXT_CHUNK]->(:Chunk) relationships.\n",
    "    \"\"\"\n",
    "    df = chunk_dataframe.copy()\n",
    "    df['next_id'] = df['id'].shift(-1)\n",
    "    df.dropna(inplace=True)\n",
    "    res = df[['id', 'next_id']].rename({\"id\": \"source_id\", \"next_id\": \"target_id\"}, axis=1)\n",
    "    return res\n",
    "\n",
    "def extract_document_title(text_elements_dataframe: pd.DataFrame) -> str:\n",
    "    \"\"\"\n",
    "    Extract the title of the document from the text elements.\n",
    "    Here we assume that the first 'Title' element is the title of the document.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        The title of the document.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return text_elements_dataframe[text_elements_dataframe['type'] == 'Title'].iloc[0]['text']\n",
    "    except Exception as e:\n",
    "        print(f\"Unable to extract document title: {e}\")\n",
    "        return 'unknown title'\n",
    "\n",
    "def parse_node_and_relationship_from_composite_element(composite_element: CompositeElement, parent_document_id: str) -> dict[str, dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Parse the nodes and relationships for a given chunk (CompositeElement). \n",
    "    This will find the following nodes:\n",
    "    * Chunk\n",
    "    * TextElement\n",
    "    * ImageElement\n",
    "    * TableElement\n",
    "    * UnstructuredElement (Shared label for TextElement, ImageElement and TableElement)\n",
    "\n",
    "    And the following relationships:\n",
    "    * (:Chunk)-[:PART_OF_DOCUMENT]->(:Document)\n",
    "    * (:UnstructuredElement)-[:PART_OF_CHUNK]->(:Chunk)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict[str, dict[str, Any]]\n",
    "        A dictionary containing a list of records for each node and relationship type.\n",
    "    \"\"\"\n",
    "    chunk = Chunk(id=composite_element.id, text=composite_element.text, type=composite_element.category)\n",
    "    chunk_part_of_document = ChunkPartOfDocument(chunk_id=chunk.id, document_id=parent_document_id)\n",
    "\n",
    "    text_elements: list[TextElement] = list()\n",
    "    image_elements: list[ImageElement] = list()\n",
    "    table_elements: list[TableElement] = list()\n",
    "    unstructured_element_part_of_chunk: list[UnstructuredElementPartOfChunk] = list()\n",
    "\n",
    "    # Chunks (CompositeElements) are made of many smaller text chunks (UnstructuredElements)\n",
    "    # We can parse what type of elements these subchunks are and load them as well\n",
    "    # This will give us access to images and tables from the document\n",
    "    for element in composite_element.metadata.orig_elements:\n",
    "        match element.category:\n",
    "            case \"NarrativeText\":\n",
    "                text_elements.append(TextElement(id=element.id, \n",
    "                                                 text=element.text, \n",
    "                                                 type=element.category, \n",
    "                                                 page_number=element.metadata.page_number))\n",
    "            case \"Image\":\n",
    "                image_elements.append(ImageElement(id=element.id, \n",
    "                                                   text=element.text,\n",
    "                                                   type=element.category, \n",
    "                                                   page_number=element.metadata.page_number,\n",
    "                                                   image_base64=element.metadata.image_base64, \n",
    "                                                   image_mime_type=element.metadata.image_mime_type))\n",
    "            case \"Table\":\n",
    "                table_elements.append(TableElement(id=element.id, \n",
    "                                                   text=element.text,\n",
    "                                                   type=element.category, \n",
    "                                                   page_number=element.metadata.page_number,\n",
    "                                                   image_base_64=element.metadata.image_base64,\n",
    "                                                   image_mime_type=element.metadata.image_mime_type,\n",
    "                                                   text_as_html=element.metadata.text_as_html))\n",
    "            # Assume some kind of text element if we can't match the category\n",
    "            # Could be headers, figure captions, etc\n",
    "            case _:\n",
    "                try:\n",
    "                    text_elements.append(TextElement(id=element.id, \n",
    "                                                 text=element.text, \n",
    "                                                 type=element.category, \n",
    "                                                 page_number=element.metadata.page_number))\n",
    "                except Exception as e:\n",
    "                    print(f\"Error parsing text element: {e}\")\n",
    "\n",
    "        unstructured_element_part_of_chunk.append(UnstructuredElementPartOfChunk(unstructured_element_id=element.id, chunk_id=chunk.id))\n",
    "\n",
    "        \n",
    "    return {\n",
    "        \"nodes\": {\n",
    "            \"chunk\": [chunk.model_dump()],\n",
    "            \"text_element\": [el.model_dump() for el in text_elements],\n",
    "            \"image_element\": [el.model_dump() for el in image_elements],\n",
    "            \"table_element\": [el.model_dump() for el in table_elements],\n",
    "        },\n",
    "        \"relationships\": {\n",
    "            \"chunk_part_of_document\": [chunk_part_of_document.model_dump()],\n",
    "            \"unstructured_element_part_of_chunk\": [rel.model_dump() for rel in unstructured_element_part_of_chunk],\n",
    "        }\n",
    "    }\n",
    "\n",
    "def parse_nodes_and_relationships_from_composite_elements(composite_elements: list[CompositeElement], parent_doc_id: str) -> dict[str, dict[str, pd.DataFrame]]:\n",
    "    \"\"\"Parse entity nodes and document relationships for a set of chunks (CompositeElements) and their parent document\"\"\"\n",
    "    \n",
    "    data = {\n",
    "        \"nodes\": {\n",
    "            \"document\": list(),\n",
    "            \"chunk\": list(),\n",
    "            \"text_element\": list(),\n",
    "            \"image_element\": list(),\n",
    "            \"table_element\": list(),\n",
    "        },\n",
    "        \"relationships\": {\n",
    "            \"chunk_part_of_document\": list(),\n",
    "            \"unstructured_element_part_of_chunk\": list(),\n",
    "            \"chunk_has_next_chunk\": list()\n",
    "        }\n",
    "    }\n",
    "\n",
    "    for composite_element in composite_elements:\n",
    "        new_data = parse_node_and_relationship_from_composite_element(composite_element, parent_doc_id)\n",
    "        data[\"nodes\"][\"chunk\"].extend(new_data[\"nodes\"][\"chunk\"])\n",
    "        data[\"relationships\"][\"chunk_part_of_document\"].extend(new_data[\"relationships\"][\"chunk_part_of_document\"])\n",
    "        data[\"nodes\"][\"text_element\"].extend(new_data[\"nodes\"][\"text_element\"])\n",
    "        data[\"nodes\"][\"image_element\"].extend(new_data[\"nodes\"][\"image_element\"])\n",
    "        data[\"nodes\"][\"table_element\"].extend(new_data[\"nodes\"][\"table_element\"])\n",
    "        data[\"relationships\"][\"unstructured_element_part_of_chunk\"].extend(new_data[\"relationships\"][\"unstructured_element_part_of_chunk\"])\n",
    "\n",
    "    # convert to pandas dataframe for ingestion\n",
    "    # node DataFrames\n",
    "    data[\"nodes\"][\"chunk\"] = pd.DataFrame(data[\"nodes\"][\"chunk\"])\n",
    "    data[\"nodes\"][\"text_element\"] = pd.DataFrame(data[\"nodes\"][\"text_element\"])\n",
    "    data[\"nodes\"][\"image_element\"] = pd.DataFrame(data[\"nodes\"][\"image_element\"])\n",
    "    data[\"nodes\"][\"table_element\"] = pd.DataFrame(data[\"nodes\"][\"table_element\"])\n",
    "\n",
    "    document_title = extract_document_title(data[\"nodes\"][\"text_element\"])\n",
    "    data[\"nodes\"][\"document\"] = pd.DataFrame([Document(id=parent_doc_id, name=document_title, source=\"pubmed\").model_dump()])\n",
    "\n",
    "    # relationship DataFrames\n",
    "    data[\"relationships\"][\"chunk_part_of_document\"] = pd.DataFrame(data[\"relationships\"][\"chunk_part_of_document\"])\n",
    "    data[\"relationships\"][\"unstructured_element_part_of_chunk\"] = pd.DataFrame(data[\"relationships\"][\"unstructured_element_part_of_chunk\"])\n",
    "    data[\"relationships\"][\"chunk_has_next_chunk\"] = create_chunk_has_next_chunk_relationship_dataframe(data[\"nodes\"][\"chunk\"])\n",
    "\n",
    "    return data\n",
    "\n",
    "# def process_xml_article(file_name: str) -> dict[str, dict[str, pd.DataFrame]]:\n",
    "#     \"\"\"\n",
    "#     Process an article and return the nodes and relationships for ingestion into the knowledge graph.\n",
    "#     Assumes that \n",
    "#     * the article name follows the format \"{pmid}-{title}.xml\"\n",
    "#     * the article is stored in the \"articles/\" directory\n",
    "\n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     file_name : str\n",
    "#         The name of the file to process.\n",
    "\n",
    "#     Returns\n",
    "#     -------\n",
    "#     dict[str, dict[str, pd.DataFrame]]\n",
    "#         A dictionary containing the node and relationship Pandas DataFrames for ingestion into the knowledge graph.\n",
    "#     \"\"\"\n",
    "#     doc_id, title = parse_article_file_name(file_name)\n",
    "#     parent_document = Document(id=str(uuid4()), pm_id=doc_id, name=title, source=\"pubmed\")\n",
    "#     partitioned_doc = partition_xml(\"articles/\" + file_name, \n",
    "#                                     xml_keep_tags=False, \n",
    "#                                     chunking_strategy=\"by_title\", \n",
    "#                                     combine_text_under_n_chars=200, \n",
    "#                                     max_characters=500, \n",
    "#                                     multipage_sections=True)\n",
    "#     # return partitioned_doc\n",
    "#     return parse_nodes_and_relationships_from_chunk_elements(partitioned_doc, parent_document)\n",
    "\n",
    "def process_pdf_article(file_name: str) -> dict[str, dict[str, pd.DataFrame]]:\n",
    "    \"\"\"\n",
    "    Process an article and return the nodes and relationships for ingestion into the knowledge graph.\n",
    "    Assumes that \n",
    "    * the article name follows the format \"{pmid}-{title}.pdf\"\n",
    "    * the article is stored in the \"articles/\" directory\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_name : str\n",
    "        The name of the file to process.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict[str, dict[str, pd.DataFrame]]\n",
    "        A dictionary containing the node and relationship Pandas DataFrames for ingestion into the knowledge graph.\n",
    "    \"\"\"\n",
    "    # doc_id, title = parse_article_file_name(file_name)\n",
    "    doc_id = hashlib.sha256(file_name.encode()).hexdigest()\n",
    "    # parent_document = Document(id=str(uuid4()), pm_id=file_name, name=file_name, source=\"pubmed\")\n",
    "    partitioned_doc = partition_pdf(\"articles/pdf/\" + file_name, \n",
    "                                    strategy=\"hi_res\",                                     \n",
    "                                    extract_images_in_pdf=True,\n",
    "                                    extract_image_block_types=[\"Image\", \"Table\"], \n",
    "                                    extract_image_block_to_payload=True,               \n",
    "                                    # extract_image_block_output_dir=f\"figures/{file_name[:-4]}\",\n",
    "                                    chunking_strategy=\"by_title\", \n",
    "                                    combine_text_under_n_chars=200, \n",
    "                                    max_characters=1000, \n",
    "                                    multipage_sections=True)\n",
    "    # return partitioned_doc\n",
    "    return parse_nodes_and_relationships_from_composite_elements(partitioned_doc, doc_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexical Graph Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will embed the text fields of our lexical graph for vector similarity search. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Domain Graph Schema Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to define our knowledge graph schema. This information will be passed to the entity extraction LLM to control which entities and relationships are pulled out of the text.\n",
    "\n",
    "This is necessary to prevent our schema from growing too large with an unbounded extraction process.\n",
    "\n",
    "We are using Pydantic to define the schema here since it can be used to validate any returned results as well. This ensures that all data we are ingesting into Neo4j adheres to this structure.\n",
    "\n",
    "Here is what our domain graph data model looks like.\n",
    "\n",
    "<img src=\"./assets/images/domain-data-model-v1.png\" alt=\"domain-data-model\" width=\"600px\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Medication(BaseModel):\n",
    "    \"\"\"a substance used for medical treatment, especially a medicine or drug. This is a base medication, not a medication implemented in a study.\"\"\"\n",
    "    \n",
    "    name: str = Field(..., description=\"Name of the medication. Should also be uniquely identifiable.\")\n",
    "    medication_class: str = Field(..., description=\"Drug class (e.g., GLP-1 RA, SGLT2i)\")\n",
    "    mechanism: Optional[str] = Field(None, description=\"Mechanism of action\")\n",
    "    generic_name: Optional[str] = Field(None, description=\"Generic name if different from name\")\n",
    "    brand_names: Optional[List[str]] = Field(None, description=\"Commercial brand names\")\n",
    "    approval_status: Optional[str] = Field(None, description=\"FDA approval status\")\n",
    "    \n",
    "    class Config:\n",
    "        json_schema_extra = {\n",
    "            \"examples\": [\n",
    "                {\n",
    "                    \"name\": \"Semaglutide\", \n",
    "                    \"medication_class\": \"GLP-1 receptor agonist\",\n",
    "                    \"mechanism\": \"GLP-1 receptor activation\",\n",
    "                    \"generic_name\": \"semaglutide\",\n",
    "                    \"brand_names\": [\"Ozempic\", \"Wegovy\", \"Rybelsus\"],\n",
    "                    \"approval_status\": \"FDA approved\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "\n",
    "\n",
    "class StudyMedication(BaseModel):\n",
    "    \"\"\"Study-specific medication usage - how a medication was used in a particular study\"\"\"\n",
    "    \n",
    "    study_medication_id: str = Field(..., description=\"Unique identifier for this study medication instance\")\n",
    "    dosage: Optional[str] = Field(None, description=\"Dosage used in this study\")\n",
    "    route: Optional[str] = Field(None, description=\"Route of administration\")\n",
    "    frequency: Optional[str] = Field(None, description=\"Dosing frequency\")\n",
    "    treatment_duration: Optional[str] = Field(None, description=\"Duration of treatment\")\n",
    "    treatment_arm: Optional[str] = Field(None, description=\"Treatment arm description\")\n",
    "    comparator: Optional[str] = Field(None, description=\"What this was compared against\")\n",
    "    adherence_rate: Optional[float] = Field(None, description=\"Treatment adherence rate\")\n",
    "    formulation: Optional[str] = Field(None, description=\"Specific formulation used\")\n",
    "    \n",
    "    class Config:\n",
    "        json_schema_extra = {\n",
    "            \"examples\": [\n",
    "                {\n",
    "                    \"study_medication_id\": \"STUDY_MED001\",\n",
    "                    \"dosage\": \"1.0 mg\",\n",
    "                    \"route\": \"subcutaneous\",\n",
    "                    \"frequency\": \"weekly\",\n",
    "                    \"treatment_duration\": \"12 weeks\",\n",
    "                    \"treatment_arm\": \"Active treatment group\",\n",
    "                    \"comparator\": \"placebo\",\n",
    "                    \"adherence_rate\": 85.5,\n",
    "                    \"formulation\": \"pre-filled pen\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "\n",
    "\n",
    "class ClinicalOutcome(BaseModel):\n",
    "    \"\"\"Measured clinical outcomes and biomarkers\"\"\"\n",
    "    \n",
    "    clinical_outcome_id: str = Field(..., description=\"Unique identifier for the outcome\")\n",
    "    name: str = Field(..., description=\"Name of the clinical outcome\")\n",
    "    category: str = Field(..., description=\"Category of outcome\")\n",
    "    measurement_unit: Optional[str] = Field(None, description=\"Unit of measurement\")\n",
    "    normal_range: Optional[str] = Field(None, description=\"Normal or target range when applicable\")\n",
    "    baseline_value: Optional[float] = Field(None, description=\"Baseline measurement value\")\n",
    "    post_treatment_value: Optional[float] = Field(None, description=\"Post-treatment measurement value\")\n",
    "    change_from_baseline: Optional[float] = Field(None, description=\"Change from baseline\")\n",
    "    p_value: Optional[float] = Field(None, description=\"Statistical significance if reported\")\n",
    "    confidence_interval: Optional[str] = Field(None, description=\"95% confidence interval\")\n",
    "    effect_size: Optional[float] = Field(None, description=\"Standardized effect size\")\n",
    "    \n",
    "    class Config:\n",
    "        json_schema_extra = {\n",
    "            \"examples\": [\n",
    "                {\n",
    "                    \"clinical_outcome_id\": \"OUT001\",\n",
    "                    \"name\": \"HbA1c\",\n",
    "                    \"category\": \"Glycemic control\",\n",
    "                    \"measurement_unit\": \"%\",\n",
    "                    \"normal_range\": \"<7.0%\",\n",
    "                    \"baseline_value\": 8.5,\n",
    "                    \"post_treatment_value\": 7.2,\n",
    "                    \"change_from_baseline\": -1.3,\n",
    "                    \"p_value\": 0.001,\n",
    "                    \"confidence_interval\": \"[-1.8, -0.8]\",\n",
    "                    \"effect_size\": -0.8\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "\n",
    "\n",
    "class MedicalCondition(BaseModel):\n",
    "    \"\"\"Medical conditions and comorbidities studied\"\"\"\n",
    "    \n",
    "    name: str = Field(..., description=\"Name of the medical condition\")\n",
    "    category: str = Field(..., description=\"Category of condition\")\n",
    "    severity: Optional[str] = Field(None, description=\"Severity or stage when specified\")\n",
    "    icd10_code: Optional[str] = Field(None, description=\"ICD-10 code when available\")\n",
    "    duration: Optional[str] = Field(None, description=\"Duration of condition if specified\")\n",
    "    prevalence: Optional[float] = Field(None, description=\"Prevalence in study population\")\n",
    "    \n",
    "    class Config:\n",
    "        json_schema_extra = {\n",
    "            \"examples\": [\n",
    "                {\n",
    "                    \"name\": \"Type 2 diabetes mellitus\",\n",
    "                    \"category\": \"Primary condition\", \n",
    "                    \"severity\": \"moderate\",\n",
    "                    \"icd10_code\": \"E11\",\n",
    "                    \"duration\": \"5-10 years\",\n",
    "                    \"prevalence\": 100.0\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "\n",
    "\n",
    "class StudyPopulation(BaseModel):\n",
    "    \"\"\"Patient populations and demographics in research studies\"\"\"\n",
    "    \n",
    "    study_population_id: str = Field(..., description=\"Unique identifier for the population\")\n",
    "    description: str = Field(..., description=\"Description of the population\")\n",
    "    age_range: Optional[str] = Field(None, description=\"Age range\")\n",
    "    mean_age: Optional[float] = Field(None, description=\"Mean age in years\")\n",
    "    male_percentage: Optional[float] = Field(None, description=\"Percentage of male gender participants\")\n",
    "    female_percentage: Optional[float] = Field(None, description=\"Percentage of female gender participants\")\n",
    "    other_gender_percentage: Optional[float] = Field(None, description=\"Percentage of participants that identify as another gender\")\n",
    "    sample_size: Optional[int] = Field(None, description=\"Number of participants\")\n",
    "    study_type: str = Field(..., description=\"Type of study\")\n",
    "    location: Optional[str] = Field(None, description=\"Geographic location of study\")\n",
    "    inclusion_criteria: Optional[List[str]] = Field(None, description=\"Key inclusion criteria\")\n",
    "    exclusion_criteria: Optional[List[str]] = Field(None, description=\"Key exclusion criteria\")\n",
    "    study_duration: Optional[str] = Field(None, description=\"Duration of study\")\n",
    "    \n",
    "    class Config:\n",
    "        json_schema_extra = {\n",
    "            \"examples\": [\n",
    "                {\n",
    "                    \"study_population_id\": \"POP001\",\n",
    "                    \"description\": \"Adults with T2DM and schizophrenia\",\n",
    "                    \"age_range\": \"18-65 years\",\n",
    "                    \"mean_age\": 43.8,\n",
    "                    \"female_percentage\": 47.0,\n",
    "                    \"male_percentage\": 53.0,\n",
    "                    \"sample_size\": 354,\n",
    "                    \"study_type\": \"Observational study\",\n",
    "                    \"location\": \"Denmark\",\n",
    "                    \"inclusion_criteria\": [\"Type 2 diabetes diagnosis\", \"Schizophrenia diagnosis\", \"Age ≥18\"],\n",
    "                    \"study_duration\": \"12 months\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "\n",
    "\n",
    "# Relationship classes\n",
    "class StudyMedicationUsesMedication(BaseModel):\n",
    "    \"\"\"Links study medication to base medication\"\"\"\n",
    "    study_medication_id: str\n",
    "    medication_name: str\n",
    "\n",
    "\n",
    "class StudyMedicationProducesClinicalOutcome(BaseModel):\n",
    "    \"\"\"Links study medication usage to clinical outcomes\"\"\"\n",
    "    study_medication_id: str\n",
    "    clinical_outcome_name: str\n",
    "\n",
    "\n",
    "class StudyPopulationHasMedicalCondition(BaseModel):\n",
    "    \"\"\"Relationship between study population and medical conditions\"\"\"\n",
    "    study_population_id: str\n",
    "    medical_condition_name: str\n",
    "\n",
    "\n",
    "class StudyPopulationReceivesStudyMedication(BaseModel):\n",
    "    \"\"\"Relationship between study population and study medication\"\"\"\n",
    "    study_population_id: str\n",
    "    study_medication_id: str\n",
    "\n",
    "\n",
    "class StudyPopulationHasOutcome(BaseModel):\n",
    "    \"\"\"Direct relationship between population and outcomes (for population-level measurements)\"\"\"\n",
    "    study_population_id: str\n",
    "    clinical_outcome_name: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lexical and domain knowledge graphs will be linked with `HAS_ENTITY` relationships between Chunk nodes and domain graph nodes.\n",
    "\n",
    "This is the combined lexical and domain graph data model.\n",
    "\n",
    "IMAGE OF DATA MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity Extraction via LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using [OpenAI](https://platform.openai.com/docs/overview) and the [Instructor](https://python.useinstructor.com/) library to perform our entity extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AsyncOpenAI\n",
    "import instructor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructor handles requesting structured outputs from the LLM. \n",
    "\n",
    "If the LLM fails to return output that adheres to the response models, Instructor will also handle the retry logic and pass any errors to inform corrections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = instructor.from_openai(AsyncOpenAI())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the system prompt defines the overall behavior of the LLM\n",
    "system_prompt = \"\"\"\n",
    "You are a healthcare research expert that is responsible for extracting detailed entities from PubMed articles. \n",
    "You will be provided a graph data model schema and must extract entities and relationships to populate a knowledge graph.\n",
    "\"\"\"\n",
    "\n",
    "async def extract_entities_from_text_chunk(text_chunk: str) -> list:\n",
    "    \"\"\"\n",
    "    Extract entities and relationships from a text chunk.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text_chunk : str\n",
    "        The text chunk to extract entities from.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list[Medication | StudyMedication | StudyMedicationUsesMedication]\n",
    "        A list of entities and relationships extracted from the text chunk.\n",
    "    \"\"\"\n",
    "\n",
    "    response = await client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": text_chunk}\n",
    "        ],\n",
    "        response_model=list[Medication | StudyMedication | StudyMedicationUsesMedication],\n",
    "    )\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def extract_entities_from_chunk_nodes(chunk_nodes: pd.DataFrame) -> list[tuple[str, list[Any]]]:\n",
    "    \"\"\"\n",
    "    Process a Pandas DataFrame of Chunk nodes and return the entities found in each chunk.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    chunk_nodes : pd.DataFrame\n",
    "        A Pandas DataFrame where each row represents a Chunk node.\n",
    "        Has columns `id` and `text`.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    list[tuple[str, list[dict[str, Any]]]]\n",
    "        A list of tuples, where the first element is the chunk id and the second element is a list of entities found in the chunk.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create tasks for all nodes\n",
    "    # order is maintained\n",
    "    tasks = [extract_entities_from_text_chunk(row[\"text\"]) for _, row in chunk_nodes.iterrows()]\n",
    "\n",
    "    # Execute all tasks concurrently\n",
    "    extraction_results = await asyncio.gather(*tasks)\n",
    "\n",
    "    # Return chunk_id paired with its entities\n",
    "    return [(chunk_id, entities) for chunk_id, entities in zip(chunk_nodes[\"id\"], extraction_results)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to collect the article file names to pass to Unstructured for parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_names = os.listdir(\"articles/pdf/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now defined \n",
    "* Lexical and domain data models\n",
    "* Partitioning and chunking logic for articles\n",
    "* Entity extraction logic for chunks\n",
    "\n",
    "It is now time to define our ingestion logic. We will run ingest in three stages \n",
    "\n",
    "1. Load lexical graph\n",
    "2. Embed lexical graph Chunk nodes\n",
    "3. Extract domain / entity graph from lexical graph\n",
    "\n",
    "Decoupling these stages allows us easily make changes as we iterate our ingestion process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from pyneoinstance import Neo4jInstance, load_yaml_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our database credentials and all of our queries are stored in the `pyneoinstance_config.yaml` file. \n",
    "\n",
    "This makes it easy to manage our queries and keeps the notebook code clean. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_yaml_file(\"pyneoinstance_config.yaml\")\n",
    "\n",
    "db_info = config['db_info']\n",
    "\n",
    "constraints = config['initializing_queries']['constraints']\n",
    "indexes = config['initializing_queries']['indexes']\n",
    "\n",
    "node_load_queries = config['loading_queries']['nodes']\n",
    "relationship_load_queries = config['loading_queries']['relationships']\n",
    "\n",
    "processing_queries = config['processing_queries']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This graph object will handle database connections and read / write transactions for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = Neo4jInstance(db_info.get('uri', os.getenv(\"NEO4J_URI\", \"neo4j://localhost:7687\")), # use config value -> use env value -> use default value\n",
    "                      db_info.get('user', os.getenv(\"NEO4J_USER\", \"neo4j\")), \n",
    "                      db_info.get('password', os.getenv(\"NEO4J_PASSWORD\", \"password\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a helper function for ingesting data using the PyNeoInstance library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_partition(data: pd.DataFrame, batch_size: int = 500) -> int:\n",
    "    \"\"\"\n",
    "    Determine the data partition based on the desired batch size.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : pd.DataFrame\n",
    "        The Pandas DataFrame to partition.\n",
    "    batch_size : int\n",
    "        The desired batch size.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        The partition size.\n",
    "    \"\"\"\n",
    "    \n",
    "    partition = int(len(data) / batch_size)\n",
    "    print(\"partition: \"+str(partition if partition > 1 else 1))\n",
    "    return partition if partition > 1 else 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we write all the constraints and indexes we need for both the lexical and domain graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_constraints_and_indexes() -> None:\n",
    "    \"\"\"\n",
    "    Create constraints and indexes for the graph.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if constraints and len(constraints) > 0:\n",
    "            graph.execute_write_queries(database=db_info['database'], queries=list(constraints.values()))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    try:\n",
    "        if indexes and len(indexes) > 0:\n",
    "            graph.execute_write_queries(database=db_info['database'], queries=list(indexes.values()))\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_constraints_and_indexes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingest Lexical Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing | Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process the articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['jciinsight-3-93936.pdf']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexical_ingest_records = process_pdf_article(article_names[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the first few records "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1e5cfcba99bcd1dfb5aeb0cbb4c95b7ca3f35c8210a04e...</td>\n",
       "      <td>Metformin-induced glucagon-like peptide-1 secr...</td>\n",
       "      <td>pubmed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  id  \\\n",
       "0  1e5cfcba99bcd1dfb5aeb0cbb4c95b7ca3f35c8210a04e...   \n",
       "\n",
       "                                                name  source  \n",
       "0  Metformin-induced glucagon-like peptide-1 secr...  pubmed  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexical_ingest_records[\"nodes\"][\"document\"][:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Document and Chunk nodes into the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_lexical_nodes(document_dataframe: pd.DataFrame, \n",
    "                       chunk_dataframe: pd.DataFrame, \n",
    "                       text_element_dataframe: pd.DataFrame, \n",
    "                       image_element_dataframe: pd.DataFrame, \n",
    "                       table_element_dataframe: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Load lexical nodes into the graph. These include Document and Chunk nodes.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    document_dataframe : pd.DataFrame\n",
    "        A Pandas DataFrame of Document nodes to load into the graph. \n",
    "    chunk_dataframe : pd.DataFrame\n",
    "        A Pandas DataFrame of Chunk nodes to load into the graph.\n",
    "    text_element_dataframe : pd.DataFrame\n",
    "        A Pandas DataFrame of TextElement nodes to load into the graph. \n",
    "    image_element_dataframe : pd.DataFrame\n",
    "        A Pandas DataFrame of ImageElement nodes to load into the graph. \n",
    "    table_element_dataframe : pd.DataFrame\n",
    "        A Pandas DataFrame of TableElement nodes to load into the graph. \n",
    "    \"\"\"\n",
    "    \n",
    "    lexical_nodes_ingest_iterator = list(zip([document_dataframe, \n",
    "                                              chunk_dataframe, \n",
    "                                              text_element_dataframe, \n",
    "                                              image_element_dataframe, \n",
    "                                              table_element_dataframe], \n",
    "                                              ['document', \n",
    "                                               'chunk', \n",
    "                                               'text_element', \n",
    "                                               'image_element', \n",
    "                                               'table_element']))\n",
    "\n",
    "    for data, query in lexical_nodes_ingest_iterator:\n",
    "        res = graph.execute_write_query_with_data(database=db_info['database'], \n",
    "                                                    data=data, \n",
    "                                                    query=node_load_queries[query], \n",
    "                                                    partitions=get_partition(data, batch_size=500),\n",
    "                                                    parallel=True,\n",
    "                                                    workers=2)\n",
    "        print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "partition: 1\n",
      "{'labels_added': 1, 'nodes_created': 1, 'properties_set': 3}\n",
      "partition: 1\n",
      "{'labels_added': 101, 'nodes_created': 101, 'properties_set': 303}\n",
      "partition: 1\n",
      "{'labels_added': 408, 'nodes_created': 204, 'properties_set': 816}\n",
      "partition: 1\n",
      "{'labels_added': 38, 'nodes_created': 19, 'properties_set': 114}\n",
      "partition: 1\n",
      "{'labels_added': 6, 'nodes_created': 3, 'properties_set': 21}\n"
     ]
    }
   ],
   "source": [
    "load_lexical_nodes(lexical_ingest_records[\"nodes\"][\"document\"], \n",
    "                   lexical_ingest_records[\"nodes\"][\"chunk\"], \n",
    "                   lexical_ingest_records[\"nodes\"][\"text_element\"], \n",
    "                   lexical_ingest_records[\"nodes\"][\"image_element\"], \n",
    "                   lexical_ingest_records[\"nodes\"][\"table_element\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the relationships into the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_lexical_relationships(chunk_part_of_document_dataframe: pd.DataFrame, unstructured_element_part_of_chunk_dataframe: pd.DataFrame, chunk_has_next_chunk_dataframe: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Load lexical relationships into the graph. This includes the Chunk - PART_OF -> Document relationship.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    chunk_part_of_document_dataframe : pd.DataFrame\n",
    "        A Pandas DataFrame of Chunk - PART_OF -> Document relationships to load into the graph.\n",
    "        Should have columns `chunk_id` and `document_id`.\n",
    "    unstructured_element_part_of_chunk_dataframe : pd.DataFrame\n",
    "        A Pandas DataFrame of UnstructuredElement - PART_OF -> Chunk relationships to load into the graph.\n",
    "        Should have columns `unstructured_element_id` and `chunk_id`.\n",
    "    chunk_has_next_chunk_dataframe : pd.DataFrame\n",
    "        A Pandas DataFrame of Chunk - HAS_NEXT_CHUNK -> Chunk relationships to load into the graph.\n",
    "        Should have columns `source_id` and `target_id`.\n",
    "    \"\"\"\n",
    "    lexical_relationships_ingest_iterator = list(zip([chunk_part_of_document_dataframe, \n",
    "                                                      unstructured_element_part_of_chunk_dataframe, \n",
    "                                                      chunk_has_next_chunk_dataframe], \n",
    "                                                      ['chunk_part_of_document', \n",
    "                                                       'unstructured_element_part_of_chunk', \n",
    "                                                       'chunk_has_next_chunk']))\n",
    "\n",
    "    for data, query in lexical_relationships_ingest_iterator:\n",
    "        res = graph.execute_write_query_with_data(database=db_info['database'], \n",
    "                                                    data=data, \n",
    "                                                    query=relationship_load_queries[query], \n",
    "                                                    partitions=get_partition(data, batch_size=500))\n",
    "        print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "partition: 1\n",
      "{'relationships_created': 101}\n",
      "partition: 1\n",
      "{'relationships_created': 248}\n",
      "partition: 1\n",
      "{'relationships_created': 100}\n"
     ]
    }
   ],
   "source": [
    "load_lexical_relationships(lexical_ingest_records[\"relationships\"][\"chunk_part_of_document\"], \n",
    "                          lexical_ingest_records[\"relationships\"][\"unstructured_element_part_of_chunk\"],\n",
    "                          lexical_ingest_records[\"relationships\"][\"chunk_has_next_chunk\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embed Lexical Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will read Chunk nodes from the graph that don't have embedding properties yet. \n",
    "\n",
    "We will then embed the Chunk text property and add the embedding as a property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_index = ...\n",
    "\n",
    "def create_vector_index() -> None:\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(driver) -> None:\n",
    "    ...\n",
    "\n",
    "def embed_lexical_graph(driver) -> None:\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Entities from Lexical Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now perform entity extraction on the Chunk nodes to augment and connect to our domain graph containing patient journey information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunk_nodes_to_process_by_article_name(article_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Retrieve Chunk node id and text from the database that have a relationship to the Document with the article name provided.\n",
    "    These chunks may then be used as input to the entity extraction process.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    article_name : str\n",
    "        The name of the article to retrieve chunks for.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A Pandas DataFrame where each row represents a Chunk node connected to the Document with the article name provided.\n",
    "        Has columns `id` and `text`.\n",
    "    \"\"\" \n",
    "    doc_id, title = parse_article_file_name(article_name)\n",
    "    return graph.execute_read_query(database=db_info['database'], \n",
    "                            parameters={\"article_name\": title}, \n",
    "                            query=processing_queries['get_chunk_nodes_to_process_by_article_name'], \n",
    "                        )\n",
    "\n",
    "def get_chunk_nodes_to_embed(min_length: int = 100) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Retrieve Chunk node id and text from the database that don't have an embedding.\n",
    "    These chunks may then be used as input to the entity extraction process.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    min_length : int\n",
    "        The minimum length the text must be to be included in the DataFrame.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A Pandas DataFrame where each row represents a Chunk node that has text and is at least `min_length` characters long.\n",
    "        Has columns `id` and `text`.\n",
    "    \"\"\"\n",
    "    return graph.execute_read_query(database=db_info['database'], \n",
    "                            query=processing_queries['get_chunk_nodes_to_embed'], \n",
    "                            parameters={\"min_length\": min_length},\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownPropertyKeyWarning} {category: UNRECOGNIZED} {title: The provided property key is not in the database} {description: One of the property names in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing property name is: embedding)} {position: line: 4, column: 9, offset: 83} for query: 'MATCH (c:Chunk)\\nWHERE c.text IS NOT NULL\\n  AND size(c.text) >= $min_length\\n  AND c.embedding IS NULL\\nRETURN c.id as id, c.text as text'\n"
     ]
    }
   ],
   "source": [
    "chunks_to_process = get_chunk_nodes_to_embed(min_length=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 99 chunks to process\n",
      "\n",
      "First chunk:\n",
      "\n",
      "statistical significance (metformin + Ex9-39 vs. placebo + Ex9-39, P = 0.053). The glucose iAUC after metformin + saline was significantly smaller than the iAUC for metformin + Ex9-39 (P = 0.004). Based on individual iAUC values, the relative contribution of GLP-1 to the acute glucose-lowering effect of metformin was 75% ± 35%, calculated as follows: 100% × ([iAUCplacebo + saline – iAUCmetformin + saline] – [iAUCplacebo + Ex9–39 – iAUCmetformin + Ex9–39])/(iAUCplacebo + saline – iAUCmetformin + saline) (P = 0.05). Using a 2-way ANOVA, both metformin and Ex9-39 were shown to significantly affect postprandial plasma glucose (iAUC) (P = 0.005 and P = 0.002, respectively), but no interaction between the 2 factors was evident. The time courses of the C-peptide/glucose ratios are illustrated in Figure 2B, and the AUCs for C-peptide/glucose, insulin/glucose, and insulin secretion\n"
     ]
    }
   ],
   "source": [
    "print(f\"Found {len(chunks_to_process)} chunks to process\\n\")\n",
    "print(f\"First chunk:\\n\\n{chunks_to_process.loc[0,'text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_ingest_records = await extract_entities_from_chunk_nodes(chunks_to_process[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('0024e9c2d7afcf519d4d13871816a21d',\n",
       "  [StudyMedication(study_medication_id='STUDY_MED002', dosage=None, route=None, frequency=None, treatment_duration=None, treatment_arm='metformin + Ex9-39', comparator='placebo + Ex9-39', adherence_rate=None, formulation=None),\n",
       "   StudyMedication(study_medication_id='STUDY_MED003', dosage=None, route=None, frequency=None, treatment_duration=None, treatment_arm='metformin + saline', comparator='metformin + Ex9-39', adherence_rate=None, formulation=None),\n",
       "   StudyMedication(study_medication_id='STUDY_MED004', dosage=None, route=None, frequency=None, treatment_duration=None, treatment_arm='placebo + saline', comparator=None, adherence_rate=None, formulation=None),\n",
       "   Medication(name='Metformin', medication_class='Biguanide', mechanism='Decreases hepatic glucose production, increases insulin sensitivity', generic_name=None, brand_names=None, approval_status='FDA approved')]),\n",
       " ('0422c069e545cbe0193630bfec136371',\n",
       "  [Medication(name='Metformin', medication_class='Biguanides', mechanism='Inhibition of a negative feedback loop', generic_name=None, brand_names=None, approval_status=None)])]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_ingest_records[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingest Entities Into Knowledge Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These functions load the extracted entities and relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These functions link the extracted entities with their text chunk nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENTITY_LABELS = {\n",
    "    \"Medication\", \n",
    "    \"StudyMedication\",\n",
    "    \"MedicalCondition\",\n",
    "    \"StudyPopulation\",\n",
    "    \"ClinicalOutcome\",\n",
    "}\n",
    "\n",
    "ENTITY_RELS = {\n",
    "    \"StudyMedicationUsesMedication\",\n",
    "    \"StudyMedicationProducesClinicalOutcome\",\n",
    "    \"StudyPopulationHasMedicalCondition\",\n",
    "    \"StudyPopulationReceivesStudyMedication\",\n",
    "    \"StudyPopulationHasOutcome\",\n",
    "}\n",
    "\n",
    "def prepare_entities_for_ingestion(entities: list[tuple[str, list[Any]]]) -> dict[str, dict[str, pd.DataFrame]]:\n",
    "    \"\"\"\n",
    "    Prepare entities for ingestion into the graph.\n",
    "    This function takes the results of the `get_chunk_nodes_to_process_by_article_name` function and returns a dictionary of entity label to list of entities.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    entities : list[tuple[str, list[Any]]]\n",
    "        A list of tuples, where the first element is the chunk id and the second element is a list of entities found in the chunk.\n",
    "        Entities are Pydantic models that adhere to the domain graph data model.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict[str, dict[str, pd.DataFrame]]\n",
    "        A dictionary of entity label to pandas dataframe of entities.\n",
    "\n",
    "        {\n",
    "            \"nodes\": {\n",
    "                \"Medication\": pd.DataFrame(...),\n",
    "                \"StudyMedication\": pd.DataFrame(...),\n",
    "                ...\n",
    "            },\n",
    "            \"relationships\": {\n",
    "                \"StudyMedicationUsesMedication\": pd.DataFrame(...),\n",
    "                \"StudyMedicationProducesClinicalOutcome\": pd.DataFrame(...),\n",
    "                ...\n",
    "            }\n",
    "        }\n",
    "    \"\"\"\n",
    "\n",
    "    records_node_dict = {lbl: list() for lbl in ENTITY_LABELS}\n",
    "    records_rel_dict = {lbl: list() for lbl in ENTITY_RELS}\n",
    "\n",
    "    for chunk_id, entities in entities:\n",
    "        for entity in entities:\n",
    "            to_add = entity.model_dump()\n",
    "            to_add.update({\"chunk_id\": chunk_id})\n",
    "            # nodes\n",
    "            if entity.__class__.__name__ in ENTITY_LABELS:\n",
    "                records_node_dict[entity.__class__.__name__].append(to_add)\n",
    "            # rels\n",
    "            elif entity.__class__.__name__ in ENTITY_RELS:\n",
    "                records_rel_dict[entity.__class__.__name__].append(to_add)\n",
    "            else:\n",
    "                print(f\"Unknown entity type: {entity.__class__.__name__}\")\n",
    "\n",
    "    for key, value in records_node_dict.items():\n",
    "        records_node_dict[key] = pd.DataFrame(value)\n",
    "\n",
    "    for key, value in records_rel_dict.items():\n",
    "        records_rel_dict[key] = pd.DataFrame(value)\n",
    "\n",
    "    return {\"nodes\": records_node_dict, \"relationships\": records_rel_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_entity_nodes(medication_dataframe: pd.DataFrame, \n",
    "                      medical_condition_dataframe: pd.DataFrame, \n",
    "                      study_medication_dataframe: pd.DataFrame, \n",
    "                      study_population_dataframe: pd.DataFrame, \n",
    "                      clinical_outcome_dataframe: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Load entity nodes into the graph.\n",
    "    \"\"\"\n",
    "    \n",
    "    entity_nodes_ingest_iterator = list(zip([medication_dataframe, \n",
    "                                             medical_condition_dataframe, \n",
    "                                             study_medication_dataframe, \n",
    "                                             study_population_dataframe, \n",
    "                                             clinical_outcome_dataframe], \n",
    "                                             ['medication', \n",
    "                                              'medical_condition', \n",
    "                                              'study_medication', \n",
    "                                              'study_population', \n",
    "                                              'clinical_outcome']))\n",
    "\n",
    "    for data, query in entity_nodes_ingest_iterator:\n",
    "        print(f\"Loading {len(data)} {query} nodes\")\n",
    "        if len(data) > 0:\n",
    "            res = graph.execute_write_query_with_data(database=db_info['database'], \n",
    "                                                    data=data, \n",
    "                                                    query=node_load_queries[query], \n",
    "                                                    partitions=get_partition(data, batch_size=500),\n",
    "                                                    parallel=False)\n",
    "            print(res)\n",
    "        else:\n",
    "            print(f\"No {query} nodes to load\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_entity_relationships(study_medication_uses_medication_dataframe: pd.DataFrame,\n",
    "                              study_medication_produces_clinical_outcome_dataframe: pd.DataFrame,\n",
    "                              study_population_has_medical_condition_dataframe: pd.DataFrame,\n",
    "                              study_population_receives_study_medication_dataframe: pd.DataFrame,\n",
    "                              study_population_has_outcome_dataframe: pd.DataFrame,\n",
    "                              ) -> None:\n",
    "    \"\"\"\n",
    "    Load entity relationships into the graph.\n",
    "    \"\"\"\n",
    "    entity_relationships_ingest_iterator = list(zip([study_medication_uses_medication_dataframe, \n",
    "                                                      study_medication_produces_clinical_outcome_dataframe, \n",
    "                                                      study_population_has_medical_condition_dataframe, \n",
    "                                                      study_population_receives_study_medication_dataframe, \n",
    "                                                      study_population_has_outcome_dataframe], \n",
    "                                                      ['study_medication_uses_medication', \n",
    "                                                       'study_medication_produces_clinical_outcome', \n",
    "                                                       'study_population_has_medical_condition', \n",
    "                                                       'study_population_receives_study_medication', \n",
    "                                                       'study_population_has_outcome']))\n",
    "    \n",
    "    for data, query in entity_relationships_ingest_iterator:\n",
    "        print(f\"Loading {len(data)} {query} relationships\")\n",
    "        if len(data) > 0:\n",
    "            res = graph.execute_write_query_with_data(database=db_info['database'], \n",
    "                                                    data=data, \n",
    "                                                    query=relationship_load_queries[query], \n",
    "                                                    partitions=get_partition(data, batch_size=500),\n",
    "                                                    parallel=False)\n",
    "            print(res)\n",
    "        else:\n",
    "            print(f\"No {query} relationships to load\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def link_entities_to_chunks(medication_link_dataframe: pd.DataFrame, \n",
    "                      medical_condition_link_dataframe: pd.DataFrame, \n",
    "                      study_medication_link_dataframe: pd.DataFrame, \n",
    "                      study_population_link_dataframe: pd.DataFrame, \n",
    "                      clinical_outcome_link_dataframe: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Link entities to chunks.\n",
    "    \"\"\"\n",
    "    entity_link_iterator = list(zip([medication_link_dataframe, \n",
    "                                     medical_condition_link_dataframe, \n",
    "                                     study_medication_link_dataframe, \n",
    "                                     study_population_link_dataframe, \n",
    "                                     clinical_outcome_link_dataframe], \n",
    "                                     [\"chunk_has_entity_medication\",\n",
    "                                      \"chunk_has_entity_medical_condition\",\n",
    "                                      \"chunk_has_entity_study_medication\",\n",
    "                                      \"chunk_has_entity_study_population\",\n",
    "                                      \"chunk_has_entity_clinical_outcome\"]))\n",
    "    \n",
    "    for data, query in entity_link_iterator:\n",
    "        print(f\"Linking {len(data)} {query} entities to chunks\")\n",
    "        if len(data) > 0:\n",
    "            res = graph.execute_write_query_with_data(database=db_info['database'], \n",
    "                                                    data=data, \n",
    "                                                    query=relationship_load_queries[query], \n",
    "                                                    partitions=get_partition(data, batch_size=500),\n",
    "                                                    parallel=False)\n",
    "            print(res)\n",
    "        else:\n",
    "            print(f\"No {query} relationships to load\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingest_records = prepare_entities_for_ingestion(entity_ingest_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(6)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ingest_records[\"nodes\"][\"Medication\"]['generic_name'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 7 medication nodes\n",
      "partition: 1\n",
      "{'labels_added': 4, 'nodes_created': 4, 'properties_set': 39}\n",
      "Loading 0 medical_condition nodes\n",
      "No medical_condition nodes to load\n",
      "Loading 3 study_medication nodes\n",
      "partition: 1\n",
      "{'labels_added': 3, 'nodes_created': 3, 'properties_set': 27}\n",
      "Loading 0 study_population nodes\n",
      "No study_population nodes to load\n",
      "Loading 0 clinical_outcome nodes\n",
      "No clinical_outcome nodes to load\n"
     ]
    }
   ],
   "source": [
    "load_entity_nodes(ingest_records[\"nodes\"][\"Medication\"], \n",
    "                  ingest_records[\"nodes\"][\"MedicalCondition\"], \n",
    "                  ingest_records[\"nodes\"][\"StudyMedication\"], \n",
    "                  ingest_records[\"nodes\"][\"StudyPopulation\"], \n",
    "                  ingest_records[\"nodes\"][\"ClinicalOutcome\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 0 study_medication_uses_medication relationships\n",
      "No study_medication_uses_medication relationships to load\n",
      "Loading 0 study_medication_produces_clinical_outcome relationships\n",
      "No study_medication_produces_clinical_outcome relationships to load\n",
      "Loading 0 study_population_has_medical_condition relationships\n",
      "No study_population_has_medical_condition relationships to load\n",
      "Loading 0 study_population_receives_study_medication relationships\n",
      "No study_population_receives_study_medication relationships to load\n",
      "Loading 0 study_population_has_outcome relationships\n",
      "No study_population_has_outcome relationships to load\n"
     ]
    }
   ],
   "source": [
    "load_entity_relationships(ingest_records[\"relationships\"][\"StudyMedicationUsesMedication\"], \n",
    "                          ingest_records[\"relationships\"][\"StudyMedicationProducesClinicalOutcome\"], \n",
    "                          ingest_records[\"relationships\"][\"StudyPopulationHasMedicalCondition\"], \n",
    "                          ingest_records[\"relationships\"][\"StudyPopulationReceivesStudyMedication\"], \n",
    "                          ingest_records[\"relationships\"][\"StudyPopulationHasOutcome\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linking 7 chunk_has_entity_medication entities to chunks\n",
      "partition: 1\n",
      "{'relationships_created': 7}\n",
      "Linking 0 chunk_has_entity_medical_condition entities to chunks\n",
      "No chunk_has_entity_medical_condition relationships to load\n",
      "Linking 3 chunk_has_entity_study_medication entities to chunks\n",
      "partition: 1\n",
      "{'relationships_created': 3}\n",
      "Linking 0 chunk_has_entity_study_population entities to chunks\n",
      "No chunk_has_entity_study_population relationships to load\n",
      "Linking 0 chunk_has_entity_clinical_outcome entities to chunks\n",
      "No chunk_has_entity_clinical_outcome relationships to load\n"
     ]
    }
   ],
   "source": [
    "link_entities_to_chunks(ingest_records[\"nodes\"][\"Medication\"], \n",
    "                        ingest_records[\"nodes\"][\"MedicalCondition\"], \n",
    "                        ingest_records[\"nodes\"][\"StudyMedication\"], \n",
    "                        ingest_records[\"nodes\"][\"StudyPopulation\"], \n",
    "                        ingest_records[\"nodes\"][\"ClinicalOutcome\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
