{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PubMed Knowledge Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook walks through the process of generating a knowledge graph of PubMed articles.\n",
    "\n",
    "This notebook will\n",
    "* Download a selection of articles from PubMed\n",
    "* Define a knowledge graph schema\n",
    "* Extract entities from the articles according to the defined schema\n",
    "* Populate a Neo4j instance with articles and extracted entities\n",
    "* Connect extracted entities with existing patient journey data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook requires a local repo of articles. You may download a sample of 20 PubMed articles by running the following command.\n",
    "\n",
    "```bash\n",
    "python3 ./scripts/fetch_pubmed_articles.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Optional, List\n",
    "import asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allows for async operations in notebooks\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexical Graph Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use Unstructured.IO to partition and chunk our articles. \n",
    "\n",
    "This process breaks the articles into sensible chunks that may be used as context in our application. \n",
    "\n",
    "These chunks will also have relationships to the extracted entities.\n",
    "\n",
    "IMAGE OF LEXICAL DATA MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pydantic import BaseModel, Field\n",
    "from unstructured.partition.auto import partition\n",
    "from unstructured.documents.elements import CompositeElement\n",
    "\n",
    "from uuid import uuid4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Document(BaseModel):\n",
    "    id: str = Field(..., description=\"The id of the document\")\n",
    "    name: str = Field(..., description=\"The name of the document\")\n",
    "    source: str = Field(..., description=\"The source of the document\")\n",
    "\n",
    "class Chunk(BaseModel):\n",
    "    id: str = Field(..., description=\"The id of the chunk\")\n",
    "    text: str = Field(..., description=\"The text of the chunk\")\n",
    "\n",
    "class ChunkWithEmbedding(Chunk):\n",
    "    embedding: list[float] = Field(..., description=\"The embedding of the chunk text field\")\n",
    "\n",
    "class ChunkPartOfDocument(BaseModel):\n",
    "    chunk_id: str = Field(..., description=\"The id of the chunk\")\n",
    "    document_id: str = Field(..., description=\"The id of the document\")\n",
    "\n",
    "class ChunkHasEntity(BaseModel):\n",
    "    chunk_id: str = Field(..., description=\"The id of the chunk\")\n",
    "    entity_id: str = Field(..., description=\"The id of the entity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node_and_relationship_from_chunk_element(element: CompositeElement, parent_document_id: str) -> dict[str, dict[str, Any]]:\n",
    "    \"\"\"Parse the entity node and document relationship for a given chunk element\"\"\"\n",
    "    chunk = Chunk(id=element.id, text=element.text)\n",
    "    chunk_part_of_document = ChunkPartOfDocument(chunk_id=chunk.id, document_id=parent_document_id)\n",
    "    return {\n",
    "        \"nodes\": [chunk.model_dump()],\n",
    "        \"relationships\": [chunk_part_of_document.model_dump()],\n",
    "    }\n",
    "\n",
    "def get_nodes_and_relationships_from_chunk_elements(elements: list[CompositeElement], parent_document: Document) -> dict[str, dict[str, pd.DataFrame]]:\n",
    "    \"\"\"Parse entity nodes and document relationships for a set of chunk elements and their parent document\"\"\"\n",
    "    \n",
    "    data = {\n",
    "        \"nodes\": {\n",
    "            \"document\": pd.DataFrame([parent_document.model_dump()]),\n",
    "            \"chunk\": list(),\n",
    "        },\n",
    "        \"relationships\": {\n",
    "            \"chunk_part_of_document\": list(),\n",
    "        }\n",
    "    }\n",
    "\n",
    "    for element in elements:\n",
    "        new_data = get_node_and_relationship_from_chunk_element(element, parent_document.id)\n",
    "        data[\"nodes\"][\"chunk\"].extend(new_data[\"nodes\"])\n",
    "        data[\"relationships\"][\"chunk_part_of_document\"].extend(new_data[\"relationships\"])\n",
    "\n",
    "    # convert to pandas dataframe for ingestion\n",
    "    data[\"nodes\"][\"chunk\"] = pd.DataFrame(data[\"nodes\"][\"chunk\"])\n",
    "    data[\"relationships\"][\"chunk_part_of_document\"] = pd.DataFrame(data[\"relationships\"][\"chunk_part_of_document\"])\n",
    "\n",
    "    return data\n",
    "\n",
    "def process_article(file_name: str) -> dict[str, dict[str, pd.DataFrame]]:\n",
    "    parent_document = Document(id=str(uuid4()), name=file_name, source=\"pubmed\")\n",
    "    elements = partition(file_name, chunking_strategy=\"by_title\")\n",
    "    return get_nodes_and_relationships_from_chunk_elements(elements, parent_document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexical Graph Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will embed the text fields of our lexical graph for vector similarity search. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schema Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to define our knowledge graph schema. This information will be passed to the entity extraction LLM to control which entities and relationships are pulled out of the text.\n",
    "\n",
    "This is necessary to prevent our schema from growing too large with an unbounded extraction process.\n",
    "\n",
    "We are using Pydantic to define the schema here since it can be used to validate any returned results as well. This ensures that all data we are ingesting into Neo4j adheres to this structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Medication(BaseModel):\n",
    "    \"\"\"a substance used for medical treatment, especially a medicine or drug. This is a base medication, not a medication implemented in a study.\"\"\"\n",
    "    \n",
    "    name: str = Field(..., description=\"Name of the medication. Should also be uniquely identifiable.\")\n",
    "    medication_class: str = Field(..., description=\"Drug class (e.g., GLP-1 RA, SGLT2i)\")\n",
    "    mechanism: Optional[str] = Field(None, description=\"Mechanism of action\")\n",
    "    generic_name: Optional[str] = Field(None, description=\"Generic name if different from name\")\n",
    "    brand_names: Optional[List[str]] = Field(None, description=\"Commercial brand names\")\n",
    "    approval_status: Optional[str] = Field(None, description=\"FDA approval status\")\n",
    "    \n",
    "    class Config:\n",
    "        json_schema_extra = {\n",
    "            \"examples\": [\n",
    "                {\n",
    "                    \"name\": \"Semaglutide\", \n",
    "                    \"medication_class\": \"GLP-1 receptor agonist\",\n",
    "                    \"mechanism\": \"GLP-1 receptor activation\",\n",
    "                    \"generic_name\": \"semaglutide\",\n",
    "                    \"brand_names\": [\"Ozempic\", \"Wegovy\", \"Rybelsus\"],\n",
    "                    \"approval_status\": \"FDA approved\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "\n",
    "\n",
    "class StudyMedication(BaseModel):\n",
    "    \"\"\"Study-specific medication usage - how a medication was used in a particular study\"\"\"\n",
    "    \n",
    "    study_medication_id: str = Field(..., description=\"Unique identifier for this study medication instance\")\n",
    "    dosage: Optional[str] = Field(None, description=\"Dosage used in this study\")\n",
    "    route: Optional[str] = Field(None, description=\"Route of administration\")\n",
    "    frequency: Optional[str] = Field(None, description=\"Dosing frequency\")\n",
    "    treatment_duration: Optional[str] = Field(None, description=\"Duration of treatment\")\n",
    "    treatment_arm: Optional[str] = Field(None, description=\"Treatment arm description\")\n",
    "    comparator: Optional[str] = Field(None, description=\"What this was compared against\")\n",
    "    adherence_rate: Optional[float] = Field(None, description=\"Treatment adherence rate\")\n",
    "    formulation: Optional[str] = Field(None, description=\"Specific formulation used\")\n",
    "    \n",
    "    class Config:\n",
    "        json_schema_extra = {\n",
    "            \"examples\": [\n",
    "                {\n",
    "                    \"study_medication_id\": \"STUDY_MED001\",\n",
    "                    \"dosage\": \"1.0 mg\",\n",
    "                    \"route\": \"subcutaneous\",\n",
    "                    \"frequency\": \"weekly\",\n",
    "                    \"treatment_duration\": \"12 weeks\",\n",
    "                    \"treatment_arm\": \"Active treatment group\",\n",
    "                    \"comparator\": \"placebo\",\n",
    "                    \"adherence_rate\": 85.5,\n",
    "                    \"formulation\": \"pre-filled pen\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "\n",
    "\n",
    "class ClinicalOutcome(BaseModel):\n",
    "    \"\"\"Measured clinical outcomes and biomarkers\"\"\"\n",
    "    \n",
    "    clinical_outcome_id: str = Field(..., description=\"Unique identifier for the outcome\")\n",
    "    name: str = Field(..., description=\"Name of the clinical outcome\")\n",
    "    category: str = Field(..., description=\"Category of outcome\")\n",
    "    measurement_unit: Optional[str] = Field(None, description=\"Unit of measurement\")\n",
    "    normal_range: Optional[str] = Field(None, description=\"Normal or target range when applicable\")\n",
    "    baseline_value: Optional[float] = Field(None, description=\"Baseline measurement value\")\n",
    "    post_treatment_value: Optional[float] = Field(None, description=\"Post-treatment measurement value\")\n",
    "    change_from_baseline: Optional[float] = Field(None, description=\"Change from baseline\")\n",
    "    p_value: Optional[float] = Field(None, description=\"Statistical significance if reported\")\n",
    "    confidence_interval: Optional[str] = Field(None, description=\"95% confidence interval\")\n",
    "    effect_size: Optional[float] = Field(None, description=\"Standardized effect size\")\n",
    "    \n",
    "    class Config:\n",
    "        json_schema_extra = {\n",
    "            \"examples\": [\n",
    "                {\n",
    "                    \"clinical_outcome_id\": \"OUT001\",\n",
    "                    \"name\": \"HbA1c\",\n",
    "                    \"category\": \"Glycemic control\",\n",
    "                    \"measurement_unit\": \"%\",\n",
    "                    \"normal_range\": \"<7.0%\",\n",
    "                    \"baseline_value\": 8.5,\n",
    "                    \"post_treatment_value\": 7.2,\n",
    "                    \"change_from_baseline\": -1.3,\n",
    "                    \"p_value\": 0.001,\n",
    "                    \"confidence_interval\": \"[-1.8, -0.8]\",\n",
    "                    \"effect_size\": -0.8\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "\n",
    "\n",
    "class MedicalCondition(BaseModel):\n",
    "    \"\"\"Medical conditions and comorbidities studied\"\"\"\n",
    "    \n",
    "    name: str = Field(..., description=\"Name of the medical condition\")\n",
    "    category: str = Field(..., description=\"Category of condition\")\n",
    "    severity: Optional[str] = Field(None, description=\"Severity or stage when specified\")\n",
    "    icd10_code: Optional[str] = Field(None, description=\"ICD-10 code when available\")\n",
    "    duration: Optional[str] = Field(None, description=\"Duration of condition if specified\")\n",
    "    prevalence: Optional[float] = Field(None, description=\"Prevalence in study population\")\n",
    "    \n",
    "    class Config:\n",
    "        json_schema_extra = {\n",
    "            \"examples\": [\n",
    "                {\n",
    "                    \"name\": \"Type 2 diabetes mellitus\",\n",
    "                    \"category\": \"Primary condition\", \n",
    "                    \"severity\": \"moderate\",\n",
    "                    \"icd10_code\": \"E11\",\n",
    "                    \"duration\": \"5-10 years\",\n",
    "                    \"prevalence\": 100.0\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "\n",
    "\n",
    "class StudyPopulation(BaseModel):\n",
    "    \"\"\"Patient populations and demographics in research studies\"\"\"\n",
    "    \n",
    "    study_population_id: str = Field(..., description=\"Unique identifier for the population\")\n",
    "    description: str = Field(..., description=\"Description of the population\")\n",
    "    age_range: Optional[str] = Field(None, description=\"Age range\")\n",
    "    mean_age: Optional[float] = Field(None, description=\"Mean age in years\")\n",
    "    male_percentage: Optional[float] = Field(None, description=\"Percentage of male gender participants\")\n",
    "    female_percentage: Optional[float] = Field(None, description=\"Percentage of female gender participants\")\n",
    "    other_gender_percentage: Optional[float] = Field(None, description=\"Percentage of participants that identify as another gender\")\n",
    "    sample_size: Optional[int] = Field(None, description=\"Number of participants\")\n",
    "    study_type: str = Field(..., description=\"Type of study\")\n",
    "    location: Optional[str] = Field(None, description=\"Geographic location of study\")\n",
    "    inclusion_criteria: Optional[List[str]] = Field(None, description=\"Key inclusion criteria\")\n",
    "    exclusion_criteria: Optional[List[str]] = Field(None, description=\"Key exclusion criteria\")\n",
    "    study_duration: Optional[str] = Field(None, description=\"Duration of study\")\n",
    "    \n",
    "    class Config:\n",
    "        json_schema_extra = {\n",
    "            \"examples\": [\n",
    "                {\n",
    "                    \"study_population_id\": \"POP001\",\n",
    "                    \"description\": \"Adults with T2DM and schizophrenia\",\n",
    "                    \"age_range\": \"18-65 years\",\n",
    "                    \"mean_age\": 43.8,\n",
    "                    \"female_percentage\": 47.0,\n",
    "                    \"male_percentage\": 53.0,\n",
    "                    \"sample_size\": 354,\n",
    "                    \"study_type\": \"Observational study\",\n",
    "                    \"location\": \"Denmark\",\n",
    "                    \"inclusion_criteria\": [\"Type 2 diabetes diagnosis\", \"Schizophrenia diagnosis\", \"Age ≥18\"],\n",
    "                    \"study_duration\": \"12 months\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "\n",
    "\n",
    "# Relationship classes\n",
    "class StudyMedicationUsesMedication(BaseModel):\n",
    "    \"\"\"Links study medication to base medication\"\"\"\n",
    "    study_medication_id: str\n",
    "    medication_name: str\n",
    "\n",
    "\n",
    "class StudyMedicationProducesClinicalOutcome(BaseModel):\n",
    "    \"\"\"Links study medication usage to clinical outcomes\"\"\"\n",
    "    study_medication_id: str\n",
    "    clinical_outcome_name: str\n",
    "\n",
    "\n",
    "class StudyPopulationHasMedicalCondition(BaseModel):\n",
    "    \"\"\"Relationship between study population and medical conditions\"\"\"\n",
    "    study_population_id: str\n",
    "    medical_condition_name: str\n",
    "\n",
    "\n",
    "class StudyPopulationReceivesStudyMedication(BaseModel):\n",
    "    \"\"\"Relationship between study population and study medication\"\"\"\n",
    "    study_population_id: str\n",
    "    study_medication_id: str\n",
    "\n",
    "\n",
    "class StudyPopulationHasOutcome(BaseModel):\n",
    "    \"\"\"Direct relationship between population and outcomes (for population-level measurements)\"\"\"\n",
    "    study_population_id: str\n",
    "    clinical_outcome_name: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our knowledge graph data model looks like this \n",
    "\n",
    "IMAGE OF DATA MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity Extraction via LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using [OpenAI](https://platform.openai.com/docs/overview) and the [Instructor](https://python.useinstructor.com/) library to perform our entity extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AsyncOpenAI\n",
    "import instructor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructor handles requesting structured outputs from the LLM. \n",
    "\n",
    "If the LLM fails to return output that adheres to the response models, Instructor will also handle the retry logic and pass any errors to inform corrections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = instructor.from_openai(AsyncOpenAI())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are a healthcare research expert that is responsible for extracting detailed entities from PubMed articles. \n",
    "You will be provided a graph data model schema and must extract entities and relationships to populate a knowledge graph.\n",
    "\"\"\"\n",
    "\n",
    "async def extract_entities_from_text_chunk(text_chunk: str) -> list:\n",
    "    response = await client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": text_chunk}\n",
    "        ],\n",
    "        response_model=list[Medication | StudyMedication | StudyMedicationUsesMedication],\n",
    "    )\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def extract_entities_from_chunk_nodes(chunk_nodes: pd.DataFrame) -> list[tuple[str, list[Any]]]:\n",
    "    \"\"\"\n",
    "    Process a Pandas DataFrame of Chunk nodes and return the entities found in each chunk.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    list[tuple[str, list[dict[str, Any]]]]\n",
    "        A list of tuples, where the first element is the chunk id and the second element is a list of entities found in the chunk.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create tasks for all nodes\n",
    "    # order is maintained\n",
    "    tasks = [extract_entities_from_text_chunk(row[\"text\"]) for _, row in chunk_nodes.iterrows()]\n",
    "\n",
    "    # Execute all tasks concurrently\n",
    "    extraction_results = await asyncio.gather(*tasks)\n",
    "\n",
    "    # Return chunk_id paired with its entities\n",
    "    return [(chunk_id, entities) for chunk_id, entities in zip(chunk_nodes[\"id\"], extraction_results)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "al Science, Faculty of Medicine, Universitas Airlangga, \n",
      "Surabaya, Indonesia. fahrul.nurkolis.mail@gmail.com.\n",
      "(11)Medical Research Center of Indonesia, Surabaya, East Java, Indonesia. \n",
      "fahrul.nurkolis.mail@gmail.com.\n",
      "\n",
      "BACKGROUND: The global rise in obesity and type 2 diabetes highlights the need \n",
      "for safe and effective therapeutic interventions. Enhalus acoroides is a \n",
      "tropical seagrass rich in carotenoids and other bioactives. Its potential for \n",
      "metabolic regulation has been suggested in vitro, but in vivo efficacy and \n",
      "molecular mechanisms remain unexplored. This study aimed to evaluate the \n",
      "anti-obesity and anti-diabetic effects of Enhalus acoroides extract (SEAE) in a \n",
      "zebrafish model of diet- and glucose-induced metabolic dysfunction.\n",
      "METHODS: Adult zebrafish were subjected to overfeeding and glucose immersion, \n",
      "after overfeeding and 14 days of glucose immersion to induce diabetes, adult \n",
      "zebrafish were randomized into three groups: untreated diabetic, SEAE-treated \n",
      "(5 mg/L), and \n"
     ]
    }
   ],
   "source": [
    "with open(\"pubmed_abstracts.txt\", \"r\") as f:\n",
    "    text = f.read()[1500:2500]\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ents = await extract_entities_from_text_chunk(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Medication(name='Enhalus acoroides extract', medication_class='Natural Product', mechanism='Metabolic regulation', generic_name='SEAE', brand_names=None, approval_status=None)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now defined \n",
    "* Lexical and domain data models\n",
    "* Partitioning and chunking logic for articles\n",
    "* Entity extraction logic for chunks\n",
    "\n",
    "It is now time to define our ingestion logic. We will run ingest in three stages \n",
    "\n",
    "1. Load lexical graph\n",
    "2. Embed lexical graph Chunk nodes\n",
    "3. Extract domain / entity graph from lexical graph\n",
    "\n",
    "Decoupling these stages allows us easily make changes as we iterate our ingestion process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using PyNeoInstance to ingest our data. \n",
    "\n",
    "This library allows for easy ingest orchestration and features parallel ingest options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from pyneoinstance import Neo4jInstance, load_yaml_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our database credentials and all of our queries are stored in the `pyneoinstance_config.yaml` file. \n",
    "\n",
    "This makes it easy to manage our queries and keeps the notebook code clean. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_yaml_file(\"pyneoinstance_config.yaml\")\n",
    "\n",
    "db_info = config['db_info']\n",
    "\n",
    "constraints = config['initializing_queries']['constraints']\n",
    "indexes = config['initializing_queries']['indexes']\n",
    "\n",
    "node_load_queries = config['loading_queries']['nodes']\n",
    "relationship_load_queries = config['loading_queries']['relationships']\n",
    "\n",
    "processing_queries = config['processing_queries']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This graph object will handle database connections and read / write transactions for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = Neo4jInstance(db_info.get('uri', os.getenv(\"NEO4J_URI\", \"neo4j://localhost:7687\")), # use config value -> use env value -> use default value\n",
    "                      db_info.get('user', os.getenv(\"NEO4J_USER\", \"neo4j\")), \n",
    "                      db_info.get('password', os.getenv(\"NEO4J_PASSWORD\", \"password\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a helper function for ingesting data using the PyNeoInstance library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_partition(data: list[dict[str, Any]], batch_size: int = 500) -> int:\n",
    "    \"\"\"\n",
    "    Determine the data partition based on the desired batch size.\n",
    "    \"\"\"\n",
    "    \n",
    "    partition = int(len(data) / batch_size)\n",
    "    print(\"partition: \"+str(partition if partition > 1 else 1))\n",
    "    return partition if partition > 1 else 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we write all the constraints and indexes we need for both the lexical and domain graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_constraints_and_indexes() -> None:\n",
    "    \"\"\"\n",
    "    Create constraints and indexes for the graph.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        graph.execute_write_queries(database=db_info['database'], queries=list(constraints.values()))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    try:\n",
    "        graph.execute_write_queries(database=db_info['database'], queries=list(indexes.values()))\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'values'\n"
     ]
    }
   ],
   "source": [
    "create_constraints_and_indexes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingest Lexical Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing | Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process the articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n"
     ]
    }
   ],
   "source": [
    "lexical_ingest_records = process_article(\"pubmed_abstracts.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the first few records "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c7575a02b7776f6183bcb0c3aa2b3a58</td>\n",
       "      <td>1. Diabetol Metab Syndr. 2025 Jun 21;17(1):235...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aaf28f4d0b350bd4c312418709797fd5</td>\n",
       "      <td>Author information: (1)Faculty of Medicine, Un...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>def7de5c464d6254bdaa2d73b86034eb</td>\n",
       "      <td>Indonesia, Jakarta, 12930, Indonesia. (6)Divis...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 id  \\\n",
       "0  c7575a02b7776f6183bcb0c3aa2b3a58   \n",
       "1  aaf28f4d0b350bd4c312418709797fd5   \n",
       "2  def7de5c464d6254bdaa2d73b86034eb   \n",
       "\n",
       "                                                text  \n",
       "0  1. Diabetol Metab Syndr. 2025 Jun 21;17(1):235...  \n",
       "1  Author information: (1)Faculty of Medicine, Un...  \n",
       "2  Indonesia, Jakarta, 12930, Indonesia. (6)Divis...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexical_ingest_records[\"nodes\"][\"chunk\"][:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_lexical_graph(driver, lexical_ingest_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Document and Chunk nodes into the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_lexical_nodes(document_records: pd.DataFrame, chunk_records: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Load lexical nodes into the graph. These include Document and Chunk nodes.\n",
    "    \"\"\"\n",
    "    lexical_nodes_ingest_iterator = list(zip([document_records, chunk_records], ['document', 'chunk']))\n",
    "\n",
    "    for data, query in lexical_nodes_ingest_iterator:\n",
    "        res = graph.execute_write_query_with_data(database=db_info['database'], \n",
    "                                                    data=data, \n",
    "                                                    query=node_load_queries[query], \n",
    "                                                    partitions=get_partition(data, batch_size=500),\n",
    "                                                    parallel=True,\n",
    "                                                    workers=2)\n",
    "        print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "partition: 1\n",
      "{}\n",
      "partition: 1\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "load_lexical_nodes(lexical_ingest_records[\"nodes\"][\"document\"], lexical_ingest_records[\"nodes\"][\"chunk\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the relationships into the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_lexical_relationships(chunk_part_of_document_records: list[ChunkPartOfDocument]) -> None:\n",
    "    \"\"\"\n",
    "    Load lexical relationships into the graph. This includes the Chunk - PART_OF -> Document relationship.\n",
    "    \"\"\"\n",
    "    lexical_relationships_ingest_iterator = list(zip([chunk_part_of_document_records], ['chunk_part_of_document']))\n",
    "\n",
    "    for data, query in lexical_relationships_ingest_iterator:\n",
    "        res = graph.execute_write_query_with_data(database=db_info['database'], \n",
    "                                                    data=data, \n",
    "                                                    query=relationship_load_queries[query], \n",
    "                                                    partitions=get_partition(data, batch_size=500))\n",
    "        print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "partition: 1\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "load_lexical_relationships(lexical_ingest_records[\"relationships\"][\"chunk_part_of_document\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embed Lexical Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will read Chunk nodes from the graph that don't have embedding properties yet. \n",
    "\n",
    "We will then embed the Chunk text property and add the embedding as a property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_index = ...\n",
    "\n",
    "def create_vector_index() -> None:\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(driver) -> None:\n",
    "    ...\n",
    "\n",
    "def embed_lexical_graph(driver) -> None:\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Entities from Lexical Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now perform entity extraction on the Chunk nodes to augment and connect to our domain graph containing patient journey information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunk_nodes_to_process_by_article_name(article_name: str) -> list[Chunk]:\n",
    "    \"\"\"\n",
    "    Retrieve Chunk node id and text from the database that have a relationship to the Document with the article name provided.\n",
    "    These chunks may then be used as input to the entity extraction process.\n",
    "    \"\"\"\n",
    "\n",
    "    return graph.execute_read_query(database=db_info['database'], \n",
    "                            parameters={\"article_name\": article_name}, \n",
    "                            query=processing_queries['get_chunk_nodes_to_process_by_article_name'], \n",
    "                        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_to_process = get_chunk_nodes_to_process_by_article_name(\"pubmed_abstracts.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 171 chunks to process\n",
      "First chunk:\n",
      "\n",
      "1. Diabetol Metab Syndr. 2025 Jun 21;17(1):235. doi: 10.1186/s13098-025-01823-4.\n",
      "\n",
      "Seagrass Enhalus acoroides extract mitigates obesity and diabetes via GLP-1, PPARγ, SREBP-1c modulation and gut microbiome restoration in diabetic zebrafish.\n",
      "\n",
      "Kadharusman MM(1), Syahputra RA(2), Kurniawan R(3), Hadinata E(4), Tjandrawinata RR(5), Taslim NA(6), Romano R(7), Santini A(8), Nurkolis F(9)(10)(11).\n"
     ]
    }
   ],
   "source": [
    "print(f\"Found {len(chunks_to_process)} chunks to process\")\n",
    "print(f\"First chunk:\\n\\n{chunks_to_process.loc[0,'text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_ingest_records = await extract_entities_from_chunk_nodes(chunks_to_process[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('c7575a02b7776f6183bcb0c3aa2b3a58',\n",
       "  [Medication(name='Seagrass Enhalus acoroides extract', medication_class='Herbal Supplement', mechanism='GLP-1, PPARγ, SREBP-1c modulation and gut microbiome restoration', generic_name=None, brand_names=None, approval_status=None)]),\n",
       " ('aaf28f4d0b350bd4c312418709797fd5', []),\n",
       " ('def7de5c464d6254bdaa2d73b86034eb', []),\n",
       " ('e0313766b618ceaaf67ad271da664657', []),\n",
       " ('13e0b5fa31fac4fc45f154b8a820a39c', []),\n",
       " ('a367e70b651483e37ff05b067c18204f',\n",
       "  [StudyMedication(study_medication_id='STUDY_MED002', dosage='5 mg/L', route='Immersion', frequency=None, treatment_duration='20 days', treatment_arm='SEAE-treated group', comparator=None, adherence_rate=None, formulation=None),\n",
       "   StudyMedication(study_medication_id='STUDY_MED003', dosage='3.3 mg/L', route='Immersion', frequency=None, treatment_duration='20 days', treatment_arm='Metformin-treated group', comparator=None, adherence_rate=None, formulation=None),\n",
       "   StudyMedicationUsesMedication(study_medication_id='STUDY_MED003', medication_name='Metformin')]),\n",
       " ('3965d94b1bde07e1076d839475c2d987',\n",
       "  [Medication(name='Metformin', medication_class='Biguanide', mechanism='Decreases hepatic glucose production, decreases intestinal absorption of glucose, and improves insulin sensitivity by increasing peripheral glucose uptake and utilization', generic_name=None, brand_names=['Glucophage', 'Fortamet', 'Glumetza'], approval_status='FDA approved')]),\n",
       " ('0b59be3d8bb5ff8bb44669c8708e2031', [])]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_ingest_records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingest Entities Into Knowledge Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These functions load the extracted entities and relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These functions link the extracted entities with their text chunk nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENTITY_LABELS = {\n",
    "    \"Medication\", \n",
    "    \"StudyMedication\",\n",
    "    \"MedicalCondition\",\n",
    "    \"StudyPopulation\",\n",
    "    \"ClinicalOutcome\",\n",
    "}\n",
    "\n",
    "ENTITY_RELS = {\n",
    "    \"StudyMedicationUsesMedication\",\n",
    "    \"StudyMedicationProducesClinicalOutcome\",\n",
    "    \"StudyPopulationHasMedicalCondition\",\n",
    "    \"StudyPopulationReceivesStudyMedication\",\n",
    "    \"StudyPopulationHasOutcome\",\n",
    "}\n",
    "\n",
    "def prepare_entities_for_ingestion(entities: list[tuple[str, list[Any]]]) -> dict[str, dict[str, pd.DataFrame]]:\n",
    "    \"\"\"\n",
    "    Prepare entities for ingestion into the graph.\n",
    "    This function takes the results of the `get_chunk_nodes_to_process_by_article_name` function and returns a dictionary of entity label to list of entities.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict[str, dict[str, pd.DataFrame]]\n",
    "        A dictionary of entity label to pandas dataframe of entities.\n",
    "\n",
    "        {\n",
    "            \"nodes\": {\n",
    "                \"Medication\": pd.DataFrame(...),\n",
    "                \"StudyMedication\": pd.DataFrame(...),\n",
    "                ...\n",
    "            },\n",
    "            \"relationships\": {\n",
    "                \"StudyMedicationUsesMedication\": pd.DataFrame(...),\n",
    "                \"StudyMedicationProducesClinicalOutcome\": pd.DataFrame(...),\n",
    "                ...\n",
    "            }\n",
    "        }\n",
    "    \"\"\"\n",
    "\n",
    "    records_node_dict = {lbl: list() for lbl in ENTITY_LABELS}\n",
    "    records_rel_dict = {lbl: list() for lbl in ENTITY_RELS}\n",
    "\n",
    "    for chunk_id, entities in entities:\n",
    "        for entity in entities:\n",
    "            to_add = entity.model_dump()\n",
    "            to_add.update({\"chunk_id\": chunk_id})\n",
    "            # nodes\n",
    "            if entity.__class__.__name__ in ENTITY_LABELS:\n",
    "                records_node_dict[entity.__class__.__name__].append(to_add)\n",
    "            # rels\n",
    "            elif entity.__class__.__name__ in ENTITY_RELS:\n",
    "                records_rel_dict[entity.__class__.__name__].append(to_add)\n",
    "            else:\n",
    "                print(f\"Unknown entity type: {entity.__class__.__name__}\")\n",
    "\n",
    "    for key, value in records_node_dict.items():\n",
    "        records_node_dict[key] = pd.DataFrame(value)\n",
    "\n",
    "    for key, value in records_rel_dict.items():\n",
    "        records_rel_dict[key] = pd.DataFrame(value)\n",
    "\n",
    "    return {\"nodes\": records_node_dict, \"relationships\": records_rel_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_entity_nodes(medication_records: pd.DataFrame, \n",
    "                      medical_condition_records: pd.DataFrame, \n",
    "                      study_medication_records: pd.DataFrame, \n",
    "                      study_population_records: pd.DataFrame, \n",
    "                      clinical_outcome_records: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Load entity nodes into the graph.\n",
    "    \"\"\"\n",
    "    \n",
    "    entity_nodes_ingest_iterator = list(zip([medication_records, \n",
    "                                             medical_condition_records, \n",
    "                                             study_medication_records, \n",
    "                                             study_population_records, \n",
    "                                             clinical_outcome_records], \n",
    "                                             ['medication', \n",
    "                                              'medical_condition', \n",
    "                                              'study_medication', \n",
    "                                              'study_population', \n",
    "                                              'clinical_outcome']))\n",
    "\n",
    "    for data, query in entity_nodes_ingest_iterator:\n",
    "        print(f\"Loading {len(data)} {query} nodes\")\n",
    "        if len(data) > 0:\n",
    "            res = graph.execute_write_query_with_data(database=db_info['database'], \n",
    "                                                    data=data, \n",
    "                                                    query=node_load_queries[query], \n",
    "                                                    partitions=get_partition(data, batch_size=500),\n",
    "                                                    parallel=False)\n",
    "            print(res)\n",
    "        else:\n",
    "            print(f\"No {query} nodes to load\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_entity_relationships(study_medication_uses_medication: pd.DataFrame,\n",
    "                              study_medication_produces_clinical_outcome: pd.DataFrame,\n",
    "                              study_population_has_medical_condition: pd.DataFrame,\n",
    "                              study_population_receives_study_medication: pd.DataFrame,\n",
    "                              study_population_has_outcome: pd.DataFrame,\n",
    "                              ) -> None:\n",
    "    \"\"\"\n",
    "    Load entity relationships into the graph.\n",
    "    \"\"\"\n",
    "    entity_relationships_ingest_iterator = list(zip([study_medication_uses_medication, \n",
    "                                                      study_medication_produces_clinical_outcome, \n",
    "                                                      study_population_has_medical_condition, \n",
    "                                                      study_population_receives_study_medication, \n",
    "                                                      study_population_has_outcome], \n",
    "                                                      ['study_medication_uses_medication', \n",
    "                                                       'study_medication_produces_clinical_outcome', \n",
    "                                                       'study_population_has_medical_condition', \n",
    "                                                       'study_population_receives_study_medication', \n",
    "                                                       'study_population_has_outcome']))\n",
    "    \n",
    "    for data, query in entity_relationships_ingest_iterator:\n",
    "        print(f\"Loading {len(data)} {query} relationships\")\n",
    "        if len(data) > 0:\n",
    "            res = graph.execute_write_query_with_data(database=db_info['database'], \n",
    "                                                    data=data, \n",
    "                                                    query=relationship_load_queries[query], \n",
    "                                                    partitions=get_partition(data, batch_size=500),\n",
    "                                                    parallel=False)\n",
    "            print(res)\n",
    "        else:\n",
    "            print(f\"No {query} relationships to load\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def link_entities_to_chunks(medication_link_records: pd.DataFrame, \n",
    "                      medical_condition_link_records: pd.DataFrame, \n",
    "                      study_medication_link_records: pd.DataFrame, \n",
    "                      study_population_link_records: pd.DataFrame, \n",
    "                      clinical_outcome_link_records: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Link entities to chunks.\n",
    "    \"\"\"\n",
    "    entity_link_iterator = list(zip([medication_link_records, \n",
    "                                     medical_condition_link_records, \n",
    "                                     study_medication_link_records, \n",
    "                                     study_population_link_records, \n",
    "                                     clinical_outcome_link_records], \n",
    "                                     [\"chunk_has_entity_medication\",\n",
    "                                      \"chunk_has_entity_medical_condition\",\n",
    "                                      \"chunk_has_entity_study_medication\",\n",
    "                                      \"chunk_has_entity_study_population\",\n",
    "                                      \"chunk_has_entity_clinical_outcome\"]))\n",
    "    \n",
    "    for data, query in entity_link_iterator:\n",
    "        print(f\"Linking {len(data)} {query} entities to chunks\")\n",
    "        if len(data) > 0:\n",
    "            res = graph.execute_write_query_with_data(database=db_info['database'], \n",
    "                                                    data=data, \n",
    "                                                    query=relationship_load_queries[query], \n",
    "                                                    partitions=get_partition(data, batch_size=500),\n",
    "                                                    parallel=False)\n",
    "            print(res)\n",
    "        else:\n",
    "            print(f\"No {query} relationships to load\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingest_records = prepare_entities_for_ingestion(entity_ingest_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(2)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ingest_records[\"nodes\"][\"Medication\"]['generic_name'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 2 medication nodes\n",
      "partition: 1\n",
      "{'properties_set': 10}\n",
      "Loading 0 medical_condition nodes\n",
      "No medical_condition nodes to load\n",
      "Loading 2 study_medication nodes\n",
      "partition: 1\n",
      "{'labels_added': 2, 'nodes_created': 2, 'properties_set': 18}\n",
      "Loading 0 study_population nodes\n",
      "No study_population nodes to load\n",
      "Loading 0 clinical_outcome nodes\n",
      "No clinical_outcome nodes to load\n"
     ]
    }
   ],
   "source": [
    "load_entity_nodes(ingest_records[\"nodes\"][\"Medication\"], \n",
    "                  ingest_records[\"nodes\"][\"MedicalCondition\"], \n",
    "                  ingest_records[\"nodes\"][\"StudyMedication\"], \n",
    "                  ingest_records[\"nodes\"][\"StudyPopulation\"], \n",
    "                  ingest_records[\"nodes\"][\"ClinicalOutcome\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 1 study_medication_uses_medication relationships\n",
      "partition: 1\n",
      "{'relationships_created': 1}\n",
      "Loading 0 study_medication_produces_clinical_outcome relationships\n",
      "No study_medication_produces_clinical_outcome relationships to load\n",
      "Loading 0 study_population_has_medical_condition relationships\n",
      "No study_population_has_medical_condition relationships to load\n",
      "Loading 0 study_population_receives_study_medication relationships\n",
      "No study_population_receives_study_medication relationships to load\n",
      "Loading 0 study_population_has_outcome relationships\n",
      "No study_population_has_outcome relationships to load\n"
     ]
    }
   ],
   "source": [
    "load_entity_relationships(ingest_records[\"relationships\"][\"StudyMedicationUsesMedication\"], \n",
    "                          ingest_records[\"relationships\"][\"StudyMedicationProducesClinicalOutcome\"], \n",
    "                          ingest_records[\"relationships\"][\"StudyPopulationHasMedicalCondition\"], \n",
    "                          ingest_records[\"relationships\"][\"StudyPopulationReceivesStudyMedication\"], \n",
    "                          ingest_records[\"relationships\"][\"StudyPopulationHasOutcome\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linking 2 chunk_has_entity_medication entities to chunks\n",
      "partition: 1\n",
      "{}\n",
      "Linking 0 chunk_has_entity_medical_condition entities to chunks\n",
      "No chunk_has_entity_medical_condition relationships to load\n",
      "Linking 2 chunk_has_entity_study_medication entities to chunks\n",
      "partition: 1\n",
      "{'relationships_created': 2}\n",
      "Linking 0 chunk_has_entity_study_population entities to chunks\n",
      "No chunk_has_entity_study_population relationships to load\n",
      "Linking 0 chunk_has_entity_clinical_outcome entities to chunks\n",
      "No chunk_has_entity_clinical_outcome relationships to load\n"
     ]
    }
   ],
   "source": [
    "link_entities_to_chunks(ingest_records[\"nodes\"][\"Medication\"], \n",
    "                        ingest_records[\"nodes\"][\"MedicalCondition\"], \n",
    "                        ingest_records[\"nodes\"][\"StudyMedication\"], \n",
    "                        ingest_records[\"nodes\"][\"StudyPopulation\"], \n",
    "                        ingest_records[\"nodes\"][\"ClinicalOutcome\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
