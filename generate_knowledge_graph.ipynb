{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PubMed Knowledge Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook walks through the process of generating a knowledge graph of PubMed articles.\n",
    "\n",
    "This notebook will\n",
    "* Download a selection of articles from PubMed\n",
    "* Define a knowledge graph schema\n",
    "* Extract entities from the articles according to the defined schema\n",
    "* Populate a Neo4j instance with articles and extracted entities\n",
    "* Connect extracted entities with existing patient journey data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook requires a local repo of articles. You may download a sample of 20 PubMed articles by running the following command.\n",
    "\n",
    "```bash\n",
    "python3 ./scripts/fetch_pubmed_articles.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter some Numpy warnings that pop up during ingestion\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import hashlib\n",
    "from typing import Any, Optional, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allows for async operations in notebooks\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexical Graph Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use Unstructured.IO to partition and chunk our articles. \n",
    "\n",
    "This process breaks the articles into sensible chunks that may be used as context in our application. \n",
    "\n",
    "These chunks will also have relationships to the extracted entities, but we will add these relationships later.\n",
    "\n",
    "The lexical graph will adhere to the structure defined in the ['Lexical Graph with Extracted Entities'](https://graphrag.com/reference/knowledge-graph/lexical-graph-extracted-entities/) section of [graphrag.com](graphrag.com).\n",
    "\n",
    "Here is the data model we will be using.\n",
    "\n",
    "\n",
    "<img src=\"./assets/images/lexical-data-model.png\" alt=\"lexical-data-model\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pydantic import BaseModel, Field, computed_field, field_validator\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "\n",
    "from unstructured.documents.elements import CompositeElement\n",
    "\n",
    "from uuid import uuid4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------\n",
    "# Nodes\n",
    "# ------------\n",
    "\n",
    "class Document(BaseModel):\n",
    "    \"\"\"\n",
    "    A Document.\n",
    "    This is the top level node in our knowledge graph.\n",
    "    Documents are made of many Chunks.\n",
    "    \"\"\"\n",
    "    id: str = Field(..., description=\"The id of the document\")\n",
    "    name: str = Field(..., description=\"The name of the document\")\n",
    "    source: str = Field(..., description=\"The source of the document\")\n",
    "\n",
    "class Chunk(BaseModel):\n",
    "    \"\"\"\n",
    "    A Chunk.\n",
    "    This is a collection of `UnstructuredElements`.\n",
    "    Unstructured.IO represents Chunks as `CompositeElement` objects.\n",
    "    \"\"\"\n",
    "    id: str = Field(..., description=\"The id of the chunk\")\n",
    "    type: str = Field(..., description=\"The type of the chunk\")\n",
    "    text: str = Field(..., description=\"The text of the chunk\")\n",
    "\n",
    "class ChunkWithEmbedding(Chunk):\n",
    "    \"\"\"\n",
    "    A Chunk with an embedding.\n",
    "    This is used to represent chunks that have been embedded.\n",
    "    \"\"\"\n",
    "    embedding: list[float] = Field(..., description=\"The embedding of the chunk text field\")\n",
    "\n",
    "class UnstructuredElement(BaseModel):\n",
    "    \"\"\"\n",
    "    A base class for all unstructured elements. \n",
    "    These are the smallest units in our chunking process. \n",
    "    One or more of these elements are combined to form a Chunk.\n",
    "    \"\"\"\n",
    "    id: str = Field(..., description=\"The id of the element\")\n",
    "    text: str = Field(..., description=\"The text of the element\")\n",
    "    type: str = Field(..., description=\"The type of the element\")\n",
    "    page_number: int = Field(..., description=\"The page number of the element\")\n",
    "\n",
    "class TextElement(UnstructuredElement):\n",
    "    \"\"\"\n",
    "    A TextElement. Structurally identical to the UnstructuredElement class.\n",
    "    This is used to represent text elements that contain no tables or images.\n",
    "    \"\"\"\n",
    "\n",
    "class ImageElement(UnstructuredElement):\n",
    "    \"\"\"\n",
    "    An ImageElement.\n",
    "    \"\"\"\n",
    "    image_base64: str = Field(..., description=\"The base64 encoded image\")\n",
    "    image_mime_type: str = Field(..., description=\"The mime type of the image\")\n",
    "\n",
    "class TableElement(UnstructuredElement):\n",
    "    \"\"\"\n",
    "    A TableElement. \n",
    "    This may also have image features and so it inherits from ImageElement.\n",
    "    \"\"\"\n",
    "    image_base64: str | None = Field(None, description=\"The base64 encoded table\")\n",
    "    image_mime_type: str | None = Field(None, description=\"The mime type of the table\")\n",
    "    text_as_html: str | None = Field(None, description=\"The text of the table as HTML\")\n",
    "    \n",
    "# -------------\n",
    "# Relationships\n",
    "# -------------\n",
    "\n",
    "class ChunkPartOfDocument(BaseModel):\n",
    "    \"\"\"\n",
    "    (:Chunk {id: $chunk_id})-[:PART_OF_DOCUMENT]->(:Document {id: $document_id})\n",
    "    \"\"\"\n",
    "    chunk_id: str = Field(..., description=\"The id of the chunk\")\n",
    "    document_id: str = Field(..., description=\"The id of the document\")\n",
    "\n",
    "class UnstructuredElementPartOfChunk(BaseModel):\n",
    "    \"\"\"\n",
    "    (:UnstructuredElement {id: $unstructured_element_id})-[:PART_OF_CHUNK]->(:Chunk {id: $chunk_id})\n",
    "\n",
    "    This covers TextElement, ImageElement and TableElement nodes since they all share the UnstructuredElement label.\n",
    "    \"\"\" \n",
    "    unstructured_element_id: str = Field(..., description=\"The id of the unstructured element\")\n",
    "    chunk_id: str = Field(..., description=\"The id of the chunk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_article_file_name(file_name: str) -> tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Parse the article file name and return the PubMed ID and title.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_name : str\n",
    "        The name of the file to parse.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple[str, str]\n",
    "        The PubMed ID and title of the article.\n",
    "    \"\"\"\n",
    "    doc_id, title = file_name.split(\"-\", 1)\n",
    "    title = title.replace(\"_\", \" \")\n",
    "\n",
    "    return doc_id, title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chunk_has_next_chunk_relationship_dataframe(chunk_dataframe: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create the DataFrame for loading (:Chunk)-[:HAS_NEXT_CHUNK]->(:Chunk) relationships.\n",
    "    \"\"\"\n",
    "    df = chunk_dataframe.copy()\n",
    "    df['next_id'] = df['id'].shift(-1)\n",
    "    df.dropna(inplace=True)\n",
    "    res = df[['id', 'next_id']].rename({\"id\": \"source_id\", \"next_id\": \"target_id\"}, axis=1)\n",
    "    return res\n",
    "\n",
    "def extract_document_title(text_elements_dataframe: pd.DataFrame) -> str:\n",
    "    \"\"\"\n",
    "    Extract the title of the document from the text elements.\n",
    "    Here we assume that the first 'Title' element is the title of the document.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        The title of the document.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return text_elements_dataframe[text_elements_dataframe['type'] == 'Title'].iloc[0]['text']\n",
    "    except Exception as e:\n",
    "        print(f\"Unable to extract document title: {e}\")\n",
    "        return 'unknown title'\n",
    "\n",
    "def parse_node_and_relationship_from_composite_element(composite_element: CompositeElement, parent_document_id: str) -> dict[str, dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Parse the nodes and relationships for a given chunk (CompositeElement). \n",
    "    This will find the following nodes:\n",
    "    * Chunk\n",
    "    * TextElement\n",
    "    * ImageElement\n",
    "    * TableElement\n",
    "    * UnstructuredElement (Shared label for TextElement, ImageElement and TableElement)\n",
    "\n",
    "    And the following relationships:\n",
    "    * (:Chunk)-[:PART_OF_DOCUMENT]->(:Document)\n",
    "    * (:UnstructuredElement)-[:PART_OF_CHUNK]->(:Chunk)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict[str, dict[str, Any]]\n",
    "        A dictionary containing a list of records for each node and relationship type.\n",
    "    \"\"\"\n",
    "    chunk = Chunk(id=composite_element.id, text=composite_element.text, type=composite_element.category)\n",
    "    chunk_part_of_document = ChunkPartOfDocument(chunk_id=chunk.id, document_id=parent_document_id)\n",
    "\n",
    "    text_elements: list[TextElement] = list()\n",
    "    image_elements: list[ImageElement] = list()\n",
    "    table_elements: list[TableElement] = list()\n",
    "    unstructured_element_part_of_chunk: list[UnstructuredElementPartOfChunk] = list()\n",
    "\n",
    "    # Chunks (CompositeElements) are made of many smaller text chunks (UnstructuredElements)\n",
    "    # We can parse what type of elements these subchunks are and load them as well\n",
    "    # This will give us access to images and tables from the document\n",
    "    for element in composite_element.metadata.orig_elements:\n",
    "        match element.category:\n",
    "            case \"NarrativeText\":\n",
    "                text_elements.append(TextElement(id=element.id, \n",
    "                                                 text=element.text, \n",
    "                                                 type=element.category, \n",
    "                                                 page_number=element.metadata.page_number))\n",
    "            case \"Image\":\n",
    "                image_elements.append(ImageElement(id=element.id, \n",
    "                                                   text=element.text,\n",
    "                                                   type=element.category, \n",
    "                                                   page_number=element.metadata.page_number,\n",
    "                                                   image_base64=element.metadata.image_base64, \n",
    "                                                   image_mime_type=element.metadata.image_mime_type))\n",
    "            case \"Table\":\n",
    "                table_elements.append(TableElement(id=element.id, \n",
    "                                                   text=element.text,\n",
    "                                                   type=element.category, \n",
    "                                                   page_number=element.metadata.page_number,\n",
    "                                                   image_base_64=element.metadata.image_base64,\n",
    "                                                   image_mime_type=element.metadata.image_mime_type,\n",
    "                                                   text_as_html=element.metadata.text_as_html))\n",
    "            # Assume some kind of text element if we can't match the category\n",
    "            # Could be headers, figure captions, etc\n",
    "            case _:\n",
    "                try:\n",
    "                    text_elements.append(TextElement(id=element.id, \n",
    "                                                 text=element.text, \n",
    "                                                 type=element.category, \n",
    "                                                 page_number=element.metadata.page_number))\n",
    "                except Exception as e:\n",
    "                    print(f\"Error parsing text element: {e}\")\n",
    "\n",
    "        unstructured_element_part_of_chunk.append(UnstructuredElementPartOfChunk(unstructured_element_id=element.id, chunk_id=chunk.id))\n",
    "\n",
    "    # we return a list of records for each entity and relationship instead of the Pydantic classes\n",
    "    return {\n",
    "        \"nodes\": {\n",
    "            \"chunk\": [chunk.model_dump()],\n",
    "            \"text_element\": [el.model_dump() for el in text_elements],\n",
    "            \"image_element\": [el.model_dump() for el in image_elements],\n",
    "            \"table_element\": [el.model_dump() for el in table_elements],\n",
    "        },\n",
    "        \"relationships\": {\n",
    "            \"chunk_part_of_document\": [chunk_part_of_document.model_dump()],\n",
    "            \"unstructured_element_part_of_chunk\": [rel.model_dump() for rel in unstructured_element_part_of_chunk],\n",
    "        }\n",
    "    }\n",
    "\n",
    "def parse_nodes_and_relationships_from_composite_elements(composite_elements: list[CompositeElement], parent_doc_id: str) -> dict[str, dict[str, pd.DataFrame]]:\n",
    "    \"\"\"Parse entity nodes and document relationships for a set of chunks (CompositeElements) and their parent document\"\"\"\n",
    "    \n",
    "    data = {\n",
    "        \"nodes\": {\n",
    "            \"document\": list(),\n",
    "            \"chunk\": list(),\n",
    "            \"text_element\": list(),\n",
    "            \"image_element\": list(),\n",
    "            \"table_element\": list(),\n",
    "        },\n",
    "        \"relationships\": {\n",
    "            \"chunk_part_of_document\": list(),\n",
    "            \"unstructured_element_part_of_chunk\": list(),\n",
    "            \"chunk_has_next_chunk\": list()\n",
    "        }\n",
    "    }\n",
    "\n",
    "    for composite_element in composite_elements:\n",
    "        new_data = parse_node_and_relationship_from_composite_element(composite_element, parent_doc_id)\n",
    "\n",
    "        # update the records with new nodes and relationships\n",
    "        data[\"nodes\"][\"chunk\"].extend(new_data[\"nodes\"][\"chunk\"])\n",
    "        data[\"relationships\"][\"chunk_part_of_document\"].extend(new_data[\"relationships\"][\"chunk_part_of_document\"])\n",
    "        data[\"nodes\"][\"text_element\"].extend(new_data[\"nodes\"][\"text_element\"])\n",
    "        data[\"nodes\"][\"image_element\"].extend(new_data[\"nodes\"][\"image_element\"])\n",
    "        data[\"nodes\"][\"table_element\"].extend(new_data[\"nodes\"][\"table_element\"])\n",
    "        data[\"relationships\"][\"unstructured_element_part_of_chunk\"].extend(new_data[\"relationships\"][\"unstructured_element_part_of_chunk\"])\n",
    "\n",
    "    # convert to pandas dataframe for ingestion\n",
    "    # node DataFrames\n",
    "    data[\"nodes\"][\"chunk\"] = pd.DataFrame(data[\"nodes\"][\"chunk\"])\n",
    "    data[\"nodes\"][\"text_element\"] = pd.DataFrame(data[\"nodes\"][\"text_element\"])\n",
    "    data[\"nodes\"][\"image_element\"] = pd.DataFrame(data[\"nodes\"][\"image_element\"])\n",
    "    data[\"nodes\"][\"table_element\"] = pd.DataFrame(data[\"nodes\"][\"table_element\"])\n",
    "\n",
    "    document_title = extract_document_title(data[\"nodes\"][\"text_element\"])\n",
    "    data[\"nodes\"][\"document\"] = pd.DataFrame([Document(id=parent_doc_id, name=document_title, source=\"pubmed\").model_dump()])\n",
    "\n",
    "    # relationship DataFrames\n",
    "    data[\"relationships\"][\"chunk_part_of_document\"] = pd.DataFrame(data[\"relationships\"][\"chunk_part_of_document\"])\n",
    "    data[\"relationships\"][\"unstructured_element_part_of_chunk\"] = pd.DataFrame(data[\"relationships\"][\"unstructured_element_part_of_chunk\"])\n",
    "    data[\"relationships\"][\"chunk_has_next_chunk\"] = create_chunk_has_next_chunk_relationship_dataframe(data[\"nodes\"][\"chunk\"])\n",
    "\n",
    "    return data\n",
    "\n",
    "# def process_xml_article(file_name: str) -> dict[str, dict[str, pd.DataFrame]]:\n",
    "#     \"\"\"\n",
    "#     Process an article and return the nodes and relationships for ingestion into the knowledge graph.\n",
    "#     Assumes that \n",
    "#     * the article name follows the format \"{pmid}-{title}.xml\"\n",
    "#     * the article is stored in the \"articles/\" directory\n",
    "\n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     file_name : str\n",
    "#         The name of the file to process.\n",
    "\n",
    "#     Returns\n",
    "#     -------\n",
    "#     dict[str, dict[str, pd.DataFrame]]\n",
    "#         A dictionary containing the node and relationship Pandas DataFrames for ingestion into the knowledge graph.\n",
    "#     \"\"\"\n",
    "#     doc_id, title = parse_article_file_name(file_name)\n",
    "#     parent_document = Document(id=str(uuid4()), pm_id=doc_id, name=title, source=\"pubmed\")\n",
    "#     partitioned_doc = partition_xml(\"articles/\" + file_name, \n",
    "#                                     xml_keep_tags=False, \n",
    "#                                     chunking_strategy=\"by_title\", \n",
    "#                                     combine_text_under_n_chars=200, \n",
    "#                                     max_characters=500, \n",
    "#                                     multipage_sections=True)\n",
    "#     # return partitioned_doc\n",
    "#     return parse_nodes_and_relationships_from_chunk_elements(partitioned_doc, parent_document)\n",
    "\n",
    "def process_pdf_article(file_name: str) -> dict[str, dict[str, pd.DataFrame]]:\n",
    "    \"\"\"\n",
    "    Process an article and return the nodes and relationships for ingestion into the knowledge graph.\n",
    "    Assumes that \n",
    "    * the article name follows the format \"{pmid}-{title}.pdf\"\n",
    "    * the article is stored in the \"articles/\" directory\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_name : str\n",
    "        The name of the file to process.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict[str, dict[str, pd.DataFrame]]\n",
    "        A dictionary containing the node and relationship Pandas DataFrames for ingestion into the knowledge graph.\n",
    "    \"\"\"\n",
    "    # doc_id, title = parse_article_file_name(file_name)\n",
    "    doc_id = hashlib.sha256(file_name.encode()).hexdigest()\n",
    "    # parent_document = Document(id=str(uuid4()), pm_id=file_name, name=file_name, source=\"pubmed\")\n",
    "    partitioned_doc = partition_pdf(\"articles/pdf/\" + file_name, \n",
    "                                    strategy=\"hi_res\",                                     \n",
    "                                    extract_images_in_pdf=True,\n",
    "                                    extract_image_block_types=[\"Image\", \"Table\"], \n",
    "                                    extract_image_block_to_payload=True,               \n",
    "                                    # extract_image_block_output_dir=f\"figures/{file_name[:-4]}\",\n",
    "                                    chunking_strategy=\"by_title\", \n",
    "                                    combine_text_under_n_chars=200, \n",
    "                                    max_characters=1000, \n",
    "                                    multipage_sections=True)\n",
    "    # return partitioned_doc\n",
    "    return parse_nodes_and_relationships_from_composite_elements(partitioned_doc, doc_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdf_articles(article_file_names: list[str]) -> dict[str, dict[str, pd.DataFrame]]:\n",
    "    \"\"\"\n",
    "    Process a list of articles and return the nodes and relationships for ingestion into the knowledge graph.\n",
    "    Assumes that the articles are stored in the \"articles/pdf/\" directory\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    article_file_names : list[str]\n",
    "        A list of the names of the files to process.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict[str, dict[str, pd.DataFrame]]\n",
    "        A dictionary containing the node and relationship Pandas DataFrames for ingestion into the knowledge graph.\n",
    "    \"\"\"\n",
    "\n",
    "    # initialize the DataFrames\n",
    "    data = {\n",
    "        \"nodes\": {\n",
    "            \"document\": pd.DataFrame(),\n",
    "            \"chunk\": pd.DataFrame(),\n",
    "            \"text_element\": pd.DataFrame(),\n",
    "            \"image_element\": pd.DataFrame(),\n",
    "            \"table_element\": pd.DataFrame(),\n",
    "        },\n",
    "        \"relationships\": {\n",
    "            \"chunk_part_of_document\": pd.DataFrame(),\n",
    "            \"unstructured_element_part_of_chunk\": pd.DataFrame(),\n",
    "            \"chunk_has_next_chunk\": pd.DataFrame()\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # process each article individually\n",
    "    # this will\n",
    "    # * partition the article into chunks using Unstructured.IO\n",
    "    # * Identify TextElements, ImageElements, and TableElements in each chunk\n",
    "    # * Create DataFrames for all lexical nodes and relationships found in the article\n",
    "    # * Update the global DataFrames with the new article data\n",
    "    for file_name in article_file_names:\n",
    "        print(f\"Processing article: {file_name}\")\n",
    "        # process each article \n",
    "        article_data = process_pdf_article(file_name)\n",
    "\n",
    "        # update the DataFrames with the new article data\n",
    "        data[\"nodes\"][\"document\"] = pd.concat([data[\"nodes\"][\"document\"], article_data[\"nodes\"][\"document\"]], ignore_index=True)\n",
    "        data[\"nodes\"][\"chunk\"] = pd.concat([data[\"nodes\"][\"chunk\"], article_data[\"nodes\"][\"chunk\"]], ignore_index=True)\n",
    "        data[\"nodes\"][\"text_element\"] = pd.concat([data[\"nodes\"][\"text_element\"], article_data[\"nodes\"][\"text_element\"]], ignore_index=True)\n",
    "        data[\"nodes\"][\"image_element\"] = pd.concat([data[\"nodes\"][\"image_element\"], article_data[\"nodes\"][\"image_element\"]], ignore_index=True)\n",
    "        data[\"nodes\"][\"table_element\"] = pd.concat([data[\"nodes\"][\"table_element\"], article_data[\"nodes\"][\"table_element\"]], ignore_index=True)\n",
    "        data[\"relationships\"][\"chunk_part_of_document\"] = pd.concat([data[\"relationships\"][\"chunk_part_of_document\"], article_data[\"relationships\"][\"chunk_part_of_document\"]], ignore_index=True)\n",
    "        data[\"relationships\"][\"unstructured_element_part_of_chunk\"] = pd.concat([data[\"relationships\"][\"unstructured_element_part_of_chunk\"], article_data[\"relationships\"][\"unstructured_element_part_of_chunk\"]], ignore_index=True)\n",
    "        data[\"relationships\"][\"chunk_has_next_chunk\"] = pd.concat([data[\"relationships\"][\"chunk_has_next_chunk\"], article_data[\"relationships\"][\"chunk_has_next_chunk\"]], ignore_index=True)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexical Graph Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will embed the text fields of our lexical graph for vector similarity search. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Domain Graph Schema Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to define our knowledge graph schema. This information will be passed to the entity extraction LLM to control which entities and relationships are pulled out of the text.\n",
    "\n",
    "This is necessary to prevent our schema from growing too large with an unbounded extraction process.\n",
    "\n",
    "We are using Pydantic to define the schema here since it can be used to validate any returned results as well. This ensures that all data we are ingesting into Neo4j adheres to this structure.\n",
    "\n",
    "Here is what our domain graph data model looks like.\n",
    "\n",
    "<img src=\"./assets/images/domain-data-model-v1.png\" alt=\"domain-data-model\" width=\"600px\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Medication(BaseModel):\n",
    "    \"\"\"\n",
    "    A substance used for medical treatment - a medicine or drug. \n",
    "    This is a general representation of a medication. \n",
    "    A Medication node may have relationships to StudyMedication nodes that are specific to a particular study.\n",
    "    \"\"\"\n",
    "    \n",
    "    name: str = Field(..., description=\"Name of the medication. Should also be uniquely identifiable.\")\n",
    "    medication_class: str = Field(..., description=\"Drug class (e.g., GLP-1 RA, SGLT2i)\")\n",
    "    mechanism: Optional[str] = Field(None, description=\"Mechanism of action\")\n",
    "    generic_name: Optional[str] = Field(None, description=\"Generic name if different from name\")\n",
    "    brand_names: Optional[List[str]] = Field(None, description=\"Commercial brand names\")\n",
    "    approval_status: Optional[str] = Field(None, description=\"FDA approval status\")\n",
    "    \n",
    "    class Config:\n",
    "        json_schema_extra = {\n",
    "            \"examples\": [\n",
    "                {\n",
    "                    \"name\": \"Semaglutide\", \n",
    "                    \"medication_class\": \"GLP-1 receptor agonist\",\n",
    "                    \"mechanism\": \"GLP-1 receptor activation\",\n",
    "                    \"generic_name\": \"semaglutide\",\n",
    "                    \"brand_names\": [\"Ozempic\", \"Wegovy\", \"Rybelsus\"],\n",
    "                    \"approval_status\": \"FDA approved\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "\n",
    "\n",
    "class StudyMedication(BaseModel):\n",
    "    \"\"\"\n",
    "    Study-specific medication usage - how a medication was used in a particular study\n",
    "    This is an instance of a medication that is used in a particular study. \n",
    "    A StudyMedication node should also have a relationship with a Medication node.\n",
    "    \"\"\"\n",
    "    study_name: str = Field(..., description=\"Name of the study. This is used to uniquely identify the StudyMedication node.\")\n",
    "    treatment_arm: str = Field(..., description=\"Treatment arm of the study medication. This uniquely identifies the StudyMedication node.\")\n",
    "    dosage: Optional[str] = Field(None, description=\"Dosage used in this study\")\n",
    "    route: Optional[str] = Field(None, description=\"Route of administration\")\n",
    "    frequency: Optional[str] = Field(None, description=\"Dosing frequency\")\n",
    "    treatment_duration: Optional[str] = Field(None, description=\"Duration of treatment\")\n",
    "    comparator: Optional[str] = Field(None, description=\"What this was compared against\")\n",
    "    adherence_rate: Optional[float] = Field(None, description=\"Treatment adherence rate\")\n",
    "    formulation: Optional[str] = Field(None, description=\"Specific formulation used\")\n",
    "\n",
    "    @computed_field(return_type=str)\n",
    "    def study_medication_id(self) -> str:\n",
    "        \"\"\"\n",
    "        The unique id of the study medication.\n",
    "        This is a sha256 hash of the study name and treatment arm.\n",
    "        \"\"\"\n",
    "        return hashlib.sha256(f\"{self.study_name}_{self.treatment_arm}\".encode()).hexdigest()\n",
    "    \n",
    "    class Config:\n",
    "        json_schema_extra = {\n",
    "            # don't include the study_medication_id in the example since this is computed from extracted fields\n",
    "            \"examples\": [\n",
    "                {\n",
    "                    \"study_name\": \"Study 1\",\n",
    "                    \"treatment_arm\": \"Treatment arm 1\",\n",
    "                    \"dosage\": \"1.0 mg\",\n",
    "                    \"route\": \"subcutaneous\",\n",
    "                    \"frequency\": \"weekly\",\n",
    "                    \"treatment_duration\": \"12 weeks\",\n",
    "                    \"comparator\": \"placebo\",\n",
    "                    \"adherence_rate\": 85.5,\n",
    "                    \"formulation\": \"pre-filled pen\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "\n",
    "\n",
    "class ClinicalOutcome(BaseModel):\n",
    "    \"\"\"\n",
    "    Measured clinical outcomes and biomarkers.\n",
    "    This node represents a clinical outcome present in a study.\n",
    "    ClinicalOutcome nodes should have relationships with other entity nodes from a study.\n",
    "    ClinicalOutcome nodes should not have relationships with entities that exist outside the study.\n",
    "    \"\"\"\n",
    "    \n",
    "    study_name: str = Field(..., description=\"Name of the study this outcome is associated with. This is used to uniquely identify the ClinicalOutcome node.\")\n",
    "    name: str = Field(..., description=\"A concise detailed name for the outcome.\")\n",
    "\n",
    "    @computed_field(return_type=str)\n",
    "    def clinical_outcome_id(self) -> str:\n",
    "        \"\"\"\n",
    "        The unique id of the clinical outcome.\n",
    "        This is a sha256 hash of the study name and the name of the outcome.\n",
    "        \"\"\"\n",
    "        return hashlib.sha256(f\"{self.study_name}_{self.name}\".encode()).hexdigest()\n",
    "    \n",
    "    class Config:\n",
    "        json_schema_extra = {\n",
    "            \"examples\": [\n",
    "                # don't include the clinical_outcome_id in the example since this is computed from extracted fields\n",
    "                {\n",
    "                    \"study_name\": \"Study 1\",\n",
    "                    \"name\": \"A1C controlled\",\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "\n",
    "\n",
    "class MedicalCondition(BaseModel):\n",
    "    \"\"\"Medical conditions and comorbidities studied\"\"\"\n",
    "    \n",
    "    name: str = Field(..., description=\"Name of the medical condition\")\n",
    "    category: str = Field(..., description=\"Category of condition\")\n",
    "    severity: Optional[str] = Field(None, description=\"Severity or stage when specified\")\n",
    "    icd10_code: Optional[str] = Field(None, description=\"ICD-10 code when available\")\n",
    "    duration: Optional[str] = Field(None, description=\"Duration of condition if specified\")\n",
    "    \n",
    "    @field_validator(\"icd10_code\")\n",
    "    def validate_icd10_code(cls, v: str) -> str:\n",
    "        \"\"\"\n",
    "        Validate that the ICD-10 code is valid.\n",
    "        \"\"\"\n",
    "        # ICD-10 codes are 3-7 characters long\n",
    "        if len(v) < 3 or len(v) > 7:\n",
    "            raise ValueError(\"ICD-10 code must be between 3 and 7 characters long.\")\n",
    "        # first character must be a letter\n",
    "        elif not v[0].isalpha():\n",
    "            raise ValueError(\"ICD-10 code must start with a letter.\")\n",
    "        # first character not case sensitive, can't be U, O, or I\n",
    "        elif v[0].upper() in [\"U\", \"O\", \"I\"]:\n",
    "            raise ValueError(\"ICD-10 code can not start with 'U', 'O', or 'I'.\")\n",
    "        # second character must be a digit\n",
    "        elif not v[1].isdigit():\n",
    "            raise ValueError(\"ICD-10 code second character must be a digit.\")\n",
    "        # '.' must separate the first 3 characters from the rest of the code\n",
    "        # examples:\n",
    "        # S52 Fracture of forearm\n",
    "        # S52.5 Fracture of lower end of radius\n",
    "        # S52.52 Torus fracture of lower end of radius\n",
    "        # S52.521 Torus fracture of lower end of right radius\n",
    "        # S52.521A Torus fracture of lower end of right radius, initial encounter, closed fracture\n",
    "        elif len(v) > 3 and not v[3] == '.':\n",
    "            raise ValueError(\"ICD-10 code must have a '.' after the first 3 characters.\")\n",
    "        return v\n",
    "    \n",
    "    class Config:\n",
    "        json_schema_extra = {\n",
    "            \"examples\": [\n",
    "                {\n",
    "                    \"name\": \"Type 2 diabetes mellitus\",\n",
    "                    \"category\": \"Primary condition\", \n",
    "                    \"severity\": \"moderate\",\n",
    "                    \"icd10_code\": \"E11\",\n",
    "                    \"duration\": \"5-10 years\",\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "\n",
    "\n",
    "class StudyPopulation(BaseModel):\n",
    "    \"\"\"Patient populations and demographics in research studies\"\"\"\n",
    "    \n",
    "    study_name: str = Field(..., description=\"Name of the study. This is used to uniquely identify the StudyPopulation node.\")\n",
    "    description: str = Field(..., description=\"Description of the population\")\n",
    "    min_age: Optional[int] = Field(None, description=\"Minimum age in years\")\n",
    "    max_age: Optional[int] = Field(None, description=\"Maximum age in years\")\n",
    "    male_percentage: Optional[float] = Field(None, description=\"Percentage of male gender participants\")\n",
    "    female_percentage: Optional[float] = Field(None, description=\"Percentage of female gender participants\")\n",
    "    other_gender_percentage: Optional[float] = Field(None, description=\"Percentage of participants that identify as another gender\")\n",
    "    sample_size: Optional[int] = Field(None, description=\"Number of participants\")\n",
    "    study_type: str = Field(..., description=\"Type of study\")\n",
    "    location: Optional[str] = Field(None, description=\"Geographic location of study\")\n",
    "    inclusion_criteria: Optional[List[str]] = Field(None, description=\"Key inclusion criteria\")\n",
    "    exclusion_criteria: Optional[List[str]] = Field(None, description=\"Key exclusion criteria\")\n",
    "    study_duration: Optional[str] = Field(None, description=\"Duration of study\")\n",
    "    \n",
    "    class Config:\n",
    "        json_schema_extra = {\n",
    "            \"examples\": [\n",
    "                {\n",
    "                    \"study_name\": \"Study 1\",\n",
    "                    \"description\": \"Adults with T2DM and schizophrenia\",\n",
    "                    \"min_age\": 30,\n",
    "                    \"max_age\": 39,\n",
    "                    \"male_percentage\": 46.0,\n",
    "                    \"female_percentage\": 53.0,\n",
    "                    \"other_gender_percentage\": 1.0,\n",
    "                    \"sample_size\": 100,\n",
    "                    \"study_type\": \"Observational study\",\n",
    "                    \"location\": \"Denmark\",\n",
    "                    \"inclusion_criteria\": [\"Type 2 diabetes diagnosis\", \"Schizophrenia diagnosis\", \"Age ≥18\"],\n",
    "                    \"study_duration\": \"12 months\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "\n",
    "    @computed_field(return_type=str)\n",
    "    def study_population_id(self) -> str:\n",
    "        \"\"\"\n",
    "        The unique id of the study population.\n",
    "        This is a sha256 hash of the study name.\n",
    "        \"\"\"\n",
    "        return hashlib.sha256(f\"{self.study_name}_{self.description}\".encode()).hexdigest()\n",
    "\n",
    "\n",
    "# Relationship classes\n",
    "class StudyMedicationUsesMedication(BaseModel):\n",
    "    \"\"\"\n",
    "    Links StudyMedication to Medication nodes.\n",
    "    StudyMedication nodes should have a relationship with a Medication node.\n",
    "    Pattern: (:StudyMedication)-[:USES_MEDICATION]->(:Medication)\n",
    "    \"\"\"\n",
    "    medication_name: str\n",
    "    study_medication_study_name: str = Field(..., description=\"Name of the study. This is used to uniquely identify the StudyMedication node.\")\n",
    "    study_medication_treatment_arm: str = Field(..., description=\"Treatment arm of the study medication. This uniquely identifies the StudyMedication node.\")\n",
    "\n",
    "    @computed_field(return_type=str)\n",
    "    def study_medication_id(self) -> str:\n",
    "        \"\"\"\n",
    "        The unique id of the study medication.\n",
    "        This is a sha256 hash of the study name and treatment arm.\n",
    "        \"\"\"\n",
    "        return hashlib.sha256(f\"{self.study_medication_study_name}_{self.study_medication_treatment_arm}\".encode()).hexdigest()\n",
    "\n",
    "\n",
    "class StudyMedicationProducesClinicalOutcome(BaseModel):\n",
    "    \"\"\"\n",
    "    Links StudyMedication to ClinicalOutcome nodes.\n",
    "    StudyMedication nodes should have a relationship with a ClinicalOutcome node.\n",
    "    Pattern: (:StudyMedication)-[:PRODUCES_CLINICAL_OUTCOME]->(:ClinicalOutcome)\n",
    "    \"\"\"\n",
    "    study_medication_study_name: str = Field(..., description=\"Name of the study. This is used to uniquely identify the StudyMedication node.\")\n",
    "    study_medication_treatment_arm: str = Field(..., description=\"Treatment arm of the study medication. This uniquely identifies the StudyMedication node.\")\n",
    "    clinical_outcome_name: str = Field(..., description=\"Name of the clinical outcome\")\n",
    "\n",
    "    @computed_field(return_type=str)\n",
    "    def clinical_outcome_id(self) -> str:\n",
    "        \"\"\"\n",
    "        The unique id of the clinical outcome.\n",
    "        This is a sha256 hash of the study name and the name of the outcome.\n",
    "        \"\"\"\n",
    "        return hashlib.sha256(f\"{self.study_medication_study_name}_{self.clinical_outcome_name}\".encode()).hexdigest()\n",
    "    \n",
    "    @computed_field(return_type=str)\n",
    "    def study_medication_id(self) -> str:\n",
    "        \"\"\"\n",
    "        The unique id of the study medication.\n",
    "        This is a sha256 hash of the study name and treatment arm.\n",
    "        \"\"\"\n",
    "        return hashlib.sha256(f\"{self.study_medication_study_name}_{self.study_medication_treatment_arm}\".encode()).hexdigest()\n",
    "\n",
    "\n",
    "class StudyPopulationHasMedicalCondition(BaseModel):\n",
    "    \"\"\"\n",
    "    Links StudyPopulation to MedicalCondition nodes.\n",
    "    StudyPopulation nodes should have a relationship with a MedicalCondition node.\n",
    "    Pattern: (:StudyPopulation)-[:HAS_MEDICAL_CONDITION]->(:MedicalCondition)\n",
    "    \"\"\"\n",
    "    study_name: str = Field(..., description=\"Name of the study. This is used to uniquely identify the StudyPopulation node.\")\n",
    "    study_population_description: str = Field(..., description=\"Description of the study population.\")\n",
    "    medical_condition_name: str\n",
    "\n",
    "    @computed_field(return_type=str)\n",
    "    def study_population_id(self) -> str:\n",
    "        \"\"\"\n",
    "        The unique id of the study population.\n",
    "        This is a sha256 hash of the study name and population description.\n",
    "        \"\"\"\n",
    "        return hashlib.sha256(f\"{self.study_name}_{self.study_population_description}\".encode()).hexdigest()\n",
    "\n",
    "\n",
    "class StudyPopulationReceivesStudyMedication(BaseModel):\n",
    "    \"\"\"\n",
    "    Links StudyPopulation to StudyMedication nodes.\n",
    "    StudyPopulation nodes should have a relationship with a StudyMedication node.\n",
    "    Pattern: (:StudyPopulation)-[:RECEIVES_STUDY_MEDICATION]->(:StudyMedication)\n",
    "    \"\"\"\n",
    "    study_name: str = Field(..., description=\"Name of the study. This is used to uniquely identify the StudyPopulation node.\")\n",
    "    study_population_description: str = Field(..., description=\"Description of the study population.\")\n",
    "    study_medication_treatment_arm: str = Field(..., description=\"Treatment arm of the study medication. This uniquely identifies the StudyMedication node.\")\n",
    "\n",
    "    @computed_field(return_type=str)\n",
    "    def study_medication_id(self) -> str:\n",
    "        \"\"\"\n",
    "        The unique id of the study medication.\n",
    "        This is a sha256 hash of the study name and treatment arm.\n",
    "        \"\"\"\n",
    "        return hashlib.sha256(f\"{self.study_name}_{self.study_medication_treatment_arm}\".encode()).hexdigest()\n",
    "\n",
    "    @computed_field(return_type=str)\n",
    "    def study_population_id(self) -> str:\n",
    "        \"\"\"\n",
    "        The unique id of the study population.\n",
    "        This is a sha256 hash of the study name and population description.\n",
    "        \"\"\"\n",
    "        return hashlib.sha256(f\"{self.study_name}_{self.study_population_description}\".encode()).hexdigest()\n",
    "\n",
    "\n",
    "class StudyPopulationHasClinicalOutcome(BaseModel):\n",
    "    \"\"\"\n",
    "    Links StudyPopulation to ClinicalOutcome nodes.\n",
    "    StudyPopulation nodes should have a relationship with a ClinicalOutcome node.\n",
    "    Pattern: (:StudyPopulation)-[:HAS_CLINICAL_OUTCOME]->(:ClinicalOutcome)\n",
    "    \"\"\"\n",
    "    study_name: str = Field(..., description=\"Name of the study. This is used to uniquely identify the StudyPopulation node.\")\n",
    "    study_population_description: str = Field(..., description=\"Description of the study population.\")\n",
    "    clinical_outcome_name: str = Field(..., description=\"Name of the clinical outcome to match on.\")\n",
    "\n",
    "    @computed_field(return_type=str)\n",
    "    def study_population_id(self) -> str:\n",
    "        \"\"\"\n",
    "        The unique id of the study population.\n",
    "        This is a sha256 hash of the study name and population description.\n",
    "        \"\"\"\n",
    "        return hashlib.sha256(f\"{self.study_name}_{self.study_population_description}\".encode()).hexdigest()\n",
    "\n",
    "    @computed_field(return_type=str)\n",
    "    def clinical_outcome_id(self) -> str:\n",
    "        \"\"\"\n",
    "        The unique id of the clinical outcome.\n",
    "        This is a sha256 hash of the study name and the name of the outcome.\n",
    "        \"\"\"\n",
    "        return hashlib.sha256(f\"{self.study_name}_{self.clinical_outcome_name}\".encode()).hexdigest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lexical and domain knowledge graphs will be linked with `HAS_ENTITY` relationships between Chunk nodes and domain graph nodes.\n",
    "\n",
    "This is the combined lexical and domain graph data model.\n",
    "\n",
    "IMAGE OF DATA MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity Extraction via LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using [OpenAI](https://platform.openai.com/docs/overview) and the [Instructor](https://python.useinstructor.com/) library to perform our entity extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AsyncOpenAI\n",
    "import instructor\n",
    "from instructor.exceptions import IncompleteOutputException, InstructorRetryException, ValidationError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructor handles requesting structured outputs from the LLM. \n",
    "\n",
    "If the LLM fails to return output that adheres to the response models, Instructor will also handle the retry logic and pass any errors to inform corrections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = instructor.from_openai(AsyncOpenAI())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the system prompt defines the overall behavior of the LLM\n",
    "system_prompt = \"\"\"\n",
    "You are a healthcare research expert that is responsible for extracting detailed entities from PubMed articles. \n",
    "You will be provided a graph data model schema and must extract entities and relationships to populate a knowledge graph.\n",
    "\"\"\"\n",
    "\n",
    "async def extract_entities_from_text_chunk(text_chunk: str) -> list:\n",
    "    \"\"\"\n",
    "    Extract entities and relationships from a text chunk.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text_chunk : str\n",
    "        The text chunk to extract entities from.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list[Medication | StudyMedication | ClinicalOutcome | StudyMedicationUsesMedication | StudyMedicationProducesClinicalOutcome],\n",
    "        A list of entities and relationships extracted from the text chunk.\n",
    "        If the response is truncated, an empty list is returned.\n",
    "        If retries are exhausted, an empty list is returned.\n",
    "        If the response is invalid, an empty list is returned.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = await client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": text_chunk}\n",
    "            ],\n",
    "            response_model=list[\n",
    "                            # first test batch  \n",
    "                            Medication | \n",
    "                            StudyMedication | \n",
    "                            ClinicalOutcome | \n",
    "                            StudyMedicationUsesMedication | \n",
    "                            StudyMedicationProducesClinicalOutcome |\n",
    "                            # then add these\n",
    "                            StudyPopulation |\n",
    "                            MedicalCondition |\n",
    "                            StudyPopulationHasMedicalCondition |\n",
    "                            StudyPopulationReceivesStudyMedication |\n",
    "                            StudyPopulationHasClinicalOutcome\n",
    "                            ],\n",
    "            temperature=0.0\n",
    "        )\n",
    "        return response\n",
    "    except IncompleteOutputException as e:\n",
    "        # Handle truncated output\n",
    "        print(f\"Response output truncated. Skipping chunk.\")\n",
    "        return list()\n",
    "    except InstructorRetryException as e:\n",
    "        # Handle retry exhaustion\n",
    "        print(f\"Failed after {e.n_attempts} attempts. Skipping chunk.\")\n",
    "        return list()\n",
    "    except ValidationError as e:\n",
    "        # Handle validation errors\n",
    "        print(f\"Validation failed. Skipping chunk.\\nError: {e}\")\n",
    "        return list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def extract_entities_from_chunk_nodes(chunk_nodes_dataframe: pd.DataFrame, batch_size: int = 100) -> list[tuple[str, list[Any]]]:\n",
    "    \"\"\"\n",
    "    Process a Pandas DataFrame of Chunk nodes and return the entities found in each chunk.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    chunk_nodes_dataframe : pd.DataFrame\n",
    "        A Pandas DataFrame where each row represents a Chunk node.\n",
    "        Has columns `id` and `text`.\n",
    "    batch_size : int\n",
    "        The number of text chunks to process in each batch.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list[tuple[str, list[dict[str, Any]]]]\n",
    "        A list of tuples, where the first element is the chunk id and the second element is a list of entities found in the chunk.\n",
    "    \"\"\"\n",
    "\n",
    "    results = list()\n",
    "\n",
    "    for batch_idx, i in enumerate(range(0, len(chunk_nodes_dataframe), batch_size)):\n",
    "        if i + batch_size >= len(chunk_nodes_dataframe):\n",
    "            batch = chunk_nodes_dataframe.iloc[i:]\n",
    "        else:\n",
    "            batch = chunk_nodes_dataframe.iloc[i:i+batch_size]\n",
    "        print(f\"Processing batch {batch_idx+1} of {int(len(chunk_nodes_dataframe)/(batch_size))}  \\n\", end=\"\\r\")\n",
    "        # Create tasks for all nodes in the batch\n",
    "        # order is maintained\n",
    "        tasks = [extract_entities_from_text_chunk(row[\"text\"]) for _, row in batch.iterrows()]\n",
    "        # Execute all tasks concurrently\n",
    "        extraction_results = await asyncio.gather(*tasks)\n",
    "        # Add extracted records to the results list\n",
    "        results.extend(extraction_results)\n",
    "\n",
    "    # Return chunk_id paired with its entities from the results list\n",
    "    return [(chunk_id, entities) for chunk_id, entities in zip(chunk_nodes_dataframe[\"id\"], results)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to collect the article file names to pass to Unstructured for parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_names = os.listdir(\"articles/pdf/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now defined \n",
    "* Lexical and domain data models\n",
    "* Partitioning and chunking logic for articles\n",
    "* Entity extraction logic for chunks\n",
    "\n",
    "It is now time to define our ingestion logic. We will run ingest in three stages \n",
    "\n",
    "1. Load lexical graph\n",
    "2. Embed lexical graph Chunk nodes\n",
    "3. Extract domain / entity graph from lexical graph\n",
    "\n",
    "Decoupling these stages allows us easily make changes as we iterate our ingestion process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from pyneoinstance import Neo4jInstance, load_yaml_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our database credentials and all of our queries are stored in the `pyneoinstance_config.yaml` file. \n",
    "\n",
    "This makes it easy to manage our queries and keeps the notebook code clean. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_yaml_file(\"pyneoinstance_config.yaml\")\n",
    "\n",
    "db_info = config['db_info']\n",
    "\n",
    "constraints = config['initializing_queries']['constraints']\n",
    "indexes = config['initializing_queries']['indexes']\n",
    "\n",
    "node_load_queries = config['loading_queries']['nodes']\n",
    "relationship_load_queries = config['loading_queries']['relationships']\n",
    "\n",
    "processing_queries = config['processing_queries']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This graph object will handle database connections and read / write transactions for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = Neo4jInstance(db_info.get('uri', os.getenv(\"NEO4J_URI\", \"neo4j://localhost:7687\")), # use config value -> use env value -> use default value\n",
    "                      db_info.get('user', os.getenv(\"NEO4J_USER\", \"neo4j\")), \n",
    "                      db_info.get('password', os.getenv(\"NEO4J_PASSWORD\", \"password\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a helper function for ingesting data using the PyNeoInstance library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_partition(data: pd.DataFrame, batch_size: int = 500) -> int:\n",
    "    \"\"\"\n",
    "    Determine the data partition based on the desired batch size.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : pd.DataFrame\n",
    "        The Pandas DataFrame to partition.\n",
    "    batch_size : int\n",
    "        The desired batch size.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        The partition size.\n",
    "    \"\"\"\n",
    "    \n",
    "    partition = int(len(data) / batch_size)\n",
    "    print(\"partition: \"+str(partition if partition > 1 else 1))\n",
    "    return partition if partition > 1 else 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we write all the constraints and indexes we need for both the lexical and domain graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_constraints_and_indexes() -> None:\n",
    "    \"\"\"\n",
    "    Create constraints and indexes for the lexical and domain graphs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if constraints and len(constraints) > 0:\n",
    "            graph.execute_write_queries(database=db_info['database'], queries=list(constraints.values()))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    try:\n",
    "        if indexes and len(indexes) > 0:\n",
    "            graph.execute_write_queries(database=db_info['database'], queries=list(indexes.values()))\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "create_constraints_and_indexes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingest Lexical Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing | Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process the articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nihms-1852972.pdf',\n",
       " 'fendo-11-00178.pdf',\n",
       " 'Diabetic Medicine - 2023 - Brønden - Effects of DPP‐4 inhibitors  GLP‐1 receptor agonists  SGLT‐2 inhibitors and.pdf',\n",
       " 'jama_rosenstock_2019_oi_190026_1655321720.77793.pdf',\n",
       " 'jciinsight-3-93936.pdf']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing article: nihms-1852972.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cannot set gray non-stroke color because /'R50' is an invalid float value\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing article: fendo-11-00178.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cannot set gray non-stroke color because /'R50' is an invalid float value\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing article: Diabetic Medicine - 2023 - Brønden - Effects of DPP‐4 inhibitors  GLP‐1 receptor agonists  SGLT‐2 inhibitors and.pdf\n",
      "Processing article: jama_rosenstock_2019_oi_190026_1655321720.77793.pdf\n",
      "Processing article: jciinsight-3-93936.pdf\n"
     ]
    }
   ],
   "source": [
    "# lexical_ingest_records = process_pdf_article(article_names[0])\n",
    "lexical_ingest_records = process_pdf_articles(article_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the first few records "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>753d70915e2a2a747cee355745bd17ff08c45f90d938b1...</td>\n",
       "      <td>HHS Public Access</td>\n",
       "      <td>pubmed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>e0048a13f033f7fe71581406cd2d3bac1ffc4db7ce88a8...</td>\n",
       "      <td>OPEN ACCESS</td>\n",
       "      <td>pubmed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7d560f487686b70e9b42d82c08ea1b9a43e804ebc75b74...</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>pubmed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>db75d65961558fccb83719eed3a308ce6f794c27511b36...</td>\n",
       "      <td>KeyPoints</td>\n",
       "      <td>pubmed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1e5cfcba99bcd1dfb5aeb0cbb4c95b7ca3f35c8210a04e...</td>\n",
       "      <td>Metformin-induced glucagon-like peptide-1 secr...</td>\n",
       "      <td>pubmed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  id  \\\n",
       "0  753d70915e2a2a747cee355745bd17ff08c45f90d938b1...   \n",
       "1  e0048a13f033f7fe71581406cd2d3bac1ffc4db7ce88a8...   \n",
       "2  7d560f487686b70e9b42d82c08ea1b9a43e804ebc75b74...   \n",
       "3  db75d65961558fccb83719eed3a308ce6f794c27511b36...   \n",
       "4  1e5cfcba99bcd1dfb5aeb0cbb4c95b7ca3f35c8210a04e...   \n",
       "\n",
       "                                                name  source  \n",
       "0                                  HHS Public Access  pubmed  \n",
       "1                                        OPEN ACCESS  pubmed  \n",
       "2                                           Abstract  pubmed  \n",
       "3                                          KeyPoints  pubmed  \n",
       "4  Metformin-induced glucagon-like peptide-1 secr...  pubmed  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexical_ingest_records[\"nodes\"][\"document\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Document and Chunk nodes into the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_lexical_nodes(document_dataframe: pd.DataFrame, \n",
    "                       chunk_dataframe: pd.DataFrame, \n",
    "                       text_element_dataframe: pd.DataFrame, \n",
    "                       image_element_dataframe: pd.DataFrame, \n",
    "                       table_element_dataframe: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Load lexical nodes into the graph. These include Document and Chunk nodes.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    document_dataframe : pd.DataFrame\n",
    "        A Pandas DataFrame of Document nodes to load into the graph. \n",
    "    chunk_dataframe : pd.DataFrame\n",
    "        A Pandas DataFrame of Chunk nodes to load into the graph.\n",
    "    text_element_dataframe : pd.DataFrame\n",
    "        A Pandas DataFrame of TextElement nodes to load into the graph. \n",
    "    image_element_dataframe : pd.DataFrame\n",
    "        A Pandas DataFrame of ImageElement nodes to load into the graph. \n",
    "    table_element_dataframe : pd.DataFrame\n",
    "        A Pandas DataFrame of TableElement nodes to load into the graph. \n",
    "    \"\"\"\n",
    "    \n",
    "    lexical_nodes_ingest_iterator = list(zip([document_dataframe, \n",
    "                                              chunk_dataframe, \n",
    "                                              text_element_dataframe, \n",
    "                                              image_element_dataframe, \n",
    "                                              table_element_dataframe], \n",
    "                                              ['document', \n",
    "                                               'chunk', \n",
    "                                               'text_element', \n",
    "                                               'image_element', \n",
    "                                               'table_element']))\n",
    "\n",
    "    for data, query in lexical_nodes_ingest_iterator:\n",
    "        res = graph.execute_write_query_with_data(database=db_info['database'], \n",
    "                                                    data=data, \n",
    "                                                    query=node_load_queries[query], \n",
    "                                                    partitions=get_partition(data, batch_size=500), \n",
    "                                                    parallel=True,\n",
    "                                                    workers=2)\n",
    "        print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "partition: 1\n",
      "{'labels_added': 5, 'nodes_created': 5, 'properties_set': 15}\n",
      "partition: 1\n",
      "{'labels_added': 500, 'nodes_created': 500, 'properties_set': 1500}\n",
      "partition: 7\n",
      "{'labels_added': 7076, 'nodes_created': 3538, 'properties_set': 14152}\n",
      "partition: 1\n",
      "{'labels_added': 74, 'nodes_created': 37, 'properties_set': 222}\n",
      "partition: 1\n",
      "{'labels_added': 36, 'nodes_created': 18, 'properties_set': 108}\n"
     ]
    }
   ],
   "source": [
    "load_lexical_nodes(lexical_ingest_records[\"nodes\"][\"document\"], \n",
    "                   lexical_ingest_records[\"nodes\"][\"chunk\"], \n",
    "                   lexical_ingest_records[\"nodes\"][\"text_element\"], \n",
    "                   lexical_ingest_records[\"nodes\"][\"image_element\"], \n",
    "                   lexical_ingest_records[\"nodes\"][\"table_element\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the relationships into the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_lexical_relationships(chunk_part_of_document_dataframe: pd.DataFrame, \n",
    "                               unstructured_element_part_of_chunk_dataframe: pd.DataFrame, \n",
    "                               chunk_has_next_chunk_dataframe: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Load lexical relationships into the graph.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    chunk_part_of_document_dataframe : pd.DataFrame\n",
    "        A Pandas DataFrame of Chunk - PART_OF -> Document relationships to load into the graph.\n",
    "        Should have columns `chunk_id` and `document_id`.\n",
    "    unstructured_element_part_of_chunk_dataframe : pd.DataFrame\n",
    "        A Pandas DataFrame of UnstructuredElement - PART_OF -> Chunk relationships to load into the graph.\n",
    "        Should have columns `unstructured_element_id` and `chunk_id`.\n",
    "    chunk_has_next_chunk_dataframe : pd.DataFrame\n",
    "        A Pandas DataFrame of Chunk - HAS_NEXT_CHUNK -> Chunk relationships to load into the graph.\n",
    "        Should have columns `source_id` and `target_id`.\n",
    "    \"\"\"\n",
    "    lexical_relationships_ingest_iterator = list(zip([chunk_part_of_document_dataframe, \n",
    "                                                      unstructured_element_part_of_chunk_dataframe, \n",
    "                                                      chunk_has_next_chunk_dataframe], \n",
    "                                                      ['chunk_part_of_document', \n",
    "                                                       'unstructured_element_part_of_chunk', \n",
    "                                                       'chunk_has_next_chunk']))\n",
    "\n",
    "    for data, query in lexical_relationships_ingest_iterator:\n",
    "        res = graph.execute_write_query_with_data(database=db_info['database'], \n",
    "                                                    data=data, \n",
    "                                                    query=relationship_load_queries[query], \n",
    "                                                    partitions=get_partition(data, batch_size=500))\n",
    "        print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "partition: 1\n",
      "{'relationships_created': 500}\n",
      "partition: 7\n",
      "{'relationships_created': 3672}\n",
      "partition: 1\n",
      "{'relationships_created': 495}\n"
     ]
    }
   ],
   "source": [
    "load_lexical_relationships(lexical_ingest_records[\"relationships\"][\"chunk_part_of_document\"], \n",
    "                          lexical_ingest_records[\"relationships\"][\"unstructured_element_part_of_chunk\"],\n",
    "                          lexical_ingest_records[\"relationships\"][\"chunk_has_next_chunk\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embed Lexical Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will read Chunk nodes from the graph that don't have embedding properties yet. \n",
    "\n",
    "We will then embed the Chunk text property and add the embedding as a property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_index = ...\n",
    "\n",
    "def create_vector_index() -> None:\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(driver) -> None:\n",
    "    ...\n",
    "\n",
    "def embed_lexical_graph(driver) -> None:\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Entities from Lexical Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now perform entity extraction on the Chunk nodes to augment and connect to our patient journey graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunk_nodes_to_process_by_article_name(article_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Retrieve Chunk node id and text from the database that have a relationship to the Document with the article name provided.\n",
    "    These chunks may then be used as input to the entity extraction process.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    article_name : str\n",
    "        The name of the article to retrieve chunks for.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A Pandas DataFrame where each row represents a Chunk node connected to the Document with the article name provided.\n",
    "        Has columns `id` and `text`.\n",
    "    \"\"\" \n",
    "    _, title = parse_article_file_name(article_name)\n",
    "    return graph.execute_read_query(database=db_info['database'], \n",
    "                            parameters={\"article_name\": title}, \n",
    "                            query=processing_queries['get_chunk_nodes_to_process_by_article_name'], \n",
    "                        )\n",
    "\n",
    "def get_chunk_nodes_to_process(min_length: int = 100) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Retrieve Chunk node id and text from the database that don't have an embedding.\n",
    "    These chunks may then be used as input to the entity extraction process.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    min_length : int\n",
    "        The minimum length the text must be to be included in the DataFrame.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A Pandas DataFrame where each row represents a Chunk node that has text and is at least `min_length` characters long.\n",
    "        Has columns `id` and `text`.\n",
    "    \"\"\"\n",
    "    return graph.execute_read_query(database=db_info['database'], \n",
    "                            query=processing_queries['get_chunk_nodes_to_process'], \n",
    "                            parameters={\"min_length\": min_length},\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownPropertyKeyWarning} {category: UNRECOGNIZED} {title: The provided property key is not in the database} {description: One of the property names in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing property name is: embedding)} {position: line: 3, column: 9, offset: 49} for query: 'MATCH (c:Chunk)\\nWHERE c.text IS NOT NULL\\n  AND c.embedding IS NULL\\n  AND size(c.text) >= $min_length\\nRETURN c.id as id, c.text as text'\n"
     ]
    }
   ],
   "source": [
    "chunks_to_process = get_chunk_nodes_to_process(min_length=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 494 chunks to process\n",
      "\n",
      "First chunk:\n",
      "\n",
      "statistical significance (metformin + Ex9-39 vs. placebo + Ex9-39, P = 0.053). The glucose iAUC after metformin + saline was significantly smaller than the iAUC for metformin + Ex9-39 (P = 0.004). Based on individual iAUC values, the relative contribution of GLP-1 to the acute glucose-lowering effect of metformin was 75% ± 35%, calculated as follows: 100% × ([iAUCplacebo + saline – iAUCmetformin + saline] – [iAUCplacebo + Ex9–39 – iAUCmetformin + Ex9–39])/(iAUCplacebo + saline – iAUCmetformin + saline) (P = 0.05). Using a 2-way ANOVA, both metformin and Ex9-39 were shown to significantly affect postprandial plasma glucose (iAUC) (P = 0.005 and P = 0.002, respectively), but no interaction between the 2 factors was evident. The time courses of the C-peptide/glucose ratios are illustrated in Figure 2B, and the AUCs for C-peptide/glucose, insulin/glucose, and insulin secretion\n"
     ]
    }
   ],
   "source": [
    "print(f\"Found {len(chunks_to_process)} chunks to process\\n\")\n",
    "print(f\"First chunk:\\n\\n{chunks_to_process.loc[0,'text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed after 3 attempts10  \n"
     ]
    }
   ],
   "source": [
    "entity_ingest_records = await extract_entities_from_chunk_nodes(chunks_to_process[:200], batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('0024e9c2d7afcf519d4d13871816a21d',\n",
       "  [Medication(name='Metformin', medication_class='Biguanide', mechanism='Decreases hepatic glucose production and increases insulin sensitivity', generic_name='metformin', brand_names=['Glucophage', 'Fortamet', 'Glumetza'], approval_status='FDA approved'),\n",
       "   Medication(name='Ex9-39', medication_class='GLP-1 receptor antagonist', mechanism='Blocks GLP-1 receptor', generic_name='Exendin 9-39', brand_names=None, approval_status=None),\n",
       "   StudyMedication(study_name='Study on Metformin and Ex9-39', treatment_arm='Metformin + Ex9-39', dosage=None, route=None, frequency=None, treatment_duration=None, comparator='Placebo + Ex9-39', adherence_rate=None, formulation=None, study_medication_id='5028738cbc272b71e202b93d8188e3f3dd898fcb237b95dbe381f09f7a1c54c1'),\n",
       "   StudyMedication(study_name='Study on Metformin and Ex9-39', treatment_arm='Metformin + Saline', dosage=None, route=None, frequency=None, treatment_duration=None, comparator='Metformin + Ex9-39', adherence_rate=None, formulation=None, study_medication_id='d11550973b0d5b041ce554ad59c22aaea186a5db988cb0c3da719543e3456532'),\n",
       "   ClinicalOutcome(study_name='Study on Metformin and Ex9-39', name='Glucose iAUC reduction', clinical_outcome_id='4399d62d403f3f44c095d091c595634bfbee016de2310e3efec566ec4f33d02f'),\n",
       "   StudyMedicationProducesClinicalOutcome(study_medication_study_name='Study on Metformin and Ex9-39', study_medication_treatment_arm='Metformin + Ex9-39', clinical_outcome_name='Glucose iAUC reduction', clinical_outcome_id='4399d62d403f3f44c095d091c595634bfbee016de2310e3efec566ec4f33d02f', study_medication_id='5028738cbc272b71e202b93d8188e3f3dd898fcb237b95dbe381f09f7a1c54c1'),\n",
       "   StudyMedicationProducesClinicalOutcome(study_medication_study_name='Study on Metformin and Ex9-39', study_medication_treatment_arm='Metformin + Saline', clinical_outcome_name='Glucose iAUC reduction', clinical_outcome_id='4399d62d403f3f44c095d091c595634bfbee016de2310e3efec566ec4f33d02f', study_medication_id='d11550973b0d5b041ce554ad59c22aaea186a5db988cb0c3da719543e3456532'),\n",
       "   StudyMedicationUsesMedication(medication_name='Metformin', study_medication_study_name='Study on Metformin and Ex9-39', study_medication_treatment_arm='Metformin + Ex9-39', study_medication_id='5028738cbc272b71e202b93d8188e3f3dd898fcb237b95dbe381f09f7a1c54c1'),\n",
       "   StudyMedicationUsesMedication(medication_name='Ex9-39', study_medication_study_name='Study on Metformin and Ex9-39', study_medication_treatment_arm='Metformin + Ex9-39', study_medication_id='5028738cbc272b71e202b93d8188e3f3dd898fcb237b95dbe381f09f7a1c54c1'),\n",
       "   StudyMedicationUsesMedication(medication_name='Metformin', study_medication_study_name='Study on Metformin and Ex9-39', study_medication_treatment_arm='Metformin + Saline', study_medication_id='d11550973b0d5b041ce554ad59c22aaea186a5db988cb0c3da719543e3456532')]),\n",
       " ('027e9b03adf9e1f31148048ec58ea1bf',\n",
       "  [Medication(name='GLP-1 receptor agonist', medication_class='GLP-1RA', mechanism='GLP-1 receptor activation', generic_name=None, brand_names=None, approval_status=None),\n",
       "   Medication(name='DPP-4 inhibitor', medication_class='DPP-4 inhibitor', mechanism=None, generic_name=None, brand_names=None, approval_status=None),\n",
       "   Medication(name='Sitagliptin', medication_class='DPP-4 inhibitor', mechanism=None, generic_name=None, brand_names=None, approval_status=None)]),\n",
       " ('02c242e8062402d8058b02e0562b16ac',\n",
       "  [ClinicalOutcome(study_name='Unnamed Study', name='Global log-rank test for differences among four groups', clinical_outcome_id='2db69520c87adafaa03d58b508b358ed6d6c29e846c1d2448f40f5513f8041d2')]),\n",
       " ('02db38e600c6387ec94b8ae7bb68093d', []),\n",
       " ('03bfd6e76b1695062cdf81fa22660313',\n",
       "  [ClinicalOutcome(study_name='REWIND', name='MACE', clinical_outcome_id='2b7b4a52394358f9ee4fa84c156473629b65c134f81212931d20e693f6579317'),\n",
       "   ClinicalOutcome(study_name='REWIND', name='HHF', clinical_outcome_id='4c5dd6ed16a2dd5fc62c8e022ccc0f4593a2e5ec5b62ad6b47a994efada3035a'),\n",
       "   ClinicalOutcome(study_name='REWIND', name='Composite endpoint for kidney disease', clinical_outcome_id='604dda30a4d0780290e64d352412d5a37e6a62d89d12b9678b45316f6afc975c'),\n",
       "   ClinicalOutcome(study_name='SUSTAIN-6', name='MACE', clinical_outcome_id='35e11b69a707cfa5c55366df150a8e79be7081878afc4bfd31bae0410054114e'),\n",
       "   ClinicalOutcome(study_name='SUSTAIN-6', name='HHF', clinical_outcome_id='3c9e070f40821013a7a86699e00dbe0a48e7330c475e1fde22588dcbc4924e51'),\n",
       "   ClinicalOutcome(study_name='SUSTAIN-6', name='Composite endpoint for kidney disease', clinical_outcome_id='aa949b4ba4474bdb45c776c362f0fbab79cbfda576fd653af030ce99d37c316c')]),\n",
       " ('0422c069e545cbe0193630bfec136371',\n",
       "  [Medication(name='Metformin', medication_class='Biguanide', mechanism='Inhibition of a negative feedback loop leading to increased glucagon secretion', generic_name=None, brand_names=None, approval_status=None)]),\n",
       " ('04bca6dfbde73f0fc1028323a02b1e53',\n",
       "  [Medication(name='Semaglutide', medication_class='GLP-1 receptor agonist', mechanism=None, generic_name='semaglutide', brand_names=['Ozempic', 'Wegovy', 'Rybelsus'], approval_status='FDA approved'),\n",
       "   Medication(name='Sitagliptin', medication_class='Dipeptidyl peptidase 4 inhibitor', mechanism=None, generic_name='sitagliptin', brand_names=None, approval_status='FDA approved')]),\n",
       " ('04de94dd03e810d0d0c032c0910e0004',\n",
       "  [ClinicalOutcome(study_name='Study on Glucagon Concentrations', name='Increased glucagon concentrations after meal ingestion', clinical_outcome_id='4ea467c01b09ffba569771406ee35126ce75218b521ca7ba2833c0ef198e3c3f')]),\n",
       " ('06d164b325030c636c55559dca5ab01f',\n",
       "  [Medication(name='Semaglutide', medication_class='GLP-1 receptor agonist', mechanism='GLP-1 receptor activation', generic_name='semaglutide', brand_names=['Ozempic', 'Wegovy', 'Rybelsus'], approval_status='FDA approved'),\n",
       "   Medication(name='Sitagliptin', medication_class='DPP-4 inhibitor', mechanism=None, generic_name=None, brand_names=None, approval_status=None)]),\n",
       " ('072f6fc5f143b89bf521a5b75867f080',\n",
       "  [Medication(name='Exenatide', medication_class='GLP-1 receptor agonist', mechanism='GLP-1 receptor activation', generic_name='exenatide', brand_names=['Bydureon'], approval_status='FDA approved'),\n",
       "   Medication(name='Sitagliptin', medication_class='DPP-4 inhibitor', mechanism='DPP-4 inhibition', generic_name='sitagliptin', brand_names=['Januvia'], approval_status='FDA approved')])]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_ingest_records[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingest Entities Into Knowledge Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These functions load the extracted entities and relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These functions link the extracted entities with their text chunk nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENTITY_LABELS = {\n",
    "    \"Medication\", \n",
    "    \"StudyMedication\",\n",
    "    \"MedicalCondition\",\n",
    "    \"StudyPopulation\",\n",
    "    \"ClinicalOutcome\",\n",
    "}\n",
    "\n",
    "ENTITY_RELS = {\n",
    "    \"StudyMedicationUsesMedication\",\n",
    "    \"StudyMedicationProducesClinicalOutcome\",\n",
    "    \"StudyPopulationHasMedicalCondition\",\n",
    "    \"StudyPopulationReceivesStudyMedication\",\n",
    "    \"StudyPopulationHasClinicalOutcome\",\n",
    "    \n",
    "}\n",
    "\n",
    "def prepare_entities_for_ingestion(entities: list[tuple[str, list[Any]]]) -> dict[str, dict[str, pd.DataFrame]]:\n",
    "    \"\"\"\n",
    "    Prepare entities for ingestion into the graph.\n",
    "    This function takes the results of the `get_chunk_nodes_to_process_by_article_name` function and returns a dictionary of entity label keys and pandas dataframes of entities.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    entities : list[tuple[str, list[Any]]]\n",
    "        A list of tuples, where the first element is the chunk id and the second element is a list of entities found in the chunk.\n",
    "        Entities are Pydantic models that adhere to the domain graph data model.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict[str, dict[str, pd.DataFrame]]\n",
    "        A dictionary of entity label to pandas dataframe of entities.\n",
    "\n",
    "        {\n",
    "            \"nodes\": {\n",
    "                \"Medication\": pd.DataFrame(...),\n",
    "                \"StudyMedication\": pd.DataFrame(...),\n",
    "                ...\n",
    "            },\n",
    "            \"relationships\": {\n",
    "                \"StudyMedicationUsesMedication\": pd.DataFrame(...),\n",
    "                \"StudyMedicationProducesClinicalOutcome\": pd.DataFrame(...),\n",
    "                ...\n",
    "            }\n",
    "        }\n",
    "    \"\"\"\n",
    "\n",
    "    records_node_dict = {lbl: list() for lbl in ENTITY_LABELS}\n",
    "    records_rel_dict = {lbl: list() for lbl in ENTITY_RELS}\n",
    "\n",
    "    for chunk_id, entities in entities:\n",
    "        for entity in entities:\n",
    "            to_add = entity.model_dump()\n",
    "            to_add.update({\"chunk_id\": chunk_id})\n",
    "            # nodes\n",
    "            if entity.__class__.__name__ in ENTITY_LABELS:\n",
    "                records_node_dict[entity.__class__.__name__].append(to_add)\n",
    "            # rels\n",
    "            elif entity.__class__.__name__ in ENTITY_RELS:\n",
    "                records_rel_dict[entity.__class__.__name__].append(to_add)\n",
    "            else:\n",
    "                print(f\"Unknown entity type: {entity.__class__.__name__}\")\n",
    "\n",
    "    for key, value in records_node_dict.items():\n",
    "        records_node_dict[key] = pd.DataFrame(value).replace({float('nan'): None})\n",
    "\n",
    "    for key, value in records_rel_dict.items():\n",
    "        records_rel_dict[key] = pd.DataFrame(value).replace({float('nan'): None})\n",
    "\n",
    "    return {\"nodes\": records_node_dict, \"relationships\": records_rel_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_entity_nodes(medication_dataframe: pd.DataFrame, \n",
    "                      medical_condition_dataframe: pd.DataFrame, \n",
    "                      study_medication_dataframe: pd.DataFrame, \n",
    "                      study_population_dataframe: pd.DataFrame, \n",
    "                      clinical_outcome_dataframe: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Load entity nodes into the graph.\n",
    "    \"\"\"\n",
    "    \n",
    "    entity_nodes_ingest_iterator = list(zip([medication_dataframe, \n",
    "                                             medical_condition_dataframe, \n",
    "                                             study_medication_dataframe, \n",
    "                                             study_population_dataframe, \n",
    "                                             clinical_outcome_dataframe], \n",
    "                                             ['medication', \n",
    "                                              'medical_condition', \n",
    "                                              'study_medication', \n",
    "                                              'study_population', \n",
    "                                              'clinical_outcome']))\n",
    "\n",
    "    for data, query in entity_nodes_ingest_iterator:\n",
    "        if len(data) > 0:\n",
    "            print(f\"Loading {len(data)} {query} nodes\")\n",
    "            res = graph.execute_write_query_with_data(database=db_info['database'], \n",
    "                                                    data=data, \n",
    "                                                    query=node_load_queries[query], \n",
    "                                                    partitions=get_partition(data, batch_size=500),\n",
    "                                                    parallel=False)\n",
    "            print(res)\n",
    "        else:\n",
    "            print(f\"No {query} nodes to load\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_entity_relationships(study_medication_uses_medication_dataframe: pd.DataFrame,\n",
    "                              study_medication_produces_clinical_outcome_dataframe: pd.DataFrame,\n",
    "                              study_population_has_medical_condition_dataframe: pd.DataFrame,\n",
    "                              study_population_receives_study_medication_dataframe: pd.DataFrame,\n",
    "                              study_population_has_outcome_dataframe: pd.DataFrame,\n",
    "                              ) -> None:\n",
    "    \"\"\"\n",
    "    Load entity relationships into the graph.\n",
    "    \"\"\"\n",
    "    entity_relationships_ingest_iterator = list(zip([study_medication_uses_medication_dataframe, \n",
    "                                                      study_medication_produces_clinical_outcome_dataframe, \n",
    "                                                      study_population_has_medical_condition_dataframe, \n",
    "                                                      study_population_receives_study_medication_dataframe, \n",
    "                                                      study_population_has_outcome_dataframe], \n",
    "                                                      ['study_medication_uses_medication', \n",
    "                                                       'study_medication_produces_clinical_outcome', \n",
    "                                                       'study_population_has_medical_condition', \n",
    "                                                       'study_population_receives_study_medication', \n",
    "                                                       'study_population_has_clinical_outcome']))\n",
    "    \n",
    "    for data, query in entity_relationships_ingest_iterator:\n",
    "        if len(data) > 0:\n",
    "            print(f\"Loading {len(data)} {query} relationships\")\n",
    "            res = graph.execute_write_query_with_data(database=db_info['database'], \n",
    "                                                    data=data, \n",
    "                                                    query=relationship_load_queries[query], \n",
    "                                                    partitions=get_partition(data, batch_size=500),\n",
    "                                                    parallel=False)\n",
    "            print(res)\n",
    "        else:\n",
    "            print(f\"No {query} relationships to load\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def link_entities_to_chunks(medication_link_dataframe: pd.DataFrame, \n",
    "                      medical_condition_link_dataframe: pd.DataFrame, \n",
    "                      study_medication_link_dataframe: pd.DataFrame, \n",
    "                      study_population_link_dataframe: pd.DataFrame, \n",
    "                      clinical_outcome_link_dataframe: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Link entities to chunks.\n",
    "    \"\"\"\n",
    "    entity_link_iterator = list(zip([medication_link_dataframe, \n",
    "                                     medical_condition_link_dataframe, \n",
    "                                     study_medication_link_dataframe, \n",
    "                                     study_population_link_dataframe, \n",
    "                                     clinical_outcome_link_dataframe], \n",
    "                                     [\"chunk_has_entity_medication\",\n",
    "                                      \"chunk_has_entity_medical_condition\",\n",
    "                                      \"chunk_has_entity_study_medication\",\n",
    "                                      \"chunk_has_entity_study_population\",\n",
    "                                      \"chunk_has_entity_clinical_outcome\"]))\n",
    "    \n",
    "    for data, query in entity_link_iterator:\n",
    "        if len(data) > 0:\n",
    "            print(f\"Linking {len(data)} {query} entities to chunks\")\n",
    "            res = graph.execute_write_query_with_data(database=db_info['database'], \n",
    "                                                    data=data, \n",
    "                                                    query=relationship_load_queries[query], \n",
    "                                                    partitions=get_partition(data, batch_size=500),\n",
    "                                                    parallel=False)\n",
    "            print(res)\n",
    "        else:\n",
    "            print(f\"No {query} relationships to load\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingest_records = prepare_entities_for_ingestion(entity_ingest_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>study_name</th>\n",
       "      <th>description</th>\n",
       "      <th>min_age</th>\n",
       "      <th>max_age</th>\n",
       "      <th>male_percentage</th>\n",
       "      <th>female_percentage</th>\n",
       "      <th>other_gender_percentage</th>\n",
       "      <th>sample_size</th>\n",
       "      <th>study_type</th>\n",
       "      <th>location</th>\n",
       "      <th>inclusion_criteria</th>\n",
       "      <th>exclusion_criteria</th>\n",
       "      <th>study_duration</th>\n",
       "      <th>study_population_id</th>\n",
       "      <th>chunk_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Study 1</td>\n",
       "      <td>Patients with established atherosclerotic card...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>56.19</td>\n",
       "      <td>43.81</td>\n",
       "      <td>None</td>\n",
       "      <td>507.0</td>\n",
       "      <td>Randomized Controlled Trial</td>\n",
       "      <td>Not specified</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2 years</td>\n",
       "      <td>8e2f978b6e2de6b3a215313941adbd8fb5e42e5f92e9b9...</td>\n",
       "      <td>097f93c7cc27f257d4c04b3153765e6b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Study 2</td>\n",
       "      <td>Patients with type 2 diabetes and high cardiov...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>56.19</td>\n",
       "      <td>43.81</td>\n",
       "      <td>None</td>\n",
       "      <td>129.0</td>\n",
       "      <td>Randomized Controlled Trial</td>\n",
       "      <td>Not specified</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>5 weeks</td>\n",
       "      <td>ebf71b271dc74f0d2faf079da304d66ce3ad27a6ae5dd4...</td>\n",
       "      <td>097f93c7cc27f257d4c04b3153765e6b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Study 3</td>\n",
       "      <td>Patients with type 2 diabetes and high cardiov...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>56.19</td>\n",
       "      <td>43.81</td>\n",
       "      <td>None</td>\n",
       "      <td>199.0</td>\n",
       "      <td>Randomized Controlled Trial</td>\n",
       "      <td>Not specified</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>7 weeks</td>\n",
       "      <td>c8a58b3f7946892e1ec52bac322ad74763258b2b72ab69...</td>\n",
       "      <td>097f93c7cc27f257d4c04b3153765e6b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Study 4</td>\n",
       "      <td>Patients with type 2 diabetes and high cardiov...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>56.19</td>\n",
       "      <td>43.81</td>\n",
       "      <td>None</td>\n",
       "      <td>182.0</td>\n",
       "      <td>Randomized Controlled Trial</td>\n",
       "      <td>Not specified</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2 weeks</td>\n",
       "      <td>29ac0b01370f069064a1e3964872ef22f46d65bead46ca...</td>\n",
       "      <td>097f93c7cc27f257d4c04b3153765e6b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Current Trial</td>\n",
       "      <td>Diverse population with type 2 diabetes</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Comparative-effectiveness trial</td>\n",
       "      <td>None</td>\n",
       "      <td>[Participants with type 2 diabetes]</td>\n",
       "      <td>[Coexisting conditions]</td>\n",
       "      <td>Long duration</td>\n",
       "      <td>eb4b9743a7d94b8a88a04ffababde3f97ac8e55d312d4f...</td>\n",
       "      <td>09d8e609d9e0e28eb1dcb67351e66f83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>Study on Semaglutide and Sitagliptin</td>\n",
       "      <td>Patients in the study on semaglutide and sitag...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1864.0</td>\n",
       "      <td>Clinical Trial</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>f3268fb59f03256893d27ed1126b44ca79236535fd5d71...</td>\n",
       "      <td>6176451015999ec938bb4c38f53b667e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>Pioneer 7</td>\n",
       "      <td>Patients with T2DM</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>504.0</td>\n",
       "      <td>Randomized clinical trial</td>\n",
       "      <td>81 sites in 10 countries</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>681d016419a23edf1780963e4d677fd5393b4c9c9e0518...</td>\n",
       "      <td>621decedae713b6e90013ab0e4f33e42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>Study 2</td>\n",
       "      <td>Patients with established type 2 diabetes (T2D...</td>\n",
       "      <td>18.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>507.0</td>\n",
       "      <td>Randomized Controlled Trial</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>[Type 2 diabetes diagnosis, BMI ≥ 30 kg/m², Ag...</td>\n",
       "      <td>None</td>\n",
       "      <td>26 weeks</td>\n",
       "      <td>85ecc31e99d6f48c59d0e25332279838ecc36ba6e789b8...</td>\n",
       "      <td>62647d63971da0c08fc7aecca4780acb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>Ahren et al. (48)</td>\n",
       "      <td>Patients with T2DM on metformin monotherapy</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Randomized controlled trial</td>\n",
       "      <td>None</td>\n",
       "      <td>[T2DM on metformin monotherapy]</td>\n",
       "      <td>None</td>\n",
       "      <td>104 weeks</td>\n",
       "      <td>e94f139e4ac6c873d9ff2f00e598cd72e681f32b74a8c8...</td>\n",
       "      <td>62c8f7607d7c903b4179eaba82922516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>Study 1</td>\n",
       "      <td>Healthy volunteers undergoing hyperinsulinemic...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Clinical study</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>7015c58c7747213814ab16f260dc48e09fb1cfe8442f2a...</td>\n",
       "      <td>62d526b993ca2afd8f971bfcded16160</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>114 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               study_name  \\\n",
       "0                                 Study 1   \n",
       "1                                 Study 2   \n",
       "2                                 Study 3   \n",
       "3                                 Study 4   \n",
       "4                           Current Trial   \n",
       "..                                    ...   \n",
       "109  Study on Semaglutide and Sitagliptin   \n",
       "110                             Pioneer 7   \n",
       "111                               Study 2   \n",
       "112                     Ahren et al. (48)   \n",
       "113                               Study 1   \n",
       "\n",
       "                                           description min_age max_age  \\\n",
       "0    Patients with established atherosclerotic card...    None    None   \n",
       "1    Patients with type 2 diabetes and high cardiov...    None    None   \n",
       "2    Patients with type 2 diabetes and high cardiov...    None    None   \n",
       "3    Patients with type 2 diabetes and high cardiov...    None    None   \n",
       "4              Diverse population with type 2 diabetes    None    None   \n",
       "..                                                 ...     ...     ...   \n",
       "109  Patients in the study on semaglutide and sitag...    None    None   \n",
       "110                                 Patients with T2DM    None    None   \n",
       "111  Patients with established type 2 diabetes (T2D...    18.0    None   \n",
       "112        Patients with T2DM on metformin monotherapy    None    None   \n",
       "113  Healthy volunteers undergoing hyperinsulinemic...    None    None   \n",
       "\n",
       "    male_percentage female_percentage other_gender_percentage sample_size  \\\n",
       "0             56.19             43.81                    None       507.0   \n",
       "1             56.19             43.81                    None       129.0   \n",
       "2             56.19             43.81                    None       199.0   \n",
       "3             56.19             43.81                    None       182.0   \n",
       "4              None              None                    None        None   \n",
       "..              ...               ...                     ...         ...   \n",
       "109            None              None                    None      1864.0   \n",
       "110            None              None                    None       504.0   \n",
       "111            None              None                    None       507.0   \n",
       "112            None              None                    None        None   \n",
       "113            None              None                    None        None   \n",
       "\n",
       "                          study_type                  location  \\\n",
       "0        Randomized Controlled Trial             Not specified   \n",
       "1        Randomized Controlled Trial             Not specified   \n",
       "2        Randomized Controlled Trial             Not specified   \n",
       "3        Randomized Controlled Trial             Not specified   \n",
       "4    Comparative-effectiveness trial                      None   \n",
       "..                               ...                       ...   \n",
       "109                   Clinical Trial                      None   \n",
       "110        Randomized clinical trial  81 sites in 10 countries   \n",
       "111      Randomized Controlled Trial                   Unknown   \n",
       "112      Randomized controlled trial                      None   \n",
       "113                   Clinical study                      None   \n",
       "\n",
       "                                    inclusion_criteria  \\\n",
       "0                                                 None   \n",
       "1                                                 None   \n",
       "2                                                 None   \n",
       "3                                                 None   \n",
       "4                  [Participants with type 2 diabetes]   \n",
       "..                                                 ...   \n",
       "109                                               None   \n",
       "110                                               None   \n",
       "111  [Type 2 diabetes diagnosis, BMI ≥ 30 kg/m², Ag...   \n",
       "112                    [T2DM on metformin monotherapy]   \n",
       "113                                               None   \n",
       "\n",
       "          exclusion_criteria study_duration  \\\n",
       "0                       None        2 years   \n",
       "1                       None        5 weeks   \n",
       "2                       None        7 weeks   \n",
       "3                       None        2 weeks   \n",
       "4    [Coexisting conditions]  Long duration   \n",
       "..                       ...            ...   \n",
       "109                     None           None   \n",
       "110                     None           None   \n",
       "111                     None       26 weeks   \n",
       "112                     None      104 weeks   \n",
       "113                     None           None   \n",
       "\n",
       "                                   study_population_id  \\\n",
       "0    8e2f978b6e2de6b3a215313941adbd8fb5e42e5f92e9b9...   \n",
       "1    ebf71b271dc74f0d2faf079da304d66ce3ad27a6ae5dd4...   \n",
       "2    c8a58b3f7946892e1ec52bac322ad74763258b2b72ab69...   \n",
       "3    29ac0b01370f069064a1e3964872ef22f46d65bead46ca...   \n",
       "4    eb4b9743a7d94b8a88a04ffababde3f97ac8e55d312d4f...   \n",
       "..                                                 ...   \n",
       "109  f3268fb59f03256893d27ed1126b44ca79236535fd5d71...   \n",
       "110  681d016419a23edf1780963e4d677fd5393b4c9c9e0518...   \n",
       "111  85ecc31e99d6f48c59d0e25332279838ecc36ba6e789b8...   \n",
       "112  e94f139e4ac6c873d9ff2f00e598cd72e681f32b74a8c8...   \n",
       "113  7015c58c7747213814ab16f260dc48e09fb1cfe8442f2a...   \n",
       "\n",
       "                             chunk_id  \n",
       "0    097f93c7cc27f257d4c04b3153765e6b  \n",
       "1    097f93c7cc27f257d4c04b3153765e6b  \n",
       "2    097f93c7cc27f257d4c04b3153765e6b  \n",
       "3    097f93c7cc27f257d4c04b3153765e6b  \n",
       "4    09d8e609d9e0e28eb1dcb67351e66f83  \n",
       "..                                ...  \n",
       "109  6176451015999ec938bb4c38f53b667e  \n",
       "110  621decedae713b6e90013ab0e4f33e42  \n",
       "111  62647d63971da0c08fc7aecca4780acb  \n",
       "112  62c8f7607d7c903b4179eaba82922516  \n",
       "113  62d526b993ca2afd8f971bfcded16160  \n",
       "\n",
       "[114 rows x 15 columns]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ingest_records['nodes']['StudyPopulation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 229 medication nodes\n",
      "partition: 1\n",
      "{'labels_added': 74, 'nodes_created': 74, 'properties_set': 1219}\n",
      "Loading 69 medical_condition nodes\n",
      "partition: 1\n",
      "{'labels_added': 28, 'nodes_created': 28, 'properties_set': 373}\n",
      "Loading 105 study_medication nodes\n",
      "partition: 1\n",
      "{'labels_added': 94, 'nodes_created': 94, 'properties_set': 1039}\n",
      "Loading 114 study_population nodes\n",
      "partition: 1\n",
      "{'labels_added': 96, 'nodes_created': 96, 'properties_set': 1464}\n",
      "Loading 146 clinical_outcome nodes\n",
      "partition: 1\n",
      "{'labels_added': 141, 'nodes_created': 141, 'properties_set': 433}\n"
     ]
    }
   ],
   "source": [
    "load_entity_nodes(ingest_records[\"nodes\"][\"Medication\"], \n",
    "                  ingest_records[\"nodes\"][\"MedicalCondition\"], \n",
    "                  ingest_records[\"nodes\"][\"StudyMedication\"], \n",
    "                  ingest_records[\"nodes\"][\"StudyPopulation\"], \n",
    "                  ingest_records[\"nodes\"][\"ClinicalOutcome\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 77 study_medication_uses_medication relationships\n",
      "partition: 1\n",
      "{'relationships_created': 65}\n",
      "Loading 76 study_medication_produces_clinical_outcome relationships\n",
      "partition: 1\n",
      "{'relationships_created': 66}\n",
      "Loading 45 study_population_has_medical_condition relationships\n",
      "partition: 1\n",
      "{'relationships_created': 37}\n",
      "Loading 35 study_population_receives_study_medication relationships\n",
      "partition: 1\n",
      "{'relationships_created': 34}\n",
      "Loading 39 study_population_has_clinical_outcome relationships\n",
      "partition: 1\n",
      "{'relationships_created': 37}\n"
     ]
    }
   ],
   "source": [
    "load_entity_relationships(ingest_records[\"relationships\"][\"StudyMedicationUsesMedication\"], \n",
    "                          ingest_records[\"relationships\"][\"StudyMedicationProducesClinicalOutcome\"], \n",
    "                          ingest_records[\"relationships\"][\"StudyPopulationHasMedicalCondition\"], \n",
    "                          ingest_records[\"relationships\"][\"StudyPopulationReceivesStudyMedication\"], \n",
    "                          ingest_records[\"relationships\"][\"StudyPopulationHasClinicalOutcome\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linking 229 chunk_has_entity_medication entities to chunks\n",
      "partition: 1\n",
      "{'relationships_created': 229}\n",
      "Linking 69 chunk_has_entity_medical_condition entities to chunks\n",
      "partition: 1\n",
      "{'relationships_created': 69}\n",
      "Linking 105 chunk_has_entity_study_medication entities to chunks\n",
      "partition: 1\n",
      "{'relationships_created': 102}\n",
      "Linking 114 chunk_has_entity_study_population entities to chunks\n",
      "partition: 1\n",
      "{'relationships_created': 110}\n",
      "Linking 146 chunk_has_entity_clinical_outcome entities to chunks\n",
      "partition: 1\n",
      "{'relationships_created': 146}\n"
     ]
    }
   ],
   "source": [
    "link_entities_to_chunks(ingest_records[\"nodes\"][\"Medication\"], \n",
    "                        ingest_records[\"nodes\"][\"MedicalCondition\"], \n",
    "                        ingest_records[\"nodes\"][\"StudyMedication\"], \n",
    "                        ingest_records[\"nodes\"][\"StudyPopulation\"], \n",
    "                        ingest_records[\"nodes\"][\"ClinicalOutcome\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def link_domain_and_patient_journey_graph() -> None:\n",
    "    \"\"\"\n",
    "    Link the domain graph with the patient journey graph. \n",
    "    This process doesn't require any input DataFrames. \n",
    "    Instead it attempts to link nodes based on matching properties.\n",
    "    \"\"\"\n",
    "\n",
    "    queries = [\"demographic_in_study_population\"]\n",
    "\n",
    "    for q in queries:\n",
    "        res = graph.execute_write_query(database=db_info['database'], \n",
    "                                        query=relationship_load_queries[q])\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    }
   ],
   "source": [
    "link_domain_and_patient_journey_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
