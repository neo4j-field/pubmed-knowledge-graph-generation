{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PubMed Knowledge Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook walks through the process of generating a knowledge graph of PubMed articles.\n",
    "\n",
    "This notebook will\n",
    "* Download a selection of articles from PubMed\n",
    "* Define a knowledge graph schema\n",
    "* Extract entities from the articles according to the defined schema\n",
    "* Populate a Neo4j instance with articles and extracted entities\n",
    "* Connect extracted entities with existing patient journey data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook requires a local repo of articles. You may download a sample of 20 PubMed articles by running the following command.\n",
    "\n",
    "```bash\n",
    "python3 ./scripts/fetch_pubmed_articles.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter some Numpy warnings that pop up during ingestion\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import hashlib\n",
    "from typing import Any, Optional, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allows for async operations in notebooks\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexical Graph Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use Unstructured.IO to partition and chunk our articles. \n",
    "\n",
    "This process breaks the articles into sensible chunks that may be used as context in our application. \n",
    "\n",
    "These chunks will also have relationships to the extracted entities, but we will add these relationships later.\n",
    "\n",
    "The lexical graph will adhere to the structure defined in the ['Lexical Graph with Extracted Entities'](https://graphrag.com/reference/knowledge-graph/lexical-graph-extracted-entities/) section of [graphrag.com](graphrag.com).\n",
    "\n",
    "Here is the data model we will be using.\n",
    "\n",
    "\n",
    "<img src=\"./assets/images/lexical-data-model.png\" alt=\"lexical-data-model\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pydantic import BaseModel, Field, computed_field\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "\n",
    "from unstructured.documents.elements import CompositeElement\n",
    "\n",
    "from uuid import uuid4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------\n",
    "# Nodes\n",
    "# ------------\n",
    "\n",
    "class Document(BaseModel):\n",
    "    \"\"\"\n",
    "    A Document.\n",
    "    This is the top level node in our knowledge graph.\n",
    "    Documents are made of many Chunks.\n",
    "    \"\"\"\n",
    "    id: str = Field(..., description=\"The id of the document\")\n",
    "    name: str = Field(..., description=\"The name of the document\")\n",
    "    source: str = Field(..., description=\"The source of the document\")\n",
    "\n",
    "class Chunk(BaseModel):\n",
    "    \"\"\"\n",
    "    A Chunk.\n",
    "    This is a collection of `UnstructuredElements`.\n",
    "    Unstructured.IO represents Chunks as `CompositeElement` objects.\n",
    "    \"\"\"\n",
    "    id: str = Field(..., description=\"The id of the chunk\")\n",
    "    type: str = Field(..., description=\"The type of the chunk\")\n",
    "    text: str = Field(..., description=\"The text of the chunk\")\n",
    "\n",
    "class ChunkWithEmbedding(Chunk):\n",
    "    \"\"\"\n",
    "    A Chunk with an embedding.\n",
    "    This is used to represent chunks that have been embedded.\n",
    "    \"\"\"\n",
    "    embedding: list[float] = Field(..., description=\"The embedding of the chunk text field\")\n",
    "\n",
    "class UnstructuredElement(BaseModel):\n",
    "    \"\"\"\n",
    "    A base class for all unstructured elements. \n",
    "    These are the smallest units in our chunking process. \n",
    "    One or more of these elements are combined to form a Chunk.\n",
    "    \"\"\"\n",
    "    id: str = Field(..., description=\"The id of the element\")\n",
    "    text: str = Field(..., description=\"The text of the element\")\n",
    "    type: str = Field(..., description=\"The type of the element\")\n",
    "    page_number: int = Field(..., description=\"The page number of the element\")\n",
    "\n",
    "class TextElement(UnstructuredElement):\n",
    "    \"\"\"\n",
    "    A TextElement. Structurally identical to the UnstructuredElement class.\n",
    "    This is used to represent text elements that contain no tables or images.\n",
    "    \"\"\"\n",
    "\n",
    "class ImageElement(UnstructuredElement):\n",
    "    \"\"\"\n",
    "    An ImageElement.\n",
    "    \"\"\"\n",
    "    image_base64: str = Field(..., description=\"The base64 encoded image\")\n",
    "    image_mime_type: str = Field(..., description=\"The mime type of the image\")\n",
    "\n",
    "class TableElement(UnstructuredElement):\n",
    "    \"\"\"\n",
    "    A TableElement. \n",
    "    This may also have image features and so it inherits from ImageElement.\n",
    "    \"\"\"\n",
    "    image_base64: str | None = Field(None, description=\"The base64 encoded table\")\n",
    "    image_mime_type: str | None = Field(None, description=\"The mime type of the table\")\n",
    "    text_as_html: str | None = Field(None, description=\"The text of the table as HTML\")\n",
    "    \n",
    "# -------------\n",
    "# Relationships\n",
    "# -------------\n",
    "\n",
    "class ChunkPartOfDocument(BaseModel):\n",
    "    \"\"\"\n",
    "    (:Chunk {id: $chunk_id})-[:PART_OF_DOCUMENT]->(:Document {id: $document_id})\n",
    "    \"\"\"\n",
    "    chunk_id: str = Field(..., description=\"The id of the chunk\")\n",
    "    document_id: str = Field(..., description=\"The id of the document\")\n",
    "\n",
    "class UnstructuredElementPartOfChunk(BaseModel):\n",
    "    \"\"\"\n",
    "    (:UnstructuredElement {id: $unstructured_element_id})-[:PART_OF_CHUNK]->(:Chunk {id: $chunk_id})\n",
    "\n",
    "    This covers TextElement, ImageElement and TableElement nodes since they all share the UnstructuredElement label.\n",
    "    \"\"\" \n",
    "    unstructured_element_id: str = Field(..., description=\"The id of the unstructured element\")\n",
    "    chunk_id: str = Field(..., description=\"The id of the chunk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_article_file_name(file_name: str) -> tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Parse the article file name and return the PubMed ID and title.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_name : str\n",
    "        The name of the file to parse.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple[str, str]\n",
    "        The PubMed ID and title of the article.\n",
    "    \"\"\"\n",
    "    doc_id, title = file_name.split(\"-\", 1)\n",
    "    title = title.replace(\"_\", \" \")\n",
    "\n",
    "    return doc_id, title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chunk_has_next_chunk_relationship_dataframe(chunk_dataframe: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create the DataFrame for loading (:Chunk)-[:HAS_NEXT_CHUNK]->(:Chunk) relationships.\n",
    "    \"\"\"\n",
    "    df = chunk_dataframe.copy()\n",
    "    df['next_id'] = df['id'].shift(-1)\n",
    "    df.dropna(inplace=True)\n",
    "    res = df[['id', 'next_id']].rename({\"id\": \"source_id\", \"next_id\": \"target_id\"}, axis=1)\n",
    "    return res\n",
    "\n",
    "def extract_document_title(text_elements_dataframe: pd.DataFrame) -> str:\n",
    "    \"\"\"\n",
    "    Extract the title of the document from the text elements.\n",
    "    Here we assume that the first 'Title' element is the title of the document.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        The title of the document.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return text_elements_dataframe[text_elements_dataframe['type'] == 'Title'].iloc[0]['text']\n",
    "    except Exception as e:\n",
    "        print(f\"Unable to extract document title: {e}\")\n",
    "        return 'unknown title'\n",
    "\n",
    "def parse_node_and_relationship_from_composite_element(composite_element: CompositeElement, parent_document_id: str) -> dict[str, dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Parse the nodes and relationships for a given chunk (CompositeElement). \n",
    "    This will find the following nodes:\n",
    "    * Chunk\n",
    "    * TextElement\n",
    "    * ImageElement\n",
    "    * TableElement\n",
    "    * UnstructuredElement (Shared label for TextElement, ImageElement and TableElement)\n",
    "\n",
    "    And the following relationships:\n",
    "    * (:Chunk)-[:PART_OF_DOCUMENT]->(:Document)\n",
    "    * (:UnstructuredElement)-[:PART_OF_CHUNK]->(:Chunk)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict[str, dict[str, Any]]\n",
    "        A dictionary containing a list of records for each node and relationship type.\n",
    "    \"\"\"\n",
    "    chunk = Chunk(id=composite_element.id, text=composite_element.text, type=composite_element.category)\n",
    "    chunk_part_of_document = ChunkPartOfDocument(chunk_id=chunk.id, document_id=parent_document_id)\n",
    "\n",
    "    text_elements: list[TextElement] = list()\n",
    "    image_elements: list[ImageElement] = list()\n",
    "    table_elements: list[TableElement] = list()\n",
    "    unstructured_element_part_of_chunk: list[UnstructuredElementPartOfChunk] = list()\n",
    "\n",
    "    # Chunks (CompositeElements) are made of many smaller text chunks (UnstructuredElements)\n",
    "    # We can parse what type of elements these subchunks are and load them as well\n",
    "    # This will give us access to images and tables from the document\n",
    "    for element in composite_element.metadata.orig_elements:\n",
    "        match element.category:\n",
    "            case \"NarrativeText\":\n",
    "                text_elements.append(TextElement(id=element.id, \n",
    "                                                 text=element.text, \n",
    "                                                 type=element.category, \n",
    "                                                 page_number=element.metadata.page_number))\n",
    "            case \"Image\":\n",
    "                image_elements.append(ImageElement(id=element.id, \n",
    "                                                   text=element.text,\n",
    "                                                   type=element.category, \n",
    "                                                   page_number=element.metadata.page_number,\n",
    "                                                   image_base64=element.metadata.image_base64, \n",
    "                                                   image_mime_type=element.metadata.image_mime_type))\n",
    "            case \"Table\":\n",
    "                table_elements.append(TableElement(id=element.id, \n",
    "                                                   text=element.text,\n",
    "                                                   type=element.category, \n",
    "                                                   page_number=element.metadata.page_number,\n",
    "                                                   image_base_64=element.metadata.image_base64,\n",
    "                                                   image_mime_type=element.metadata.image_mime_type,\n",
    "                                                   text_as_html=element.metadata.text_as_html))\n",
    "            # Assume some kind of text element if we can't match the category\n",
    "            # Could be headers, figure captions, etc\n",
    "            case _:\n",
    "                try:\n",
    "                    text_elements.append(TextElement(id=element.id, \n",
    "                                                 text=element.text, \n",
    "                                                 type=element.category, \n",
    "                                                 page_number=element.metadata.page_number))\n",
    "                except Exception as e:\n",
    "                    print(f\"Error parsing text element: {e}\")\n",
    "\n",
    "        unstructured_element_part_of_chunk.append(UnstructuredElementPartOfChunk(unstructured_element_id=element.id, chunk_id=chunk.id))\n",
    "\n",
    "    # we return a list of records for each entity and relationship instead of the Pydantic classes\n",
    "    return {\n",
    "        \"nodes\": {\n",
    "            \"chunk\": [chunk.model_dump()],\n",
    "            \"text_element\": [el.model_dump() for el in text_elements],\n",
    "            \"image_element\": [el.model_dump() for el in image_elements],\n",
    "            \"table_element\": [el.model_dump() for el in table_elements],\n",
    "        },\n",
    "        \"relationships\": {\n",
    "            \"chunk_part_of_document\": [chunk_part_of_document.model_dump()],\n",
    "            \"unstructured_element_part_of_chunk\": [rel.model_dump() for rel in unstructured_element_part_of_chunk],\n",
    "        }\n",
    "    }\n",
    "\n",
    "def parse_nodes_and_relationships_from_composite_elements(composite_elements: list[CompositeElement], parent_doc_id: str) -> dict[str, dict[str, pd.DataFrame]]:\n",
    "    \"\"\"Parse entity nodes and document relationships for a set of chunks (CompositeElements) and their parent document\"\"\"\n",
    "    \n",
    "    data = {\n",
    "        \"nodes\": {\n",
    "            \"document\": list(),\n",
    "            \"chunk\": list(),\n",
    "            \"text_element\": list(),\n",
    "            \"image_element\": list(),\n",
    "            \"table_element\": list(),\n",
    "        },\n",
    "        \"relationships\": {\n",
    "            \"chunk_part_of_document\": list(),\n",
    "            \"unstructured_element_part_of_chunk\": list(),\n",
    "            \"chunk_has_next_chunk\": list()\n",
    "        }\n",
    "    }\n",
    "\n",
    "    for composite_element in composite_elements:\n",
    "        new_data = parse_node_and_relationship_from_composite_element(composite_element, parent_doc_id)\n",
    "\n",
    "        # update the records with new nodes and relationships\n",
    "        data[\"nodes\"][\"chunk\"].extend(new_data[\"nodes\"][\"chunk\"])\n",
    "        data[\"relationships\"][\"chunk_part_of_document\"].extend(new_data[\"relationships\"][\"chunk_part_of_document\"])\n",
    "        data[\"nodes\"][\"text_element\"].extend(new_data[\"nodes\"][\"text_element\"])\n",
    "        data[\"nodes\"][\"image_element\"].extend(new_data[\"nodes\"][\"image_element\"])\n",
    "        data[\"nodes\"][\"table_element\"].extend(new_data[\"nodes\"][\"table_element\"])\n",
    "        data[\"relationships\"][\"unstructured_element_part_of_chunk\"].extend(new_data[\"relationships\"][\"unstructured_element_part_of_chunk\"])\n",
    "\n",
    "    # convert to pandas dataframe for ingestion\n",
    "    # node DataFrames\n",
    "    data[\"nodes\"][\"chunk\"] = pd.DataFrame(data[\"nodes\"][\"chunk\"])\n",
    "    data[\"nodes\"][\"text_element\"] = pd.DataFrame(data[\"nodes\"][\"text_element\"])\n",
    "    data[\"nodes\"][\"image_element\"] = pd.DataFrame(data[\"nodes\"][\"image_element\"])\n",
    "    data[\"nodes\"][\"table_element\"] = pd.DataFrame(data[\"nodes\"][\"table_element\"])\n",
    "\n",
    "    document_title = extract_document_title(data[\"nodes\"][\"text_element\"])\n",
    "    data[\"nodes\"][\"document\"] = pd.DataFrame([Document(id=parent_doc_id, name=document_title, source=\"pubmed\").model_dump()])\n",
    "\n",
    "    # relationship DataFrames\n",
    "    data[\"relationships\"][\"chunk_part_of_document\"] = pd.DataFrame(data[\"relationships\"][\"chunk_part_of_document\"])\n",
    "    data[\"relationships\"][\"unstructured_element_part_of_chunk\"] = pd.DataFrame(data[\"relationships\"][\"unstructured_element_part_of_chunk\"])\n",
    "    data[\"relationships\"][\"chunk_has_next_chunk\"] = create_chunk_has_next_chunk_relationship_dataframe(data[\"nodes\"][\"chunk\"])\n",
    "\n",
    "    return data\n",
    "\n",
    "# def process_xml_article(file_name: str) -> dict[str, dict[str, pd.DataFrame]]:\n",
    "#     \"\"\"\n",
    "#     Process an article and return the nodes and relationships for ingestion into the knowledge graph.\n",
    "#     Assumes that \n",
    "#     * the article name follows the format \"{pmid}-{title}.xml\"\n",
    "#     * the article is stored in the \"articles/\" directory\n",
    "\n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     file_name : str\n",
    "#         The name of the file to process.\n",
    "\n",
    "#     Returns\n",
    "#     -------\n",
    "#     dict[str, dict[str, pd.DataFrame]]\n",
    "#         A dictionary containing the node and relationship Pandas DataFrames for ingestion into the knowledge graph.\n",
    "#     \"\"\"\n",
    "#     doc_id, title = parse_article_file_name(file_name)\n",
    "#     parent_document = Document(id=str(uuid4()), pm_id=doc_id, name=title, source=\"pubmed\")\n",
    "#     partitioned_doc = partition_xml(\"articles/\" + file_name, \n",
    "#                                     xml_keep_tags=False, \n",
    "#                                     chunking_strategy=\"by_title\", \n",
    "#                                     combine_text_under_n_chars=200, \n",
    "#                                     max_characters=500, \n",
    "#                                     multipage_sections=True)\n",
    "#     # return partitioned_doc\n",
    "#     return parse_nodes_and_relationships_from_chunk_elements(partitioned_doc, parent_document)\n",
    "\n",
    "def process_pdf_article(file_name: str) -> dict[str, dict[str, pd.DataFrame]]:\n",
    "    \"\"\"\n",
    "    Process an article and return the nodes and relationships for ingestion into the knowledge graph.\n",
    "    Assumes that \n",
    "    * the article name follows the format \"{pmid}-{title}.pdf\"\n",
    "    * the article is stored in the \"articles/\" directory\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_name : str\n",
    "        The name of the file to process.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict[str, dict[str, pd.DataFrame]]\n",
    "        A dictionary containing the node and relationship Pandas DataFrames for ingestion into the knowledge graph.\n",
    "    \"\"\"\n",
    "    # doc_id, title = parse_article_file_name(file_name)\n",
    "    doc_id = hashlib.sha256(file_name.encode()).hexdigest()\n",
    "    # parent_document = Document(id=str(uuid4()), pm_id=file_name, name=file_name, source=\"pubmed\")\n",
    "    partitioned_doc = partition_pdf(\"articles/pdf/\" + file_name, \n",
    "                                    strategy=\"hi_res\",                                     \n",
    "                                    extract_images_in_pdf=True,\n",
    "                                    extract_image_block_types=[\"Image\", \"Table\"], \n",
    "                                    extract_image_block_to_payload=True,               \n",
    "                                    # extract_image_block_output_dir=f\"figures/{file_name[:-4]}\",\n",
    "                                    chunking_strategy=\"by_title\", \n",
    "                                    combine_text_under_n_chars=200, \n",
    "                                    max_characters=1000, \n",
    "                                    multipage_sections=True)\n",
    "    # return partitioned_doc\n",
    "    return parse_nodes_and_relationships_from_composite_elements(partitioned_doc, doc_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdf_articles(article_file_names: list[str]) -> dict[str, dict[str, pd.DataFrame]]:\n",
    "    \"\"\"\n",
    "    Process a list of articles and return the nodes and relationships for ingestion into the knowledge graph.\n",
    "    Assumes that the articles are stored in the \"articles/pdf/\" directory\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    article_file_names : list[str]\n",
    "        A list of the names of the files to process.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict[str, dict[str, pd.DataFrame]]\n",
    "        A dictionary containing the node and relationship Pandas DataFrames for ingestion into the knowledge graph.\n",
    "    \"\"\"\n",
    "\n",
    "    # initialize the DataFrames\n",
    "    data = {\n",
    "        \"nodes\": {\n",
    "            \"document\": pd.DataFrame(),\n",
    "            \"chunk\": pd.DataFrame(),\n",
    "            \"text_element\": pd.DataFrame(),\n",
    "            \"image_element\": pd.DataFrame(),\n",
    "            \"table_element\": pd.DataFrame(),\n",
    "        },\n",
    "        \"relationships\": {\n",
    "            \"chunk_part_of_document\": pd.DataFrame(),\n",
    "            \"unstructured_element_part_of_chunk\": pd.DataFrame(),\n",
    "            \"chunk_has_next_chunk\": pd.DataFrame()\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # process each article individually\n",
    "    # this will\n",
    "    # * partition the article into chunks using Unstructured.IO\n",
    "    # * Identify TextElements, ImageElements, and TableElements in each chunk\n",
    "    # * Create DataFrames for all lexical nodes and relationships found in the article\n",
    "    # * Update the global DataFrames with the new article data\n",
    "    for file_name in article_file_names:\n",
    "        print(f\"Processing article: {file_name}\")\n",
    "        # process each article \n",
    "        article_data = process_pdf_article(file_name)\n",
    "\n",
    "        # update the DataFrames with the new article data\n",
    "        data[\"nodes\"][\"document\"] = pd.concat([data[\"nodes\"][\"document\"], article_data[\"nodes\"][\"document\"]], ignore_index=True)\n",
    "        data[\"nodes\"][\"chunk\"] = pd.concat([data[\"nodes\"][\"chunk\"], article_data[\"nodes\"][\"chunk\"]], ignore_index=True)\n",
    "        data[\"nodes\"][\"text_element\"] = pd.concat([data[\"nodes\"][\"text_element\"], article_data[\"nodes\"][\"text_element\"]], ignore_index=True)\n",
    "        data[\"nodes\"][\"image_element\"] = pd.concat([data[\"nodes\"][\"image_element\"], article_data[\"nodes\"][\"image_element\"]], ignore_index=True)\n",
    "        data[\"nodes\"][\"table_element\"] = pd.concat([data[\"nodes\"][\"table_element\"], article_data[\"nodes\"][\"table_element\"]], ignore_index=True)\n",
    "        data[\"relationships\"][\"chunk_part_of_document\"] = pd.concat([data[\"relationships\"][\"chunk_part_of_document\"], article_data[\"relationships\"][\"chunk_part_of_document\"]], ignore_index=True)\n",
    "        data[\"relationships\"][\"unstructured_element_part_of_chunk\"] = pd.concat([data[\"relationships\"][\"unstructured_element_part_of_chunk\"], article_data[\"relationships\"][\"unstructured_element_part_of_chunk\"]], ignore_index=True)\n",
    "        data[\"relationships\"][\"chunk_has_next_chunk\"] = pd.concat([data[\"relationships\"][\"chunk_has_next_chunk\"], article_data[\"relationships\"][\"chunk_has_next_chunk\"]], ignore_index=True)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexical Graph Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will embed the text fields of our lexical graph for vector similarity search. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Domain Graph Schema Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to define our knowledge graph schema. This information will be passed to the entity extraction LLM to control which entities and relationships are pulled out of the text.\n",
    "\n",
    "This is necessary to prevent our schema from growing too large with an unbounded extraction process.\n",
    "\n",
    "We are using Pydantic to define the schema here since it can be used to validate any returned results as well. This ensures that all data we are ingesting into Neo4j adheres to this structure.\n",
    "\n",
    "Here is what our domain graph data model looks like.\n",
    "\n",
    "<img src=\"./assets/images/domain-data-model-v1.png\" alt=\"domain-data-model\" width=\"600px\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Medication(BaseModel):\n",
    "    \"\"\"\n",
    "    A substance used for medical treatment, especially a medicine or drug. \n",
    "    This is a general representation of a medication. \n",
    "    A Medication node may have relationships to StudyMedication nodes that are specific to a particular study.\n",
    "    \"\"\"\n",
    "    \n",
    "    name: str = Field(..., description=\"Name of the medication. Should also be uniquely identifiable.\")\n",
    "    medication_class: str = Field(..., description=\"Drug class (e.g., GLP-1 RA, SGLT2i)\")\n",
    "    mechanism: Optional[str] = Field(None, description=\"Mechanism of action\")\n",
    "    generic_name: Optional[str] = Field(None, description=\"Generic name if different from name\")\n",
    "    brand_names: Optional[List[str]] = Field(None, description=\"Commercial brand names\")\n",
    "    approval_status: Optional[str] = Field(None, description=\"FDA approval status\")\n",
    "    \n",
    "    class Config:\n",
    "        json_schema_extra = {\n",
    "            \"examples\": [\n",
    "                {\n",
    "                    \"name\": \"Semaglutide\", \n",
    "                    \"medication_class\": \"GLP-1 receptor agonist\",\n",
    "                    \"mechanism\": \"GLP-1 receptor activation\",\n",
    "                    \"generic_name\": \"semaglutide\",\n",
    "                    \"brand_names\": [\"Ozempic\", \"Wegovy\", \"Rybelsus\"],\n",
    "                    \"approval_status\": \"FDA approved\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "\n",
    "\n",
    "class StudyMedication(BaseModel):\n",
    "    \"\"\"\n",
    "    Study-specific medication usage - how a medication was used in a particular study\n",
    "    This is an instance of a medication that is used in a particular study. \n",
    "    A StudyMedication node should also have a relationship with a Medication node.\n",
    "    \"\"\"\n",
    "    study_name: str = Field(..., description=\"Name of the study. This is used to uniquely identify the StudyMedication node.\")\n",
    "    treatment_arm: str = Field(..., description=\"Treatment arm of the study medication. This uniquely identifies the StudyMedication node.\")\n",
    "    dosage: Optional[str] = Field(None, description=\"Dosage used in this study\")\n",
    "    route: Optional[str] = Field(None, description=\"Route of administration\")\n",
    "    frequency: Optional[str] = Field(None, description=\"Dosing frequency\")\n",
    "    treatment_duration: Optional[str] = Field(None, description=\"Duration of treatment\")\n",
    "    comparator: Optional[str] = Field(None, description=\"What this was compared against\")\n",
    "    adherence_rate: Optional[float] = Field(None, description=\"Treatment adherence rate\")\n",
    "    formulation: Optional[str] = Field(None, description=\"Specific formulation used\")\n",
    "\n",
    "    @computed_field(return_type=str)\n",
    "    def study_medication_id(self) -> str:\n",
    "        \"\"\"\n",
    "        The unique id of the study medication.\n",
    "        This is a sha256 hash of the study name and treatment arm.\n",
    "        \"\"\"\n",
    "        return hashlib.sha256(f\"{self.study_name}_{self.treatment_arm}\".encode()).hexdigest()\n",
    "    \n",
    "    class Config:\n",
    "        json_schema_extra = {\n",
    "            # don't include the study_medication_id in the example since this is computed from extracted fields\n",
    "            \"examples\": [\n",
    "                {\n",
    "                    \"study_name\": \"Study 1\",\n",
    "                    \"treatment_arm\": \"Treatment arm 1\",\n",
    "                    \"dosage\": \"1.0 mg\",\n",
    "                    \"route\": \"subcutaneous\",\n",
    "                    \"frequency\": \"weekly\",\n",
    "                    \"treatment_duration\": \"12 weeks\",\n",
    "                    \"comparator\": \"placebo\",\n",
    "                    \"adherence_rate\": 85.5,\n",
    "                    \"formulation\": \"pre-filled pen\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "\n",
    "\n",
    "class ClinicalOutcome(BaseModel):\n",
    "    \"\"\"\n",
    "    Measured clinical outcomes and biomarkers.\n",
    "    This node represents a clinical outcome present in a study.\n",
    "    ClinicalOutcome nodes should have relationships with other entity nodes from a study.\n",
    "    ClinicalOutcome nodes should not have relationships with entities that exist outside the study.\n",
    "    \"\"\"\n",
    "    \n",
    "    category: str = Field(..., description=\"Category of outcome\")\n",
    "    description: str = Field(..., description=\"Description of the outcome\")\n",
    "    measurement_unit: Optional[str] = Field(None, description=\"Unit of measurement\")\n",
    "    normal_range: Optional[str] = Field(None, description=\"Normal or target range when applicable\")\n",
    "    baseline_value: Optional[float] = Field(None, description=\"Baseline measurement value\")\n",
    "    post_treatment_value: Optional[float] = Field(None, description=\"Post-treatment measurement value\")\n",
    "    change_from_baseline: Optional[float] = Field(None, description=\"Change from baseline\")\n",
    "    p_value: Optional[float] = Field(None, description=\"Statistical significance if reported\")\n",
    "    confidence_interval: Optional[str] = Field(None, description=\"95% confidence interval\")\n",
    "    effect_size: Optional[float] = Field(None, description=\"Standardized effect size\")\n",
    "\n",
    "    @computed_field(return_type=str)\n",
    "    def clinical_outcome_id(self) -> str:\n",
    "        \"\"\"\n",
    "        The unique id of the clinical outcome.\n",
    "        This is a sha256 hash of the combination of the category and the description of the outcome.\n",
    "        \"\"\"\n",
    "        return hashlib.sha256(f\"{self.category}_{self.description}\".encode()).hexdigest()\n",
    "    \n",
    "    class Config:\n",
    "        json_schema_extra = {\n",
    "            \"examples\": [\n",
    "                # don't include the clinical_outcome_id in the example since this is computed from extracted fields\n",
    "                {\n",
    "                    \"description\": \"This is a description of the clinical outcome\",\n",
    "                    \"category\": \"Glycemic control\",\n",
    "                    \"measurement_unit\": \"%\",\n",
    "                    \"normal_range\": \"<7.0%\",\n",
    "                    \"baseline_value\": 8.5,\n",
    "                    \"post_treatment_value\": 7.2,\n",
    "                    \"change_from_baseline\": -1.3,\n",
    "                    \"p_value\": 0.001,\n",
    "                    \"confidence_interval\": \"[-1.8, -0.8]\",\n",
    "                    \"effect_size\": -0.8\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "\n",
    "\n",
    "class MedicalCondition(BaseModel):\n",
    "    \"\"\"Medical conditions and comorbidities studied\"\"\"\n",
    "    \n",
    "    name: str = Field(..., description=\"Name of the medical condition\")\n",
    "    category: str = Field(..., description=\"Category of condition\")\n",
    "    severity: Optional[str] = Field(None, description=\"Severity or stage when specified\")\n",
    "    icd10_code: Optional[str] = Field(None, description=\"ICD-10 code when available\")\n",
    "    duration: Optional[str] = Field(None, description=\"Duration of condition if specified\")\n",
    "    prevalence: Optional[float] = Field(None, description=\"Prevalence in study population\")\n",
    "    \n",
    "    class Config:\n",
    "        json_schema_extra = {\n",
    "            \"examples\": [\n",
    "                {\n",
    "                    \"name\": \"Type 2 diabetes mellitus\",\n",
    "                    \"category\": \"Primary condition\", \n",
    "                    \"severity\": \"moderate\",\n",
    "                    \"icd10_code\": \"E11\",\n",
    "                    \"duration\": \"5-10 years\",\n",
    "                    \"prevalence\": 100.0\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "\n",
    "\n",
    "class StudyPopulation(BaseModel):\n",
    "    \"\"\"Patient populations and demographics in research studies\"\"\"\n",
    "    \n",
    "    study_population_id: str = Field(..., description=\"Unique identifier for the population\")\n",
    "    description: str = Field(..., description=\"Description of the population\")\n",
    "    age_range: Optional[str] = Field(None, description=\"Age range\")\n",
    "    mean_age: Optional[float] = Field(None, description=\"Mean age in years\")\n",
    "    male_percentage: Optional[float] = Field(None, description=\"Percentage of male gender participants\")\n",
    "    female_percentage: Optional[float] = Field(None, description=\"Percentage of female gender participants\")\n",
    "    other_gender_percentage: Optional[float] = Field(None, description=\"Percentage of participants that identify as another gender\")\n",
    "    sample_size: Optional[int] = Field(None, description=\"Number of participants\")\n",
    "    study_type: str = Field(..., description=\"Type of study\")\n",
    "    location: Optional[str] = Field(None, description=\"Geographic location of study\")\n",
    "    inclusion_criteria: Optional[List[str]] = Field(None, description=\"Key inclusion criteria\")\n",
    "    exclusion_criteria: Optional[List[str]] = Field(None, description=\"Key exclusion criteria\")\n",
    "    study_duration: Optional[str] = Field(None, description=\"Duration of study\")\n",
    "    \n",
    "    class Config:\n",
    "        json_schema_extra = {\n",
    "            \"examples\": [\n",
    "                {\n",
    "                    \"study_population_id\": \"POP001\",\n",
    "                    \"description\": \"Adults with T2DM and schizophrenia\",\n",
    "                    \"age_range\": \"18-65 years\",\n",
    "                    \"mean_age\": 43.8,\n",
    "                    \"female_percentage\": 47.0,\n",
    "                    \"male_percentage\": 53.0,\n",
    "                    \"sample_size\": 354,\n",
    "                    \"study_type\": \"Observational study\",\n",
    "                    \"location\": \"Denmark\",\n",
    "                    \"inclusion_criteria\": [\"Type 2 diabetes diagnosis\", \"Schizophrenia diagnosis\", \"Age ≥18\"],\n",
    "                    \"study_duration\": \"12 months\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "\n",
    "\n",
    "# Relationship classes\n",
    "class StudyMedicationUsesMedication(BaseModel):\n",
    "    \"\"\"\n",
    "    Links StudyMedication to Medication nodes.\n",
    "    StudyMedication nodes should have a relationship with a Medication node.\n",
    "    Pattern: (:StudyMedication)-[:USES_MEDICATION]->(:Medication)\n",
    "    \"\"\"\n",
    "    medication_name: str\n",
    "    study_medication_study_name: str = Field(..., description=\"Name of the study. This is used to uniquely identify the StudyMedication node.\")\n",
    "    study_medication_treatment_arm: str = Field(..., description=\"Treatment arm of the study medication. This uniquely identifies the StudyMedication node.\")\n",
    "\n",
    "    @computed_field(return_type=str)\n",
    "    def study_medication_id(self) -> str:\n",
    "        \"\"\"\n",
    "        The unique id of the study medication.\n",
    "        This is a sha256 hash of the study name and treatment arm.\n",
    "        \"\"\"\n",
    "        return hashlib.sha256(f\"{self.study_medication_study_name}_{self.study_medication_treatment_arm}\".encode()).hexdigest()\n",
    "\n",
    "\n",
    "class StudyMedicationProducesClinicalOutcome(BaseModel):\n",
    "    \"\"\"\n",
    "    Links StudyMedication to ClinicalOutcome nodes.\n",
    "    StudyMedication nodes should have a relationship with a ClinicalOutcome node.\n",
    "    Pattern: (:StudyMedication)-[:PRODUCES_CLINICAL_OUTCOME]->(:ClinicalOutcome)\n",
    "    \"\"\"\n",
    "    study_medication_study_name: str = Field(..., description=\"Name of the study. This is used to uniquely identify the StudyMedication node.\")\n",
    "    study_medication_treatment_arm: str = Field(..., description=\"Treatment arm of the study medication. This uniquely identifies the StudyMedication node.\")\n",
    "    clinical_outcome_category: str = Field(..., description=\"Category of outcome\")\n",
    "    clinical_outcome_description: str = Field(..., description=\"Description of the outcome\")\n",
    "\n",
    "    @computed_field(return_type=str)\n",
    "    def clinical_outcome_id(self) -> str:\n",
    "        \"\"\"\n",
    "        The unique id of the clinical outcome.\n",
    "        This is a sha256 hash of the combination of the category and the description of the outcome.\n",
    "        \"\"\"\n",
    "        return hashlib.sha256(f\"{self.clinical_outcome_category}_{self.clinical_outcome_description}\".encode()).hexdigest()\n",
    "    \n",
    "    @computed_field(return_type=str)\n",
    "    def study_medication_id(self) -> str:\n",
    "        \"\"\"\n",
    "        The unique id of the study medication.\n",
    "        This is a sha256 hash of the study name and treatment arm.\n",
    "        \"\"\"\n",
    "        return hashlib.sha256(f\"{self.study_medication_study_name}_{self.study_medication_treatment_arm}\".encode()).hexdigest()\n",
    "\n",
    "\n",
    "class StudyPopulationHasMedicalCondition(BaseModel):\n",
    "    \"\"\"\n",
    "    Links StudyPopulation to MedicalCondition nodes.\n",
    "    StudyPopulation nodes should have a relationship with a MedicalCondition node.\n",
    "    Pattern: (:StudyPopulation)-[:HAS_MEDICAL_CONDITION]->(:MedicalCondition)\n",
    "    \"\"\"\n",
    "    study_population_id: str\n",
    "    medical_condition_name: str\n",
    "\n",
    "\n",
    "class StudyPopulationReceivesStudyMedication(BaseModel):\n",
    "    \"\"\"\n",
    "    Links StudyPopulation to StudyMedication nodes.\n",
    "    StudyPopulation nodes should have a relationship with a StudyMedication node.\n",
    "    Pattern: (:StudyPopulation)-[:RECEIVES_STUDY_MEDICATION]->(:StudyMedication)\n",
    "    \"\"\"\n",
    "    study_population_id: str\n",
    "    study_medication_study_name: str = Field(..., description=\"Name of the study. This is used to uniquely identify the StudyMedication node.\")\n",
    "    study_medication_treatment_arm: str = Field(..., description=\"Treatment arm of the study medication. This uniquely identifies the StudyMedication node.\")\n",
    "\n",
    "    @computed_field(return_type=str)\n",
    "    def study_medication_id(self) -> str:\n",
    "        \"\"\"\n",
    "        The unique id of the study medication.\n",
    "        This is a sha256 hash of the study name and treatment arm.\n",
    "        \"\"\"\n",
    "        return hashlib.sha256(f\"{self.study_medication_study_name}_{self.study_medication_treatment_arm}\".encode()).hexdigest()\n",
    "\n",
    "\n",
    "class StudyPopulationHasClinicalOutcome(BaseModel):\n",
    "    \"\"\"\n",
    "    Links StudyPopulation to ClinicalOutcome nodes.\n",
    "    StudyPopulation nodes should have a relationship with a ClinicalOutcome node.\n",
    "    Pattern: (:StudyPopulation)-[:HAS_CLINICAL_OUTCOME]->(:ClinicalOutcome)\n",
    "    \"\"\"\n",
    "    study_population_id: str\n",
    "    clinical_outcome_id: str\n",
    "    category: str = Field(..., description=\"Category of outcome\")\n",
    "    description: str = Field(..., description=\"Description of the outcome\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lexical and domain knowledge graphs will be linked with `HAS_ENTITY` relationships between Chunk nodes and domain graph nodes.\n",
    "\n",
    "This is the combined lexical and domain graph data model.\n",
    "\n",
    "IMAGE OF DATA MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity Extraction via LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using [OpenAI](https://platform.openai.com/docs/overview) and the [Instructor](https://python.useinstructor.com/) library to perform our entity extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AsyncOpenAI\n",
    "import instructor\n",
    "from instructor.exceptions import IncompleteOutputException, InstructorRetryException, ValidationError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructor handles requesting structured outputs from the LLM. \n",
    "\n",
    "If the LLM fails to return output that adheres to the response models, Instructor will also handle the retry logic and pass any errors to inform corrections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = instructor.from_openai(AsyncOpenAI())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the system prompt defines the overall behavior of the LLM\n",
    "system_prompt = \"\"\"\n",
    "You are a healthcare research expert that is responsible for extracting detailed entities from PubMed articles. \n",
    "You will be provided a graph data model schema and must extract entities and relationships to populate a knowledge graph.\n",
    "\"\"\"\n",
    "\n",
    "async def extract_entities_from_text_chunk(text_chunk: str) -> list:\n",
    "    \"\"\"\n",
    "    Extract entities and relationships from a text chunk.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text_chunk : str\n",
    "        The text chunk to extract entities from.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list[Medication | StudyMedication | ClinicalOutcome | StudyMedicationUsesMedication | StudyMedicationProducesClinicalOutcome],\n",
    "        A list of entities and relationships extracted from the text chunk.\n",
    "        If the response is truncated, an empty list is returned.\n",
    "        If retries are exhausted, an empty list is returned.\n",
    "        If the response is invalid, an empty list is returned.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = await client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": text_chunk}\n",
    "            ],\n",
    "            response_model=list[Medication | StudyMedication | ClinicalOutcome | StudyMedicationUsesMedication | StudyMedicationProducesClinicalOutcome],\n",
    "            temperature=0.0\n",
    "        )\n",
    "        return response\n",
    "    except IncompleteOutputException as e:\n",
    "        # Handle truncated output\n",
    "        print(f\"Response output truncated\")\n",
    "        return list()\n",
    "    except InstructorRetryException as e:\n",
    "        # Handle retry exhaustion\n",
    "        print(f\"Failed after {e.n_attempts} attempts\")\n",
    "        return list()\n",
    "    except ValidationError as e:\n",
    "        # Handle validation errors\n",
    "        print(f\"Validation failed: {e}\")\n",
    "        return list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def extract_entities_from_chunk_nodes(chunk_nodes_dataframe: pd.DataFrame, batch_size: int = 100) -> list[tuple[str, list[Any]]]:\n",
    "    \"\"\"\n",
    "    Process a Pandas DataFrame of Chunk nodes and return the entities found in each chunk.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    chunk_nodes_dataframe : pd.DataFrame\n",
    "        A Pandas DataFrame where each row represents a Chunk node.\n",
    "        Has columns `id` and `text`.\n",
    "    batch_size : int\n",
    "        The number of text chunks to process in each batch.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list[tuple[str, list[dict[str, Any]]]]\n",
    "        A list of tuples, where the first element is the chunk id and the second element is a list of entities found in the chunk.\n",
    "    \"\"\"\n",
    "\n",
    "    results = list()\n",
    "\n",
    "    for batch_idx, i in enumerate(range(0, len(chunk_nodes_dataframe), batch_size)):\n",
    "        if i + batch_size >= len(chunk_nodes_dataframe):\n",
    "            batch = chunk_nodes_dataframe.iloc[i:]\n",
    "        else:\n",
    "            batch = chunk_nodes_dataframe.iloc[i:i+batch_size]\n",
    "        print(f\"Processing batch {batch_idx+1} of {int(len(chunk_nodes_dataframe)/(batch_size))}  \", end=\"\\r\")\n",
    "        # Create tasks for all nodes in the batch\n",
    "        # order is maintained\n",
    "        tasks = [extract_entities_from_text_chunk(row[\"text\"]) for _, row in batch.iterrows()]\n",
    "        # Execute all tasks concurrently\n",
    "        extraction_results = await asyncio.gather(*tasks)\n",
    "        # Add extracted records to the results list\n",
    "        results.extend(extraction_results)\n",
    "\n",
    "    # Return chunk_id paired with its entities from the results list\n",
    "    return [(chunk_id, entities) for chunk_id, entities in zip(chunk_nodes_dataframe[\"id\"], results)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to collect the article file names to pass to Unstructured for parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_names = os.listdir(\"articles/pdf/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now defined \n",
    "* Lexical and domain data models\n",
    "* Partitioning and chunking logic for articles\n",
    "* Entity extraction logic for chunks\n",
    "\n",
    "It is now time to define our ingestion logic. We will run ingest in three stages \n",
    "\n",
    "1. Load lexical graph\n",
    "2. Embed lexical graph Chunk nodes\n",
    "3. Extract domain / entity graph from lexical graph\n",
    "\n",
    "Decoupling these stages allows us easily make changes as we iterate our ingestion process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from pyneoinstance import Neo4jInstance, load_yaml_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our database credentials and all of our queries are stored in the `pyneoinstance_config.yaml` file. \n",
    "\n",
    "This makes it easy to manage our queries and keeps the notebook code clean. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_yaml_file(\"pyneoinstance_config.yaml\")\n",
    "\n",
    "db_info = config['db_info']\n",
    "\n",
    "constraints = config['initializing_queries']['constraints']\n",
    "indexes = config['initializing_queries']['indexes']\n",
    "\n",
    "node_load_queries = config['loading_queries']['nodes']\n",
    "relationship_load_queries = config['loading_queries']['relationships']\n",
    "\n",
    "processing_queries = config['processing_queries']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This graph object will handle database connections and read / write transactions for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = Neo4jInstance(db_info.get('uri', os.getenv(\"NEO4J_URI\", \"neo4j://localhost:7687\")), # use config value -> use env value -> use default value\n",
    "                      db_info.get('user', os.getenv(\"NEO4J_USER\", \"neo4j\")), \n",
    "                      db_info.get('password', os.getenv(\"NEO4J_PASSWORD\", \"password\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a helper function for ingesting data using the PyNeoInstance library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_partition(data: pd.DataFrame, batch_size: int = 500) -> int:\n",
    "    \"\"\"\n",
    "    Determine the data partition based on the desired batch size.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : pd.DataFrame\n",
    "        The Pandas DataFrame to partition.\n",
    "    batch_size : int\n",
    "        The desired batch size.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        The partition size.\n",
    "    \"\"\"\n",
    "    \n",
    "    partition = int(len(data) / batch_size)\n",
    "    print(\"partition: \"+str(partition if partition > 1 else 1))\n",
    "    return partition if partition > 1 else 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we write all the constraints and indexes we need for both the lexical and domain graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_constraints_and_indexes() -> None:\n",
    "    \"\"\"\n",
    "    Create constraints and indexes for the graph.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if constraints and len(constraints) > 0:\n",
    "            graph.execute_write_queries(database=db_info['database'], queries=list(constraints.values()))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    try:\n",
    "        if indexes and len(indexes) > 0:\n",
    "            graph.execute_write_queries(database=db_info['database'], queries=list(indexes.values()))\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_constraints_and_indexes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingest Lexical Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing | Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process the articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nihms-1852972.pdf',\n",
       " 'fendo-11-00178.pdf',\n",
       " 'Diabetic Medicine - 2023 - Brønden - Effects of DPP‐4 inhibitors  GLP‐1 receptor agonists  SGLT‐2 inhibitors and.pdf',\n",
       " 'jama_rosenstock_2019_oi_190026_1655321720.77793.pdf',\n",
       " 'jciinsight-3-93936.pdf']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing article: nihms-1852972.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cannot set gray non-stroke color because /'R50' is an invalid float value\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing article: fendo-11-00178.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cannot set gray non-stroke color because /'R50' is an invalid float value\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing article: Diabetic Medicine - 2023 - Brønden - Effects of DPP‐4 inhibitors  GLP‐1 receptor agonists  SGLT‐2 inhibitors and.pdf\n",
      "Processing article: jama_rosenstock_2019_oi_190026_1655321720.77793.pdf\n",
      "Processing article: jciinsight-3-93936.pdf\n"
     ]
    }
   ],
   "source": [
    "# lexical_ingest_records = process_pdf_article(article_names[0])\n",
    "lexical_ingest_records = process_pdf_articles(article_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the first few records "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>753d70915e2a2a747cee355745bd17ff08c45f90d938b1...</td>\n",
       "      <td>HHS Public Access</td>\n",
       "      <td>pubmed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>e0048a13f033f7fe71581406cd2d3bac1ffc4db7ce88a8...</td>\n",
       "      <td>OPEN ACCESS</td>\n",
       "      <td>pubmed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7d560f487686b70e9b42d82c08ea1b9a43e804ebc75b74...</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>pubmed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>db75d65961558fccb83719eed3a308ce6f794c27511b36...</td>\n",
       "      <td>KeyPoints</td>\n",
       "      <td>pubmed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1e5cfcba99bcd1dfb5aeb0cbb4c95b7ca3f35c8210a04e...</td>\n",
       "      <td>Metformin-induced glucagon-like peptide-1 secr...</td>\n",
       "      <td>pubmed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  id  \\\n",
       "0  753d70915e2a2a747cee355745bd17ff08c45f90d938b1...   \n",
       "1  e0048a13f033f7fe71581406cd2d3bac1ffc4db7ce88a8...   \n",
       "2  7d560f487686b70e9b42d82c08ea1b9a43e804ebc75b74...   \n",
       "3  db75d65961558fccb83719eed3a308ce6f794c27511b36...   \n",
       "4  1e5cfcba99bcd1dfb5aeb0cbb4c95b7ca3f35c8210a04e...   \n",
       "\n",
       "                                                name  source  \n",
       "0                                  HHS Public Access  pubmed  \n",
       "1                                        OPEN ACCESS  pubmed  \n",
       "2                                           Abstract  pubmed  \n",
       "3                                          KeyPoints  pubmed  \n",
       "4  Metformin-induced glucagon-like peptide-1 secr...  pubmed  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexical_ingest_records[\"nodes\"][\"document\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Document and Chunk nodes into the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_lexical_nodes(document_dataframe: pd.DataFrame, \n",
    "                       chunk_dataframe: pd.DataFrame, \n",
    "                       text_element_dataframe: pd.DataFrame, \n",
    "                       image_element_dataframe: pd.DataFrame, \n",
    "                       table_element_dataframe: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Load lexical nodes into the graph. These include Document and Chunk nodes.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    document_dataframe : pd.DataFrame\n",
    "        A Pandas DataFrame of Document nodes to load into the graph. \n",
    "    chunk_dataframe : pd.DataFrame\n",
    "        A Pandas DataFrame of Chunk nodes to load into the graph.\n",
    "    text_element_dataframe : pd.DataFrame\n",
    "        A Pandas DataFrame of TextElement nodes to load into the graph. \n",
    "    image_element_dataframe : pd.DataFrame\n",
    "        A Pandas DataFrame of ImageElement nodes to load into the graph. \n",
    "    table_element_dataframe : pd.DataFrame\n",
    "        A Pandas DataFrame of TableElement nodes to load into the graph. \n",
    "    \"\"\"\n",
    "    \n",
    "    lexical_nodes_ingest_iterator = list(zip([document_dataframe, \n",
    "                                              chunk_dataframe, \n",
    "                                              text_element_dataframe, \n",
    "                                              image_element_dataframe, \n",
    "                                              table_element_dataframe], \n",
    "                                              ['document', \n",
    "                                               'chunk', \n",
    "                                               'text_element', \n",
    "                                               'image_element', \n",
    "                                               'table_element']))\n",
    "\n",
    "    for data, query in lexical_nodes_ingest_iterator:\n",
    "        res = graph.execute_write_query_with_data(database=db_info['database'], \n",
    "                                                    data=data, \n",
    "                                                    query=node_load_queries[query], \n",
    "                                                    partitions=get_partition(data, batch_size=500),\n",
    "                                                    parallel=True,\n",
    "                                                    workers=2)\n",
    "        print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "partition: 1\n",
      "{'labels_added': 5, 'nodes_created': 5, 'properties_set': 15}\n",
      "partition: 1\n",
      "{'labels_added': 500, 'nodes_created': 500, 'properties_set': 1500}\n",
      "partition: 7\n",
      "{'labels_added': 7076, 'nodes_created': 3538, 'properties_set': 14152}\n",
      "partition: 1\n",
      "{'labels_added': 74, 'nodes_created': 37, 'properties_set': 222}\n",
      "partition: 1\n",
      "{'labels_added': 36, 'nodes_created': 18, 'properties_set': 108}\n"
     ]
    }
   ],
   "source": [
    "load_lexical_nodes(lexical_ingest_records[\"nodes\"][\"document\"], \n",
    "                   lexical_ingest_records[\"nodes\"][\"chunk\"], \n",
    "                   lexical_ingest_records[\"nodes\"][\"text_element\"], \n",
    "                   lexical_ingest_records[\"nodes\"][\"image_element\"], \n",
    "                   lexical_ingest_records[\"nodes\"][\"table_element\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the relationships into the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_lexical_relationships(chunk_part_of_document_dataframe: pd.DataFrame, \n",
    "                               unstructured_element_part_of_chunk_dataframe: pd.DataFrame, \n",
    "                               chunk_has_next_chunk_dataframe: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Load lexical relationships into the graph.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    chunk_part_of_document_dataframe : pd.DataFrame\n",
    "        A Pandas DataFrame of Chunk - PART_OF -> Document relationships to load into the graph.\n",
    "        Should have columns `chunk_id` and `document_id`.\n",
    "    unstructured_element_part_of_chunk_dataframe : pd.DataFrame\n",
    "        A Pandas DataFrame of UnstructuredElement - PART_OF -> Chunk relationships to load into the graph.\n",
    "        Should have columns `unstructured_element_id` and `chunk_id`.\n",
    "    chunk_has_next_chunk_dataframe : pd.DataFrame\n",
    "        A Pandas DataFrame of Chunk - HAS_NEXT_CHUNK -> Chunk relationships to load into the graph.\n",
    "        Should have columns `source_id` and `target_id`.\n",
    "    \"\"\"\n",
    "    lexical_relationships_ingest_iterator = list(zip([chunk_part_of_document_dataframe, \n",
    "                                                      unstructured_element_part_of_chunk_dataframe, \n",
    "                                                      chunk_has_next_chunk_dataframe], \n",
    "                                                      ['chunk_part_of_document', \n",
    "                                                       'unstructured_element_part_of_chunk', \n",
    "                                                       'chunk_has_next_chunk']))\n",
    "\n",
    "    for data, query in lexical_relationships_ingest_iterator:\n",
    "        res = graph.execute_write_query_with_data(database=db_info['database'], \n",
    "                                                    data=data, \n",
    "                                                    query=relationship_load_queries[query], \n",
    "                                                    partitions=get_partition(data, batch_size=500))\n",
    "        print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "partition: 1\n",
      "{'relationships_created': 500}\n",
      "partition: 7\n",
      "{'relationships_created': 3672}\n",
      "partition: 1\n",
      "{'relationships_created': 495}\n"
     ]
    }
   ],
   "source": [
    "load_lexical_relationships(lexical_ingest_records[\"relationships\"][\"chunk_part_of_document\"], \n",
    "                          lexical_ingest_records[\"relationships\"][\"unstructured_element_part_of_chunk\"],\n",
    "                          lexical_ingest_records[\"relationships\"][\"chunk_has_next_chunk\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embed Lexical Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will read Chunk nodes from the graph that don't have embedding properties yet. \n",
    "\n",
    "We will then embed the Chunk text property and add the embedding as a property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_index = ...\n",
    "\n",
    "def create_vector_index() -> None:\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(driver) -> None:\n",
    "    ...\n",
    "\n",
    "def embed_lexical_graph(driver) -> None:\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Entities from Lexical Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now perform entity extraction on the Chunk nodes to augment and connect to our domain graph containing patient journey information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunk_nodes_to_process_by_article_name(article_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Retrieve Chunk node id and text from the database that have a relationship to the Document with the article name provided.\n",
    "    These chunks may then be used as input to the entity extraction process.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    article_name : str\n",
    "        The name of the article to retrieve chunks for.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A Pandas DataFrame where each row represents a Chunk node connected to the Document with the article name provided.\n",
    "        Has columns `id` and `text`.\n",
    "    \"\"\" \n",
    "    doc_id, title = parse_article_file_name(article_name)\n",
    "    return graph.execute_read_query(database=db_info['database'], \n",
    "                            parameters={\"article_name\": title}, \n",
    "                            query=processing_queries['get_chunk_nodes_to_process_by_article_name'], \n",
    "                        )\n",
    "\n",
    "def get_chunk_nodes_to_process(min_length: int = 100) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Retrieve Chunk node id and text from the database that don't have an embedding.\n",
    "    These chunks may then be used as input to the entity extraction process.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    min_length : int\n",
    "        The minimum length the text must be to be included in the DataFrame.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A Pandas DataFrame where each row represents a Chunk node that has text and is at least `min_length` characters long.\n",
    "        Has columns `id` and `text`.\n",
    "    \"\"\"\n",
    "    return graph.execute_read_query(database=db_info['database'], \n",
    "                            query=processing_queries['get_chunk_nodes_to_process'], \n",
    "                            parameters={\"min_length\": min_length},\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownPropertyKeyWarning} {category: UNRECOGNIZED} {title: The provided property key is not in the database} {description: One of the property names in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing property name is: embedding)} {position: line: 3, column: 9, offset: 49} for query: 'MATCH (c:Chunk)\\nWHERE c.text IS NOT NULL\\n  AND c.embedding IS NULL\\n  AND size(c.text) >= $min_length\\nRETURN c.id as id, c.text as text'\n"
     ]
    }
   ],
   "source": [
    "chunks_to_process = get_chunk_nodes_to_process(min_length=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 494 chunks to process\n",
      "\n",
      "First chunk:\n",
      "\n",
      "statistical significance (metformin + Ex9-39 vs. placebo + Ex9-39, P = 0.053). The glucose iAUC after metformin + saline was significantly smaller than the iAUC for metformin + Ex9-39 (P = 0.004). Based on individual iAUC values, the relative contribution of GLP-1 to the acute glucose-lowering effect of metformin was 75% ± 35%, calculated as follows: 100% × ([iAUCplacebo + saline – iAUCmetformin + saline] – [iAUCplacebo + Ex9–39 – iAUCmetformin + Ex9–39])/(iAUCplacebo + saline – iAUCmetformin + saline) (P = 0.05). Using a 2-way ANOVA, both metformin and Ex9-39 were shown to significantly affect postprandial plasma glucose (iAUC) (P = 0.005 and P = 0.002, respectively), but no interaction between the 2 factors was evident. The time courses of the C-peptide/glucose ratios are illustrated in Figure 2B, and the AUCs for C-peptide/glucose, insulin/glucose, and insulin secretion\n"
     ]
    }
   ],
   "source": [
    "print(f\"Found {len(chunks_to_process)} chunks to process\\n\")\n",
    "print(f\"First chunk:\\n\\n{chunks_to_process.loc[0,'text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed after 3 attempts0  \n",
      "Processing batch 10 of 10  \r"
     ]
    }
   ],
   "source": [
    "entity_ingest_records = await extract_entities_from_chunk_nodes(chunks_to_process[:200], batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('0024e9c2d7afcf519d4d13871816a21d',\n",
       "  [ClinicalOutcome(category='Postprandial Glucose Control', description='Statistical significance - metformin + Ex9-39 vs. placebo + Ex9-39', measurement_unit=None, normal_range=None, baseline_value=None, post_treatment_value=None, change_from_baseline=None, p_value=0.053, confidence_interval=None, effect_size=None, clinical_outcome_id='49bca9ce5a1f8fd1350bb8a7c6f80600e28733c95f917109d14f622ad44f743a'),\n",
       "   ClinicalOutcome(category='Postprandial Glucose Control', description='The glucose iAUC after metformin + saline was significantly smaller than the iAUC for metformin + Ex9-39', measurement_unit=None, normal_range=None, baseline_value=None, post_treatment_value=None, change_from_baseline=None, p_value=0.004, confidence_interval=None, effect_size=None, clinical_outcome_id='3d3c65412bf36aeaf8a201b4ff50b70c533c93c062e1d05a971468d3af09f679'),\n",
       "   ClinicalOutcome(category='Postprandial Glucose Control', description='Relative contribution of GLP-1 to the acute glucose-lowering effect of metformin was 75% ± 35%', measurement_unit=None, normal_range=None, baseline_value=None, post_treatment_value=None, change_from_baseline=None, p_value=0.05, confidence_interval=None, effect_size=None, clinical_outcome_id='8eb6cf5ba24fd4f29bffe71cdacc4527d27edbc90d18a2a4888af99a2acc5805'),\n",
       "   ClinicalOutcome(category='Postprandial Glucose Control', description='Metformin and Ex9-39 significantly affect postprandial plasma glucose (iAUC)', measurement_unit=None, normal_range=None, baseline_value=None, post_treatment_value=None, change_from_baseline=None, p_value=0.005, confidence_interval=None, effect_size=None, clinical_outcome_id='cd397186716dfb50362a37419c2b8435814d1654c316111862b48aa3b9cd98d7'),\n",
       "   ClinicalOutcome(category='Postprandial Glucose Control', description='Ex9-39 significantly affects postprandial plasma glucose (iAUC)', measurement_unit=None, normal_range=None, baseline_value=None, post_treatment_value=None, change_from_baseline=None, p_value=0.002, confidence_interval=None, effect_size=None, clinical_outcome_id='80e3ecd0cbd89f0d6e42df6e2688d840b7c443c2071313dd479decc6296dbbbe')]),\n",
       " ('027e9b03adf9e1f31148048ec58ea1bf',\n",
       "  [Medication(name='GLP-1 receptor agonists (GLP-1 RAs)', medication_class='GLP-1 RA', mechanism='GLP-1 receptor activation', generic_name=None, brand_names=None, approval_status=None),\n",
       "   Medication(name='Dipeptidyl peptidase-4 inhibitors (DPP-4 inhibitors)', medication_class='DPP-4 inhibitor', mechanism=None, generic_name=None, brand_names=None, approval_status=None),\n",
       "   ClinicalOutcome(category='Cardiovascular outcomes', description='The reduction of a composite 3-point major adverse cardiovascular event (MACE) outcome in large, randomized cardiovascular outcome trials for GLP-1 RAs. DPP-4 inhibitors have not shown increased risk but also no cardiovascular benefit.', measurement_unit=None, normal_range=None, baseline_value=None, post_treatment_value=None, change_from_baseline=None, p_value=None, confidence_interval=None, effect_size=None, clinical_outcome_id='3c258fe56649d908964657d57f7ce3a49095c51d6d844cf4e331ba241fdb1195'),\n",
       "   ClinicalOutcome(category='Weight reduction', description='In clinical trials, more than 50% of participants treated with GLP-1RAs achieved a weight reduction of 5-10%, which is clinically significant and associated with improvements in cardiovascular risk factors in obese individuals with T2DM.', measurement_unit='%', normal_range=None, baseline_value=None, post_treatment_value=None, change_from_baseline=None, p_value=None, confidence_interval=None, effect_size=None, clinical_outcome_id='f01148065f3c9ac2b0cb24b9b8ea2865b19f88a7fe74eb01aa6eac68457e4247')]),\n",
       " ('02c242e8062402d8058b02e0562b16ac',\n",
       "  [ClinicalOutcome(category='Primary Outcome', description='A global log-rank test was used to test for any differences among the four groups, and additional tests were used to assess pairwise differences. P values are not reported due to the possibility of non-reproducibility.', measurement_unit=None, normal_range=None, baseline_value=None, post_treatment_value=None, change_from_baseline=None, p_value=None, confidence_interval='95%', effect_size=None, clinical_outcome_id='60822e36237963a63cf8d2404d394609a3173e7f3f7519d0caa1a220a701f15b')]),\n",
       " ('02db38e600c6387ec94b8ae7bb68093d', []),\n",
       " ('03bfd6e76b1695062cdf81fa22660313',\n",
       "  [ClinicalOutcome(category='MACE', description='Composite of cardiovascular death, non-fatal myocardial infarction (MI) and non-fatal stroke', measurement_unit=None, normal_range=None, baseline_value=None, post_treatment_value=None, change_from_baseline=None, p_value=None, confidence_interval=None, effect_size=None, clinical_outcome_id='8ca01a29fcf6354133abef4bf7906d845c864d896d557c9404a1ecb5f299ca4e'),\n",
       "   ClinicalOutcome(category='HHF', description='Hospital admission due to clinical manifestations of heart failure including the requirement for initiation or up-titration of relevant treatment (e.g. diuretics)', measurement_unit=None, normal_range=None, baseline_value=None, post_treatment_value=None, change_from_baseline=None, p_value=None, confidence_interval=None, effect_size=None, clinical_outcome_id='ebef907d4ed5cf47c2dedddece1f72710f31540d82efc77e127756e3b3b5bc62'),\n",
       "   ClinicalOutcome(category='Composite endpoint for kidney disease', description='Characterised by a sustained decrease of more than 30%-50% in the estimated glomerular filtration rate (eGFR), sustained end-stage kidney disease (eGFR <15 and/or renal replacement therapy) or death with renal disease as the underlying cause. Renal death was not included in the REWIND and SUSTAIN-6 trials for dulaglutide and sc. semaglutide', measurement_unit=None, normal_range=None, baseline_value=None, post_treatment_value=None, change_from_baseline=None, p_value=None, confidence_interval=None, effect_size=None, clinical_outcome_id='714498338306f6b0f24313cf5e491e753bc306a6afdbc12139a4c58f2e1ba536')]),\n",
       " ('0422c069e545cbe0193630bfec136371',\n",
       "  [ClinicalOutcome(category='Glucagon Secretion', description='Increase in plasma glucagon observed in the study due to inhibition of a negative feedback loop leading to increased glucagon secretion from pancreatic α cell.', measurement_unit=None, normal_range=None, baseline_value=None, post_treatment_value=None, change_from_baseline=None, p_value=None, confidence_interval=None, effect_size=None, clinical_outcome_id='309a4e06384a2bbe24e8fa3bf654c6da7e70177a99ab76792bfca938ae2eda58')]),\n",
       " ('04bca6dfbde73f0fc1028323a02b1e53',\n",
       "  [Medication(name='Dipeptidyl peptidase 4 inhibitors', medication_class='Dipeptidyl peptidase 4 inhibitors', mechanism='Inhibition of DPP-4 enzyme', generic_name=None, brand_names=None, approval_status='FDA approved'),\n",
       "   Medication(name='Semaglutide', medication_class='GLP-1 receptor agonist', mechanism='GLP-1 receptor activation', generic_name='semaglutide', brand_names=['Ozempic', 'Wegovy', 'Rybelsus'], approval_status='FDA approved'),\n",
       "   StudyMedication(study_name='This Trial', treatment_arm='3-mg/d oral semaglutide', dosage='3 mg/d', route='oral', frequency=None, treatment_duration=None, comparator='sitagliptin', adherence_rate=None, formulation=None, study_medication_id='450b079b864df02097f9166c8047b82f6d2dc4391c18d139c8fb83c6cae9224e'),\n",
       "   StudyMedication(study_name='This Trial', treatment_arm='7-mg/d oral semaglutide', dosage='7 mg/d', route='oral', frequency=None, treatment_duration=None, comparator='sitagliptin', adherence_rate=None, formulation=None, study_medication_id='6a5d9fe197d43148ea20019ab55466880edfa93dfedccad491b1cef460c32b85'),\n",
       "   StudyMedication(study_name='This Trial', treatment_arm='14-mg/d oral semaglutide', dosage='14 mg/d', route='oral', frequency=None, treatment_duration=None, comparator='sitagliptin', adherence_rate=None, formulation=None, study_medication_id='73e72d1cda18b5cf82bf8f46f5f5a630522d327516ddd6f48e9463c1b3350061'),\n",
       "   StudyMedication(study_name='This Trial', treatment_arm='Sitagliptin', dosage=None, route='oral', frequency=None, treatment_duration=None, comparator='oral semaglutide', adherence_rate=None, formulation=None, study_medication_id='04b98b94f7253d2dbdfade13f72f011260f0835bc65fa4c25afd556cccab6c72'),\n",
       "   ClinicalOutcome(category='Adverse Events', description='Long-term adverse event profile of oral semaglutide, consistent with GLP-1 RA class.', measurement_unit=None, normal_range=None, baseline_value=None, post_treatment_value=None, change_from_baseline=None, p_value=None, confidence_interval=None, effect_size=None, clinical_outcome_id='c825c0f69e376a0b1222cddadc19439fd71e3992b6a10dac624ff5a017909ba5'),\n",
       "   ClinicalOutcome(category='Gastrointestinal Adverse Events', description='Most frequent adverse events were nausea, vomiting, or diarrhea, observed in oral semaglutide and other GLP-1 RAs.', measurement_unit=None, normal_range=None, baseline_value=None, post_treatment_value=None, change_from_baseline=None, p_value=None, confidence_interval=None, effect_size=None, clinical_outcome_id='85a7a9379a252f42181eb1db1f227349b972307a48e27ced74f90604e047855a'),\n",
       "   ClinicalOutcome(category='Discontinuation Due to Adverse Events', description='Similar proportions of patients discontinued 3-mg/d and 7-mg/d oral semaglutide, but higher discontinuation for 14-mg/d dosage, mostly during dosage-escalation.', measurement_unit=None, normal_range=None, baseline_value=None, post_treatment_value=None, change_from_baseline=None, p_value=None, confidence_interval=None, effect_size=None, clinical_outcome_id='14a3e298f6d513f83334923c96c620f52d5dc0572b7a54e861a4616d07b52cee')]),\n",
       " ('04de94dd03e810d0d0c032c0910e0004',\n",
       "  [ClinicalOutcome(category='Hormonal response', description='Glucagon concentrations', measurement_unit=None, normal_range=None, baseline_value=None, post_treatment_value=None, change_from_baseline=None, p_value=0.004, confidence_interval=None, effect_size=None, clinical_outcome_id='08a0a4f896d13ed5eda10acfb4e67a55343ee2d844d17a79083f8f0d0f3d0b8d')]),\n",
       " ('06d164b325030c636c55559dca5ab01f',\n",
       "  [Medication(name='Semaglutide', medication_class='GLP-1 receptor agonist', mechanism=None, generic_name='oral semaglutide', brand_names=None, approval_status='FDA approved'),\n",
       "   Medication(name='Sitagliptin', medication_class='DPP-4 inhibitor', mechanism=None, generic_name=None, brand_names=None, approval_status='FDA approved')]),\n",
       " ('072f6fc5f143b89bf521a5b75867f080',\n",
       "  [Medication(name='Exenatide', medication_class='GLP-1 receptor agonist', mechanism='GLP-1 receptor activation', generic_name='exenatide', brand_names=['Bydureon', 'Byetta'], approval_status='FDA approved'),\n",
       "   Medication(name='Sitagliptin', medication_class='DPP-4 inhibitor', mechanism='DPP-4 inhibition', generic_name='sitagliptin', brand_names=['Januvia'], approval_status='FDA approved')])]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_ingest_records[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingest Entities Into Knowledge Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These functions load the extracted entities and relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These functions link the extracted entities with their text chunk nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENTITY_LABELS = {\n",
    "    \"Medication\", \n",
    "    \"StudyMedication\",\n",
    "    \"MedicalCondition\",\n",
    "    \"StudyPopulation\",\n",
    "    \"ClinicalOutcome\",\n",
    "}\n",
    "\n",
    "ENTITY_RELS = {\n",
    "    \"StudyMedicationUsesMedication\",\n",
    "    \"StudyMedicationProducesClinicalOutcome\",\n",
    "    \"StudyPopulationHasMedicalCondition\",\n",
    "    \"StudyPopulationReceivesStudyMedication\",\n",
    "    \"StudyPopulationHasOutcome\",\n",
    "}\n",
    "\n",
    "def prepare_entities_for_ingestion(entities: list[tuple[str, list[Any]]]) -> dict[str, dict[str, pd.DataFrame]]:\n",
    "    \"\"\"\n",
    "    Prepare entities for ingestion into the graph.\n",
    "    This function takes the results of the `get_chunk_nodes_to_process_by_article_name` function and returns a dictionary of entity label to list of entities.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    entities : list[tuple[str, list[Any]]]\n",
    "        A list of tuples, where the first element is the chunk id and the second element is a list of entities found in the chunk.\n",
    "        Entities are Pydantic models that adhere to the domain graph data model.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict[str, dict[str, pd.DataFrame]]\n",
    "        A dictionary of entity label to pandas dataframe of entities.\n",
    "\n",
    "        {\n",
    "            \"nodes\": {\n",
    "                \"Medication\": pd.DataFrame(...),\n",
    "                \"StudyMedication\": pd.DataFrame(...),\n",
    "                ...\n",
    "            },\n",
    "            \"relationships\": {\n",
    "                \"StudyMedicationUsesMedication\": pd.DataFrame(...),\n",
    "                \"StudyMedicationProducesClinicalOutcome\": pd.DataFrame(...),\n",
    "                ...\n",
    "            }\n",
    "        }\n",
    "    \"\"\"\n",
    "\n",
    "    records_node_dict = {lbl: list() for lbl in ENTITY_LABELS}\n",
    "    records_rel_dict = {lbl: list() for lbl in ENTITY_RELS}\n",
    "\n",
    "    for chunk_id, entities in entities:\n",
    "        for entity in entities:\n",
    "            to_add = entity.model_dump()\n",
    "            to_add.update({\"chunk_id\": chunk_id})\n",
    "            # nodes\n",
    "            if entity.__class__.__name__ in ENTITY_LABELS:\n",
    "                records_node_dict[entity.__class__.__name__].append(to_add)\n",
    "            # rels\n",
    "            elif entity.__class__.__name__ in ENTITY_RELS:\n",
    "                records_rel_dict[entity.__class__.__name__].append(to_add)\n",
    "            else:\n",
    "                print(f\"Unknown entity type: {entity.__class__.__name__}\")\n",
    "\n",
    "    for key, value in records_node_dict.items():\n",
    "        records_node_dict[key] = pd.DataFrame(value)\n",
    "\n",
    "    for key, value in records_rel_dict.items():\n",
    "        records_rel_dict[key] = pd.DataFrame(value)\n",
    "\n",
    "    return {\"nodes\": records_node_dict, \"relationships\": records_rel_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_entity_nodes(medication_dataframe: pd.DataFrame, \n",
    "                      medical_condition_dataframe: pd.DataFrame, \n",
    "                      study_medication_dataframe: pd.DataFrame, \n",
    "                      study_population_dataframe: pd.DataFrame, \n",
    "                      clinical_outcome_dataframe: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Load entity nodes into the graph.\n",
    "    \"\"\"\n",
    "    \n",
    "    entity_nodes_ingest_iterator = list(zip([medication_dataframe, \n",
    "                                             medical_condition_dataframe, \n",
    "                                             study_medication_dataframe, \n",
    "                                             study_population_dataframe, \n",
    "                                             clinical_outcome_dataframe], \n",
    "                                             ['medication', \n",
    "                                              'medical_condition', \n",
    "                                              'study_medication', \n",
    "                                              'study_population', \n",
    "                                              'clinical_outcome']))\n",
    "\n",
    "    for data, query in entity_nodes_ingest_iterator:\n",
    "        if len(data) > 0:\n",
    "            print(f\"Loading {len(data)} {query} nodes\")\n",
    "            res = graph.execute_write_query_with_data(database=db_info['database'], \n",
    "                                                    data=data, \n",
    "                                                    query=node_load_queries[query], \n",
    "                                                    partitions=get_partition(data, batch_size=500),\n",
    "                                                    parallel=False)\n",
    "            print(res)\n",
    "        else:\n",
    "            print(f\"No {query} nodes to load\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_entity_relationships(study_medication_uses_medication_dataframe: pd.DataFrame,\n",
    "                              study_medication_produces_clinical_outcome_dataframe: pd.DataFrame,\n",
    "                              study_population_has_medical_condition_dataframe: pd.DataFrame,\n",
    "                              study_population_receives_study_medication_dataframe: pd.DataFrame,\n",
    "                              study_population_has_outcome_dataframe: pd.DataFrame,\n",
    "                              ) -> None:\n",
    "    \"\"\"\n",
    "    Load entity relationships into the graph.\n",
    "    \"\"\"\n",
    "    entity_relationships_ingest_iterator = list(zip([study_medication_uses_medication_dataframe, \n",
    "                                                      study_medication_produces_clinical_outcome_dataframe, \n",
    "                                                      study_population_has_medical_condition_dataframe, \n",
    "                                                      study_population_receives_study_medication_dataframe, \n",
    "                                                      study_population_has_outcome_dataframe], \n",
    "                                                      ['study_medication_uses_medication', \n",
    "                                                       'study_medication_produces_clinical_outcome', \n",
    "                                                       'study_population_has_medical_condition', \n",
    "                                                       'study_population_receives_study_medication', \n",
    "                                                       'study_population_has_outcome']))\n",
    "    \n",
    "    for data, query in entity_relationships_ingest_iterator:\n",
    "        if len(data) > 0:\n",
    "            print(f\"Loading {len(data)} {query} relationships\")\n",
    "            res = graph.execute_write_query_with_data(database=db_info['database'], \n",
    "                                                    data=data, \n",
    "                                                    query=relationship_load_queries[query], \n",
    "                                                    partitions=get_partition(data, batch_size=500),\n",
    "                                                    parallel=False)\n",
    "            print(res)\n",
    "        else:\n",
    "            print(f\"No {query} relationships to load\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def link_entities_to_chunks(medication_link_dataframe: pd.DataFrame, \n",
    "                      medical_condition_link_dataframe: pd.DataFrame, \n",
    "                      study_medication_link_dataframe: pd.DataFrame, \n",
    "                      study_population_link_dataframe: pd.DataFrame, \n",
    "                      clinical_outcome_link_dataframe: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Link entities to chunks.\n",
    "    \"\"\"\n",
    "    entity_link_iterator = list(zip([medication_link_dataframe, \n",
    "                                     medical_condition_link_dataframe, \n",
    "                                     study_medication_link_dataframe, \n",
    "                                     study_population_link_dataframe, \n",
    "                                     clinical_outcome_link_dataframe], \n",
    "                                     [\"chunk_has_entity_medication\",\n",
    "                                      \"chunk_has_entity_medical_condition\",\n",
    "                                      \"chunk_has_entity_study_medication\",\n",
    "                                      \"chunk_has_entity_study_population\",\n",
    "                                      \"chunk_has_entity_clinical_outcome\"]))\n",
    "    \n",
    "    for data, query in entity_link_iterator:\n",
    "        if len(data) > 0:\n",
    "            print(f\"Linking {len(data)} {query} entities to chunks\")\n",
    "            res = graph.execute_write_query_with_data(database=db_info['database'], \n",
    "                                                    data=data, \n",
    "                                                    query=relationship_load_queries[query], \n",
    "                                                    partitions=get_partition(data, batch_size=500),\n",
    "                                                    parallel=False)\n",
    "            print(res)\n",
    "        else:\n",
    "            print(f\"No {query} relationships to load\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingest_records = prepare_entities_for_ingestion(entity_ingest_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(124)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ingest_records[\"nodes\"][\"Medication\"]['generic_name'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 224 medication nodes\n",
      "partition: 1\n",
      "{'labels_added': 83, 'nodes_created': 83, 'properties_set': 1203}\n",
      "No medical_condition nodes to load\n",
      "Loading 133 study_medication nodes\n",
      "partition: 1\n",
      "{'labels_added': 132, 'nodes_created': 132, 'properties_set': 1318}\n",
      "No study_population nodes to load\n",
      "Loading 236 clinical_outcome nodes\n",
      "partition: 1\n",
      "{'labels_added': 217, 'nodes_created': 217, 'properties_set': 2545}\n"
     ]
    }
   ],
   "source": [
    "load_entity_nodes(ingest_records[\"nodes\"][\"Medication\"], \n",
    "                  ingest_records[\"nodes\"][\"MedicalCondition\"], \n",
    "                  ingest_records[\"nodes\"][\"StudyMedication\"], \n",
    "                  ingest_records[\"nodes\"][\"StudyPopulation\"], \n",
    "                  ingest_records[\"nodes\"][\"ClinicalOutcome\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 45 study_medication_uses_medication relationships\n",
      "partition: 1\n",
      "{'relationships_created': 42}\n",
      "Loading 55 study_medication_produces_clinical_outcome relationships\n",
      "partition: 1\n",
      "{'relationships_created': 39}\n",
      "No study_population_has_medical_condition relationships to load\n",
      "No study_population_receives_study_medication relationships to load\n",
      "No study_population_has_outcome relationships to load\n"
     ]
    }
   ],
   "source": [
    "load_entity_relationships(ingest_records[\"relationships\"][\"StudyMedicationUsesMedication\"], \n",
    "                          ingest_records[\"relationships\"][\"StudyMedicationProducesClinicalOutcome\"], \n",
    "                          ingest_records[\"relationships\"][\"StudyPopulationHasMedicalCondition\"], \n",
    "                          ingest_records[\"relationships\"][\"StudyPopulationReceivesStudyMedication\"], \n",
    "                          ingest_records[\"relationships\"][\"StudyPopulationHasOutcome\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linking 224 chunk_has_entity_medication entities to chunks\n",
      "partition: 1\n",
      "{'relationships_created': 224}\n",
      "No chunk_has_entity_medical_condition relationships to load\n",
      "Linking 133 chunk_has_entity_study_medication entities to chunks\n",
      "partition: 1\n",
      "{'relationships_created': 133}\n",
      "No chunk_has_entity_study_population relationships to load\n",
      "Linking 236 chunk_has_entity_clinical_outcome entities to chunks\n",
      "partition: 1\n",
      "{'relationships_created': 218}\n"
     ]
    }
   ],
   "source": [
    "link_entities_to_chunks(ingest_records[\"nodes\"][\"Medication\"], \n",
    "                        ingest_records[\"nodes\"][\"MedicalCondition\"], \n",
    "                        ingest_records[\"nodes\"][\"StudyMedication\"], \n",
    "                        ingest_records[\"nodes\"][\"StudyPopulation\"], \n",
    "                        ingest_records[\"nodes\"][\"ClinicalOutcome\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
